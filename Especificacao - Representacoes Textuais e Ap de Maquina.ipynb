{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Nesta prática iremos apresentar o uso de embeddings. Para isso, você deve primeiro instalar as dependencias usando `pip install -r requirements.txt` (ou `pip3`, dependendo da forma que seu python está instalado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inicialmente, você deverá baixar os repositorios em português e inglês e salvá-los na pasta `embedding_data` seguindo as seguintes instruções: \n",
    "\n",
    "- [No respositório da USP](http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc) baixe [este arquivo (Glove 100 dimensões)](http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip). Ele possui  um pouco mais de 600 mil palavras retiradas de textos de páginas Web tais como a Wikipedia e canais de notícias [(Hartmann et al., 2017)](https://arxiv.org/abs/1708.06025). Descomprima e renomeie o arquivo txt para `glove.pt.100.txt`.\n",
    "\n",
    "- No [repositório de Stanford](https://nlp.stanford.edu/projects/glove/), baixe [este arquivo](http://nlp.stanford.edu/data/glove.6B.zip) use o arquivo . Este arquivo compreende ~400 mil palavras de textos extraidos da Wikipédia e [GigaWord](https://catalog.ldc.upenn.edu/LDC2011T07) [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). Descomprima e salve o arquivo com embeddings de 100 dimensões (nome `glove.6B.100d.txt`) na pasta `embedding_data` renomeando esse arquivo para `glove.en.100.txt`.\n",
    "\n",
    "Como você pode perceber, esta prática demandará um espaço livre em disco de aproximadamente 3GB. Os arquivos estão no seguinte formato: em cada linha, uma palavra e N valores representando o valor em cada uma das N dimensões do embedding desta palavra. Por exemplo, caso as palavras `casa`, `redondel` e `rei` sejam representadas por um embedding de 4 dimensões, uma possível representação seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "casa 0.12 0.1 0.5 -0.4\n",
    "redondel 0.2 0.1 -0.4 0.5\n",
    "rei 0.1 0.5 -0.1 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A função `get_embedding`, do arquivo `embeddings/utils.py` é responsável por ler esse arquivo e gerar um dicionário em que a chave é a palavra e o valor é sua representação por meio de embeddings. Para a  representação acima, a saída desta função seria seria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_embedding_ex = {\n",
    "                        \"casa\":np.array([0.12,0.1,0.5,-0.4]),\n",
    "                        \"redondel\":np.array([0.2,0.1,-0.4,0.5]),\n",
    "                        \"rei\":np.array([0.1,0.5,-0.1,0.1]),\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Nessa função, também é salvo o objeto criado usando [pickle](https://docs.python.org/3/library/pickle.html), assim, a próxima vez que seja lido o embedding, a leitura será mais rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Atividade 1 - obtenção do embedding**: Complete a função `get_embedding` obtendo a palavra e o vetor de embeddings com a dimensão `embeddings_size` substituindo os `None` apropriadamente. O dataset possui algumas incosistencias que você deve considerar ao modificar essas linhas: no dataset em português, a maioria das palavras compostas são separadas por hífen, porém, foi verificado que umas palavras foi separado por espaço. Por caso disso, você deve considerar que as `embeddings_size` últimas posições são os valores de cada dimensão, separados por espaço e, as demais, são a palavra. Sugiro \"brincar\" abaixo com o uso de [índice negativo](https://www.geeksforgeeks.org/python-negative-index-of-element-in-list/) entenda também o [método join](https://www.geeksforgeeks.org/join-function-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pé de moleque': [ 0.1 -0.5  0.5  0.1 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "linha = \"pé de moleque 0.1 -0.5 0.5 0.1 -0.5\"\n",
    "embedding_size = 5\n",
    "arr_line = linha.strip().split()\n",
    "\n",
    "word = \" \".join(arr_line[0:3])\n",
    "#colocamos float16 para economizar memória\n",
    "embedding = np.array(arr_line[3:len(arr_line)], dtype=np.float16)\n",
    "print(f\"'{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Execute o teste unitário abaixo para verificar o funcionamento do `get_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rei\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Execute os embeddings em português e ingles. Não se preocupe com as palavras ignoradas: foram algumas inconsistencias no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n",
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from embeddings.utils import get_embedding, plot_words_embeddings\n",
    "\n",
    "str_dataset = \"glove.en.100.txt\"\n",
    "dict_embedding_en = get_embedding(str_dataset)\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "dict_embedding_pt = get_embedding(str_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.118e-01, -3.047e-01,  7.500e-01,  6.094e-02,  2.856e-01,\n",
       "       -1.602e-01,  6.118e-01,  2.920e-01, -8.315e-01,  7.554e-01,\n",
       "        3.467e-01, -6.123e-01, -3.630e-01,  3.413e-01, -8.843e-01,\n",
       "        1.459e-01,  2.002e-01, -2.688e-01,  6.001e-01, -2.593e-01,\n",
       "        4.456e-01, -8.174e-01, -3.613e-01,  8.271e-01, -4.619e-01,\n",
       "        1.267e-01, -4.736e-01, -6.763e-02,  2.520e-01, -5.278e-01,\n",
       "        9.614e-01, -8.100e-05, -2.412e-01, -3.452e-01, -3.450e-01,\n",
       "        4.590e-01, -5.536e-02, -3.857e-01, -1.064e-01, -4.895e-02,\n",
       "       -3.962e-01, -3.657e-01,  6.016e-01, -1.979e-01, -1.947e-01,\n",
       "       -2.214e-02,  4.800e-01,  1.171e-01,  9.365e-01,  3.086e-01,\n",
       "        1.683e-01,  4.526e-01, -8.711e-01,  3.003e-01, -3.582e-01,\n",
       "        2.733e-02, -1.178e-01, -7.488e-03, -3.265e-02,  1.779e-01,\n",
       "        2.249e-03,  3.892e-01,  2.046e-01, -3.357e-01,  4.890e-01,\n",
       "        1.716e-01, -6.084e-01, -3.938e-01,  3.887e-01, -1.576e-01,\n",
       "       -4.417e-01,  6.001e-01, -8.179e-01,  3.296e-01,  1.796e-01,\n",
       "       -3.821e-01,  1.267e-01, -1.498e-01,  3.633e-01, -3.398e-01,\n",
       "       -5.645e-01, -7.440e-02,  1.129e-01,  7.417e-01, -6.235e-01,\n",
       "        2.969e-01, -1.254e-01,  3.274e-01,  4.595e-01, -1.067e+00,\n",
       "        3.713e-01, -1.938e-01, -3.139e-03,  3.665e-01,  5.195e-01,\n",
       "       -2.780e-02,  2.361e-01, -3.086e-01,  1.863e-01, -9.912e-01],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_embedding_pt[\"pri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3762  ,  0.2098  , -0.2106  , -0.007374,  0.3723  ,  0.9336  ,\n",
       "        1.033   ,  0.1132  ,  0.2944  , -0.4954  , -0.4814  , -0.3772  ,\n",
       "       -0.449   , -0.521   , -0.309   ,  0.747   ,  0.1554  ,  0.2815  ,\n",
       "        0.0805  , -0.05862 , -0.502   , -0.05664 ,  0.07007 ,  0.010994,\n",
       "       -0.0586  , -0.01761 , -0.07416 ,  0.5044  , -0.3923  , -0.315   ,\n",
       "        0.1155  ,  0.4998  , -0.282   ,  0.6377  ,  0.4724  ,  0.0976  ,\n",
       "       -0.3352  , -0.4866  , -0.09863 , -0.3853  ,  0.5537  , -0.0728  ,\n",
       "        0.02682 , -0.599   , -0.0785  , -0.01736 , -0.7437  ,  0.706   ,\n",
       "       -0.2067  , -0.2295  ,  0.07025 ,  0.9277  ,  0.3792  ,  0.1884  ,\n",
       "       -0.4568  ,  0.0794  ,  0.452   , -0.3853  , -0.424   ,  0.1328  ,\n",
       "       -0.1843  ,  0.6426  ,  0.0943  , -0.1254  , -0.7866  ,  0.05112 ,\n",
       "       -0.01639 , -0.485   ,  0.3352  ,  0.01027 , -0.402   ,  0.886   ,\n",
       "       -0.1929  ,  0.749   , -0.687   , -0.10114 , -0.4287  , -0.2101  ,\n",
       "       -0.1865  ,  0.1462  , -0.2517  , -0.0476  ,  0.6064  , -0.398   ,\n",
       "       -0.3975  , -0.1665  , -0.3083  , -0.2205  ,  0.3484  , -0.0581  ,\n",
       "       -0.477   ,  0.2722  , -0.755   , -0.01133 ,  1.064   ,  0.596   ,\n",
       "       -0.02878 ,  0.66    ,  0.0791  , -0.01016 ], dtype=float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_embedding_pt[\"principe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras ignoradas: 12\n"
     ]
    }
   ],
   "source": [
    "str_dataset = \"teste.3.txt\"\n",
    "dict_embedding_help = get_embedding(str_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='asda'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match(\"^[a-zA-Z]+$\", \"asda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O `plot_words_embeddings` utiliza [Análise de Componentes Principais](https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais) (PCA, do inglês Principal Component Analisys) para reduzir cada embedding em 2 dimensões para, logo após, plotar em um grafico a posição dessas palavras de acordo com o embedding. Veja o grafico apresentado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1300x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_to_use = {\n",
    "                        \"en\":\n",
    "                            {\"embedding\":dict_embedding_en,\n",
    "                            \"words_to_use\":[\"prince\",\"princess\",\n",
    "                                            \"duchess\", \"duke\", \"countess\", \"marquis\", \n",
    "                                            \"marquise\",\"king\",\"queen\",\n",
    "                                            \"girl\",\"boy\",\"man\",\"woman\",\"child\"]},\n",
    "\n",
    "                        \"pt\":{\"embedding\":dict_embedding_pt,\n",
    "                          \"words_to_use\":[\"principe\",\"rei\",\"rainha\",\"conde\",\"duquesa\",\"duque\",\"condessa\",\n",
    "                           \"marquês\",\"marquesa\",\n",
    "                           \"homem\",\"mulher\",\"princesa\",\"menina\",\"menino\",\"criança\",\n",
    "                           \"garoto\",\"garota\"]}\n",
    "                }\n",
    "\n",
    "language = \"pt\"#mude de 'pt' para 'en' para ver em ingles tb!\n",
    "plot_words_embeddings(embeddings_to_use[language][\"embedding\"], \n",
    "                    embeddings_to_use[language][\"words_to_use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "No exemplo acima, em português, veja que podemos pensar em dois conceitos claramente divididos: a realeza e o gênero. Pense, neste plano cartesiano: qual eixo corresponde ao conceito de realeza? E o de gênero? Perceba que \"criança\" deveria ter genero neutro - de fato, está mais próximo do zero. Porém, pode haver algum ruído associando a palavra criança ao genero feminino. Isso, em português, pode haver uma explicação, pois utilizamos  o artigo `a`, usado para palavras que remetem ao genero feminino, para se referir a criança. Assim, em português, os artigos podem aproximar uma palavra de genero neutro a um determinado genero.\n",
    "\n",
    "\n",
    "Em inglês, não foi possível verificar tão bem a divisão entre os conceitos de `genero` e `realeza`. Isso pode ocorrer devido a redução de dimensionalidade: os conceitos não necessariamente correspondem a um eixo no plano cartesiano e, mesmo se corresponder, ao mapear itens com $n$ dimensões para um plano bidimensional, pode haver perda de informação. Mesmo assim, conseguimos ver a separação entre palavras da realeza e que não são da realeza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sinta-se livre para \"brincar\", alterando/adicionando palavras. Por exemplo, adicione animais. Devido à ambiguidades, ao dataset e à própria redução de dimensionalidade, podem existir palavras que estão erroneamente próximas, se considerarmos o conceito das mesmas,  principalmente se adicionarmos palavras de conceitos muito distintos. Um detalhe: no dataset em português, há uso de palavras compostas e elas estão (geralmente) separadas por hífen. No dataset em inglês não há palavras compostas.\n",
    "\n",
    "Tanto nesta tarefa quanto na próxima você poderá perceber que os embeddings podem carregar preconceitos. Há uma forma de modificar os vetores para eliminar um determinado tipo de preconceito. Por exemplo, nesses embeddings existirão palavras erronemente similares a um determinado genero e, para corrigir, é possível deixar todas as palavras sem distinção pelo genero. Caso queira saber como minimizar esse problema, veja o artigo \"[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)\". O título do artigo se remete a um preconceito descoberto ao usar analogias, que será o próximo tópico desta prática. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Criação de analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Outra caracteristica muito interessante ao usar embedding é a criação de analogias. Por exemplo, na frase `homem está para mulher assim como rei está para...`, fazendo operações com os _embeddings_, muitas vezes é possível chegar na analogia mais provável que, neste caso, seria a palavra `rainha`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Atividade 2 - calculo da analogia: ** Nesta atividade, iremos implementar o método `calcula_embedding_analogia` da classe `Analogy`. Essa classe tem acesso ao dicionário de embeddings e a estrutura KDTree, que iremos explicá-la posteriormente. Considerando a frase <span style=\"color:blue\">\"**palavra_x** está para **palavra_y** assim como **assim_como** esta para **palavra_z**\"</span>, o método `calcula_embedding_analogia` recebe como parametro as palavras `palavra_x`, `esta_para` e `assim_ como` e retorna um embedding que, possivelmente, será muito próximo da `palavra_z`. \n",
    "\n",
    "Veja [na aula](https://docs.google.com/presentation/d/1-CggYUA2s7LW7_LcnGv7vlpUGFg9kEWG0j6lWGUnaLI/edit?usp=sharing) como é feito o calculo e, logo após, faça o teste unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.0, -1.0, -1.0, ]\n",
      "[ 12.296875, 53.09375, 30.984375, ]\n",
      "[ -10.96875, -30.90625, -9.6015625, ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2, 3],[-1.2, 3.2, 1.2],[12.2, 31.2, 11.2]], dtype=np.float16)\n",
    "esta_para = np.array([[-3, 0, 1],[11, 56, 32.2],[0, 0.2, 0.4]], dtype=np.float16)\n",
    "assim_como = np.array([[2, 1, 1],[0.1,0.3,0],[1.23, 0.1, 1.2]], dtype=np.float16)\n",
    "\n",
    "for i,x_val in enumerate(x):\n",
    "    arr_embedding = assim_como[i]-x[i]+esta_para[i]\n",
    "    print(\"[\",end=\" \")\n",
    "    for val in arr_embedding:\n",
    "        print(float(val),end=\", \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_calculo_analogia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Atividade 3 - busca da palavra mais similar:** O calculo da atividade anterior resultou em um embedding e, agora, precisamos  procuramos a palavra mais próxima a este embedding obtido. Para isso, precisamos de: (1) uma forma eficiente para percorrer os embeddings para descobrir o mais similar; (2) uma métrica de similaridade/distancia; \n",
    "\n",
    "**Como percorrer embeddings?** Para encontrarmos os embeddings similares, uma alternativa seria percorrer todos os vetores de embeddings e encontrar o mais similar. Porém, como estamos trabalhando com centenas de milhares de embeddings, essa operação seria muito custosa. Para isso, podemos usar uma estrutura de dados chamada **KDTree**. KDtree é uma arvore que organiza dados espaciais de tal forma que conseguimos alcançar elementos similares de forma mais eficiente. Caso esteja interessado em mais detalhes, [veja este video](https://www.youtube.com/watch?v=Glp7THUpGow).\n",
    "\n",
    "**Qual métrica de distancia/similaridade usaremos?**  Já foi demonstrado que esta métrica é eficiente para similaridade entre embeddings é a distancia euclidiana [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). A [distancia euclidiana](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_euclidiana) entre dois pontos $p$ e $q$ é calculada por meio do tamanho da linha entre esses pontos. Para um espaço bidimensional, considerando que os pontos $p$ e $q$ são representados pelas coordenadas $(p_1,p_2)$ e $(q_1,q_2)$, respectivamente, a equação é dada pela seguinte fórmula: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$ veja uma representação gráfica: \n",
    "\n",
    "<img width=\"400px\" src=\"img/distancia_euclidiana.svg\">\n",
    "\n",
    "Esta métrica pode ser generalizada para um espaço n-dimensional e o cálculo seria: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Assim, nesta atividade iremos utilizar [a implementação do kdtree do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html). Nessa estrutura, é possível armazenar os embeddings e, logo após fazer consultas eficiente para, por exemplo, procurar os k elementos mais próximos. Veja o exemplo abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O ponto [3, 3] é o 1º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [2, 2] é o 2º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [1, 1] é o 3º ponto mais próximo de [3, 2] distância: 2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "elementos = [[1,1],\n",
    "             [2,2],\n",
    "             [3,3],\n",
    "             [4,4],\n",
    "             [5,5],\n",
    "             [6,6],\n",
    "             ]\n",
    "#os elementos são passados como parametro na construção do KDTree junto com a métrica \n",
    "#de distancia que iremos usar\n",
    "kdtree = KDTree(elementos,  metric='euclidean')\n",
    "\n",
    "#retorna os 2 elementos mais próximos e sua distancia\n",
    "#como podemos fazer uma consulta por lista de pontos, temos que \n",
    "#passar uma lista de pontos como parametro\n",
    "ponto = [3,2]\n",
    "distancia,pos_mais_prox = kdtree.query([ponto], k=3, return_distance=True)\n",
    "for i,pos in enumerate(pos_mais_prox[0]):\n",
    "    elemento = elementos[pos]\n",
    "    distancia_ponto = distancia[0][i]\n",
    "    print(f\"O ponto {elemento} é o {i+1}º ponto mais próximo de {ponto} distância: {distancia_ponto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dessa forma, cada embedding pode ser armazenado no KTree para, logo após, obtermos os embeddings mais próximos a um embedding em questão. Não é possível armazenar na estrutura do KDTree a palavra referente a cada embedding representado, por isso, armazenamos essa estrutura como um atributo da classe `KDTreeEmbedding` (arquivo `utils.py`) que armazena também os atributos `pos_to_word` mapeando, para cada posição a palavra correspondente e o atributo `word_to_pos` que faz o oposto: mapeia, para cada palavra, a posição correspondente. Veja no construtor de `KDTreeEmbedding`  como é criado o KDTree. Nela, também será salvo um arquivo com a implementação do KDtree e os atributos `pot_to_word` e `word_to_pos` isso é necessário pois a criação da KDTree é muito custosa.\n",
    "\n",
    "\n",
    "Nesta atividade, você deverá implementar `get_most_similar_embedding` que obtém as $k$ palavras mais similares à palavra (ou embedding) representado pelo parametro `query` por meio do método `query` da KDTree. O parâmetro `query` pode ser a palavra (`string`) ou o proprio embedding (`np.array`). Logo após, implemente também o método `get_embeddings_by_similarity` que utiliza o método `query_radius` ([veja documentação](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree.query_radius)) que retorna todas as palavras que estão em um raio de `max_distance` da palavra alvo especificada pelo parametro `query`. Para ambas as implementações, utiliza-se o método `positions_to_word`, já implementado, para retornar as palavras de acordo com as posições indicadas. Caso haja alguma palavra a ser ignorada em `words_to_ignore` ela será excluída também no método `positions_to_word`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [1. 2. 3.]\n",
      "query_embedding = [-3.  0.  1.]\n",
      "query_embedding = [-1.2  3.2  1.2]\n",
      "query_embedding = [1. 2. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_most_similar_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_embeddings_by_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Agora, você pode testar os métodos utilizando os datasets de embeddings. Lembre-se  que o KDTree pode demorar mais de 30 minutos para ser criado na primeira execução de cada idioma. Caso queira testar para o ingles, não esqueça de mudar de `\"kdtree.pt.p\"` para `\"kdtree.en.p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n",
      "query_embedding = [-0.415    -0.484     0.2261   -1.467     0.3027   -0.1482   -1.3545\n",
      " -1.024    -0.476     0.35      0.2223    1.063    -0.69     -0.1981\n",
      "  0.8623   -0.683    -0.3738    0.8496    0.0626    0.2034   -0.424\n",
      "  0.2893    1.052     0.1517   -0.841    -0.4526    0.9604   -1.304\n",
      " -0.814     0.548     1.003     0.524    -0.4663    0.1271   -0.2832\n",
      " -0.06     -0.3423    0.5903    0.1247    1.868    -0.4988   -0.3267\n",
      " -0.311    -0.1957   -0.566     0.833    -0.006165 -0.1774    0.4697\n",
      " -0.5605   -0.2123   -0.02097   0.5425   -0.5825   -0.6943    0.03604\n",
      " -0.1301   -0.1166    0.3374   -0.1081   -0.011505 -0.3606   -0.3018\n",
      " -0.08     -0.2834   -0.1974    0.1219   -0.875    -0.74     -0.377\n",
      " -0.6816   -0.1464    0.2451    0.4973    2.072    -0.796    -0.1255\n",
      "  0.1443    0.4028   -0.6846   -0.5513   -1.883     1.038    -0.534\n",
      "  0.1938    0.673    -1.094     0.283     0.08746   1.019    -0.1992\n",
      "  0.3806   -0.1311   -1.18      0.1266   -0.3628    0.879     0.2108\n",
      "  1.037     0.4092  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  3.8529124618382,\n",
       "  3.879822890301186,\n",
       "  4.110205347652384,\n",
       "  4.330550049048636],\n",
       " ['carro', 'veículo', 'caminhão', 'motorista', 'moto'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "kdtree_file = \"kdtree.pt.p\"\n",
    "dict_embedding = get_embedding(str_dataset)\n",
    "kdtree = KDTreeEmbedding(dict_embedding, kdtree_file)\n",
    "kdtree.get_most_similar_embedding(\"carro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Atividade 5 - 💞 apresentando as analogias 💞:** Agora você deverá implementar o método `analogia` da classe `Analogy` que deverá utilizar os métodos `calcula_embedding_analogia` e o `get_most_similar_embedding` para retornar as 4 palavras mais prováveis para completar uma determinada analogia, com os parametros indicados. Caso, dentre as 4 palavras, haja uma palavra dos parametro de entrada, a mesma pode ser excluída, retorando menos palavras. Por exemplo, considerando \"**rei** está para **rainha** assim como **homem** está para...\" uma caso uma das palavras de saída para essaa entrada  seja `rainha`, o método poderá retornar 3 palavras (eliminando a palavra rainha). Isso já é considerado no método `get_most_similar_embedding`. Lembre-se que o método `get_most_similar_embedding` é da classe KDTreeEmbedding e a `Analogy`possui o atributo `kdtree_embedding` que é uma instancia da classe `KDTreeEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-2. -1. -1.]\n",
      "query_embedding = [12.41 64.8  32.28]\n",
      "query_embedding = [-10.97 -30.9   -9.6 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Veja as analogias (brinque a vontantade com a representação em português e em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n",
      "brasil está para brasilia assim como...\n",
      "query_embedding = [ 0.1177   -0.2874    1.219     0.8447   -0.009766  0.05652   0.517\n",
      "  0.042     0.76      0.95     -0.6807   -0.3804   -0.622    -1.079\n",
      " -1.128     0.311     0.2676    0.4448   -0.1426   -1.926     0.4478\n",
      " -0.2172    1.011     0.5557   -0.00537   0.144    -0.8345    0.8228\n",
      " -0.293    -0.5664   -0.1392    0.682     1.05     -0.2227   -0.8467\n",
      "  0.9463   -0.2341   -0.2783   -0.08777   0.0747    0.4404   -0.714\n",
      "  0.442    -0.0752   -0.0702   -0.2207    0.1943   -0.07776   1.261\n",
      "  0.537     0.74      0.2769   -0.327    -0.7295    0.617     0.9014\n",
      "  0.5522   -1.152    -0.204    -0.6187   -0.2654    0.5674   -0.4832\n",
      "  1.115     0.357    -1.003     0.021    -0.1138   -1.029    -1.777\n",
      " -0.4104    1.109    -0.8105    1.293    -1.162    -0.1377   -0.2908\n",
      " -0.7793    0.3574   -0.62      0.03906   0.8525   -0.5347    0.4727\n",
      " -1.337     1.632    -0.455     0.275     0.46      0.1873   -0.0946\n",
      " -0.54     -0.02734   0.2727    0.4336   -0.06934  -0.1558    0.1649\n",
      " -2.033     0.1399  ]\n",
      "\tperu está para aneura (ou ['trong', 'lacrima', 'vorskla'])\n",
      "query_embedding = [ 5.6641e-01 -6.8066e-01  1.2676e+00  1.3125e+00  1.8506e-01 -4.3896e-01\n",
      "  5.0439e-01  9.2334e-01  7.4805e-01  8.4180e-01 -7.1826e-01  3.6377e-02\n",
      " -1.7407e-01 -9.9854e-02 -1.8604e+00  8.7793e-01  3.1006e-01  5.5371e-01\n",
      "  2.4316e-01 -1.4297e+00 -1.7017e-01  1.2402e+00  5.0293e-01  6.6113e-01\n",
      "  4.3335e-01  2.6270e-01 -3.9648e-01  1.2646e+00 -6.5625e-01 -6.2598e-01\n",
      "  5.4102e-01  8.5156e-01  1.0925e-01 -1.9336e-01  1.5576e-01  7.4951e-01\n",
      " -3.0029e-01  2.2742e-01  9.3262e-01 -9.9121e-01 -2.9663e-01 -2.2131e-01\n",
      "  7.2656e-01 -2.3193e-01 -2.8076e-03 -7.4951e-01  1.9775e-01 -4.4287e-01\n",
      "  8.7598e-01  5.7129e-01  5.7520e-01  1.2915e-01  1.0615e+00 -1.7725e-01\n",
      "  6.5979e-02  9.5801e-01  3.5913e-01 -9.5996e-01 -2.4951e-01  8.9966e-02\n",
      " -7.2559e-01  6.0352e-01 -4.9976e-01  7.1191e-01  8.9722e-02 -1.1934e+00\n",
      "  1.3281e-01 -4.1162e-01 -4.1504e-01 -1.4746e+00 -3.2690e-01  8.1445e-01\n",
      " -5.8154e-01  1.3320e+00 -1.8760e+00  2.7710e-01  1.7090e-03 -9.4336e-01\n",
      "  1.2549e-01 -1.6528e-01  9.3262e-02  5.2734e-01 -8.9404e-01  7.2876e-02\n",
      " -1.0059e+00  7.6318e-01 -3.9600e-01 -6.4648e-01  1.7993e-01 -4.8242e-01\n",
      " -4.9536e-01 -1.6133e+00 -1.0244e+00  9.2871e-01 -2.4036e-01  5.5908e-01\n",
      " -1.8213e-01 -1.1536e-01 -2.3711e+00  3.2153e-01]\n",
      "\tgana está para aneura (ou ['seraing', 'fene', 'dewsbury', 'gria'])\n",
      "query_embedding = [ 0.3826  -0.9106   0.2405   1.083    0.4082  -0.1765   0.716    0.3809\n",
      "  0.3928   0.693   -0.4175  -0.0781  -0.573   -0.04468  0.2006   0.7744\n",
      "  0.3438   1.23    -0.093   -1.232   -0.624   -0.1322   1.074    0.4011\n",
      "  0.3152   0.627    0.02734  1.081   -1.252   -0.8096   0.59    -0.7183\n",
      "  0.6797  -0.1968  -0.662    1.285   -0.268   -0.1842   0.6465   0.375\n",
      " -0.312   -0.6416   0.4644   0.7734  -0.1793  -0.927    1.202   -0.2175\n",
      "  0.462    0.6445   0.04095 -0.4612   0.2747  -0.509    0.425    0.3901\n",
      "  0.0827  -0.735   -0.0787  -0.828   -0.3406   0.319    0.3892   0.656\n",
      "  0.1963  -0.514    0.397   -0.1267  -0.3765  -1.593    0.3162   0.9863\n",
      " -0.2622   0.1123  -0.8467   0.2163  -0.472   -0.2693   0.5684  -0.6006\n",
      "  0.2495   0.09607 -0.637   -0.1227  -1.213    0.349   -0.06433  0.03442\n",
      "  0.6514   0.4436   0.7793  -0.701   -0.4707   0.473    0.05066 -0.2007\n",
      " -0.2109   0.2059  -0.3188   0.02832]\n",
      "\tjapão está para yeonpyeong (ou ['lieja', 'pottstown', 'belém-pa', 'flexi'])\n",
      "query_embedding = [ 2.7246e-01 -9.0137e-01  1.0195e+00  5.8301e-01  7.4414e-01  4.1138e-01\n",
      "  4.6606e-01  1.7773e-01  8.1104e-01  5.6641e-01 -4.4312e-01 -4.1528e-01\n",
      " -1.5010e+00 -9.0430e-01 -2.8711e-01  2.3584e-01  1.6895e-01  2.8809e-02\n",
      " -1.1484e+00 -7.9785e-01 -3.4912e-02 -7.0361e-01  7.5049e-01  1.1543e+00\n",
      "  1.2393e+00  6.7969e-01 -7.9688e-01  7.0117e-01 -4.5239e-01 -8.6914e-01\n",
      "  4.5776e-01 -2.6221e-01  8.0664e-01  6.3379e-01 -9.4336e-01  1.2422e+00\n",
      " -4.6484e-01 -1.1267e-01  7.7637e-01  7.3975e-02  2.9883e-01 -7.4268e-01\n",
      " -1.2415e-01  3.6328e-01 -7.1777e-01 -9.7852e-01 -9.5093e-02  5.7434e-02\n",
      "  4.4043e-01  8.9551e-01  4.0088e-01  2.7637e-01 -2.2095e-01 -8.4082e-01\n",
      "  6.4844e-01  5.2832e-01 -3.8391e-02 -1.1182e+00 -1.8335e-01 -2.8003e-01\n",
      "  3.9331e-01  8.0957e-01 -7.2656e-01  1.1777e+00 -2.2791e-01 -1.7822e-01\n",
      " -3.0469e-01 -1.5540e-01 -2.6123e-01 -1.3535e+00 -2.4878e-01  4.6631e-01\n",
      "  2.3352e-01  1.4980e+00 -2.3999e-01  1.1523e-01 -6.8262e-01 -6.0156e-01\n",
      "  7.7588e-01 -8.8477e-01  4.8828e-03  3.4619e-01 -3.8745e-01  3.3691e-01\n",
      " -1.7695e+00  1.0225e+00 -3.1104e-01 -2.0972e-01 -4.0649e-02  4.8608e-01\n",
      " -4.3701e-01 -5.3223e-01 -1.0459e+00  6.1426e-01  1.5149e-01  5.7861e-01\n",
      " -8.1152e-01  9.6802e-02 -1.0254e-02  4.8828e-04]\n",
      "\tespanha está para logroño (ou ['valladolid', 'kalapa', 'cádiz', 'baiona'])\n",
      "query_embedding = [-0.3987   -0.9663    0.2698    1.296     0.749     0.176     0.561\n",
      " -0.05176   0.871     0.436    -0.404    -1.037    -0.631     0.1755\n",
      " -1.239     0.975     0.4673    1.094     0.09326  -1.354    -0.7085\n",
      "  0.59      0.9966    0.9033   -0.3513    0.8823   -0.538     1.126\n",
      " -1.041    -1.287     0.238    -0.3125    0.631    -0.705    -0.4365\n",
      "  1.883    -0.2029   -0.397     0.6943    0.5894   -0.331    -1.12\n",
      " -0.2988   -0.4346   -0.745    -0.7446   -0.2815   -0.8853   -0.3618\n",
      "  0.2415    1.114     0.10596  -0.0415   -1.129     0.4958    0.749\n",
      " -0.7427   -0.691    -0.4592   -0.7925    0.431     0.782    -0.002808\n",
      "  0.6484   -0.2769   -0.3274   -0.1079   -1.167    -0.01416  -0.9365\n",
      "  0.2395    0.5312   -0.1202    1.168    -1.842    -0.08057  -0.394\n",
      " -0.448     0.1982   -0.8477   -0.274     0.31     -0.02744   0.47\n",
      " -0.7876    1.412    -0.5073    0.963     0.548     0.482     0.571\n",
      " -1.614    -0.6177    0.3254   -0.3516    0.06104  -0.633    -0.08167\n",
      " -2.621    -0.02246 ]\n",
      "\tindia está para vjm00 (ou ['mailman', 'excursion', 'nsi', 'clementia'])\n",
      "bahia está para salvador assim como...\n",
      "query_embedding = [-0.595     0.3345    1.1875   -0.77      0.01611  -0.1431    0.4097\n",
      " -0.5127   -0.9316   -0.07166   0.1263    0.1499   -0.2544   -0.1892\n",
      " -0.3306   -1.601     0.3523    0.77      0.5137   -0.2153   -0.3062\n",
      " -0.8623   -0.1641   -0.7153   -0.4673    0.05908  -0.4775    0.4517\n",
      " -0.6787   -0.10156  -0.302     0.2634    0.27      1.309     0.3262\n",
      "  0.4922   -0.2068    0.2067    0.3267   -0.1537    0.406    -0.615\n",
      "  0.0426   -0.803     0.833     0.2042   -0.1282    0.437     0.712\n",
      " -0.2651    0.2551    0.3877    0.03967  -0.2031   -0.02466  -0.4248\n",
      " -0.7915   -0.9536   -0.548    -0.2422   -0.8774   -0.716     0.7627\n",
      "  0.01099   0.2115   -0.2607    0.2463    0.5435   -0.4673   -0.2053\n",
      " -0.417     0.03687  -0.03748   1.2705    0.669    -0.535    -0.224\n",
      " -0.2754   -0.5166   -0.05823   0.829     0.002586  0.4468   -0.0464\n",
      " -0.4817   -0.157    -0.1453    0.319     0.674    -0.2378   -0.12256\n",
      " -0.6562   -0.115     0.5713    0.11084  -0.564     0.068     0.2059\n",
      "  0.947     0.631   ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tacre está para macapá (ou ['aracaju', 'cuiabá'])\n",
      "query_embedding = [-0.175     0.399     1.376    -0.8423    0.4888   -0.1792    0.07495\n",
      " -0.334    -0.484    -0.0753    0.3533   -0.09076  -0.02441   0.2034\n",
      " -0.2769   -1.188     0.3892    0.3206    0.4094    0.293     0.794\n",
      " -0.701    -0.518    -0.5596   -0.669     0.3115   -0.863     0.3772\n",
      " -0.4      -0.296    -0.312     0.2009   -0.208     1.406     0.3047\n",
      "  0.2002    0.3872    0.314     0.4272    0.01354   0.2678   -0.315\n",
      "  0.07446  -0.7573    0.6973    0.1451   -0.3193    0.832     1.052\n",
      " -0.0337   -0.3298    0.2544    0.455    -0.2737   -0.02466  -0.8193\n",
      " -0.5166   -0.73     -0.315    -0.4172   -0.533    -0.6562    0.2339\n",
      " -0.1203   -0.1552   -0.288    -0.08057   0.2861   -0.3945   -0.5635\n",
      " -0.01074   0.1943    0.0058    1.22      0.4126   -0.7793   -0.5186\n",
      " -0.4307   -0.8755    0.1415    0.2484   -0.0648    0.4314   -0.1328\n",
      " -0.1986   -0.3582   -0.1033    0.8735    0.868    -0.0445   -0.586\n",
      " -0.4248   -0.09247   0.691     0.2585   -0.672     0.1648    0.015015\n",
      "  1.006     0.2566  ]\n",
      "\talagoas está para aracaju (ou ['maceió', 'teresina'])\n",
      "query_embedding = [-4.0088e-01  4.8096e-01  1.4014e+00 -1.9043e-01  2.0386e-01 -5.1807e-01\n",
      "  1.5527e-01 -2.9590e-01 -9.6387e-01 -6.3330e-01  1.0071e-01 -1.3525e-01\n",
      " -1.9092e-01 -1.1011e-01 -4.1113e-01 -1.2969e+00  4.9902e-01 -1.2524e-01\n",
      "  1.1025e+00 -3.4912e-01 -1.9397e-01 -7.9004e-01 -5.4834e-01 -2.0215e-01\n",
      " -3.7402e-01  6.3184e-01 -4.2456e-01  4.8218e-01 -4.9243e-01 -2.1387e-01\n",
      "  1.9983e-01  4.7705e-01  5.2734e-02  1.1045e+00  3.1006e-01  1.0760e-01\n",
      " -1.5063e-01  7.1729e-01  7.3730e-02 -5.9692e-02  4.6118e-01 -7.4854e-01\n",
      " -9.2651e-02 -6.6162e-01  1.0918e+00  4.8096e-02 -3.1958e-01  1.3184e-01\n",
      "  6.4941e-01 -4.5654e-02  9.2529e-02  7.0435e-02  4.0967e-01 -1.5723e-01\n",
      " -3.9673e-01 -1.0723e+00 -6.7529e-01 -9.6387e-01 -3.3325e-01 -1.7090e-01\n",
      " -1.1670e+00 -8.0127e-01  1.1902e-03  1.9238e-01 -3.6621e-03 -2.6514e-01\n",
      "  4.7388e-01  8.7402e-01 -6.9873e-01 -6.1768e-01 -1.6626e-01  3.7842e-01\n",
      " -4.6692e-02  1.1562e+00  5.9082e-01 -8.6084e-01  3.9307e-02 -2.4805e-01\n",
      " -7.6855e-01  5.0659e-02  2.2461e-02  1.2402e-01  6.5332e-01  2.5854e-01\n",
      " -4.8071e-01 -2.5391e-02  3.3813e-02  8.3496e-01  8.3008e-01 -3.3887e-01\n",
      " -2.4121e-01 -6.5283e-01 -9.0454e-02  7.8467e-01  2.1265e-01 -5.8594e-01\n",
      "  3.0884e-01 -6.2061e-01  1.2422e+00  3.7500e-01]\n",
      "\tamapá está para macapá (ou ['amazonas', 'xinguara'])\n",
      "query_embedding = [-0.5547   0.3486   1.097   -0.6904   0.4573  -0.03223  0.2397  -0.537\n",
      " -0.6943  -0.2847   0.2844   0.196   -0.06885 -0.5166   0.0834  -1.468\n",
      "  0.8384   0.4749   0.6865  -0.2261   0.1448  -0.927   -0.5063  -1.153\n",
      " -0.731    0.355   -0.741    0.722   -0.6133  -0.07764 -0.3242   0.3335\n",
      "  0.0381   1.646    0.01514  0.2537  -0.1736   0.03075  0.09375  0.2296\n",
      "  0.35    -0.678    0.09686 -0.73     0.6494  -0.1931  -0.57     0.8447\n",
      "  0.548   -0.4434  -0.3264   0.1128   0.563   -0.2229  -0.2732  -0.481\n",
      " -0.541   -0.822    0.0437  -0.1792  -1.008   -0.725    0.0901  -0.12494\n",
      " -0.4707  -0.1499   0.587    0.4258  -0.588   -0.5645   0.06226  0.4785\n",
      " -0.2234   1.091    0.847   -0.4978  -0.1501  -0.1587  -1.008    0.11365\n",
      "  0.1624   0.3027   0.8516  -0.03564 -0.1796   0.324    0.3853   0.332\n",
      "  1.02    -0.0561  -0.5547  -0.2495  -0.4287   0.7217   0.671   -0.3577\n",
      "  0.5684  -0.2502   1.5      0.6987 ]\n",
      "\tamazonas está para maceió (ou ['aracaju', 'macapá'])\n",
      "query_embedding = [-0.3447   0.11334  1.363   -1.239    0.509    0.2446  -0.05078 -0.8438\n",
      " -0.2396  -0.3647   0.2783   0.1156  -0.1597  -0.00806 -0.14    -1.078\n",
      "  0.4084   0.356    0.5254  -0.09033  0.1504  -0.913   -0.773   -0.877\n",
      " -0.3208   0.371   -0.3691   0.4512   0.1301   0.4502   0.2808   0.297\n",
      " -0.2231   0.9517   0.4014   0.09125 -0.0359   0.1537  -0.07324  0.2932\n",
      "  0.5303   0.1261   0.04596 -0.624    0.9756  -0.01727 -0.458    0.623\n",
      "  0.923   -0.3564  -0.2037   0.04224  0.3296   0.1736  -0.1809  -0.6704\n",
      " -0.634   -0.3643  -0.7183   0.2241  -0.3896  -0.4897  -0.128   -0.2695\n",
      " -0.3638  -0.1011  -0.1533   0.1273  -0.778   -0.5835  -0.133    0.5283\n",
      "  0.2048   0.92     0.6055  -0.914    0.12244 -0.359   -0.271    0.0365\n",
      "  0.3132  -0.01205  0.4092  -0.6753  -0.335   -0.4788   0.0664   0.6587\n",
      "  0.651   -0.4385  -0.9546  -0.1333  -0.43     0.6196   0.03833 -0.5586\n",
      "  0.2399  -0.222    1.166    0.067  ]\n",
      "\tceará está para maceió (ou ['cuiabá', 'aracaju', 'recife'])\n",
      "query_embedding = [-0.7104   -0.00701   1.3      -1.258     0.3325    0.04736   0.06665\n",
      " -1.127    -0.2769    0.09985   0.1895   -0.0756    0.1621    0.02026\n",
      " -0.3408   -1.386     0.532     0.3926    0.1409    0.1255    0.1973\n",
      " -1.135    -0.3198   -0.4473   -0.1316   -0.2983   -0.3381   -0.1091\n",
      " -0.578     0.27      0.0625    0.438    -0.167     1.098     0.1946\n",
      "  0.5874   -0.522     0.1205   -0.1309   -0.128     0.6123    0.2778\n",
      "  0.04086  -0.598     0.632     0.07056  -0.2754    0.519     1.077\n",
      "  0.1443   -0.199     0.2625    0.553     0.02911  -0.452    -0.458\n",
      " -0.657    -0.4087   -0.7983    0.2217   -0.7334   -0.5015    0.007294\n",
      " -0.08124  -0.003052 -0.2988   -0.54      0.544    -0.9404   -0.692\n",
      "  0.0698    0.4614    0.002747  0.786     0.64     -0.5483   -0.1272\n",
      " -0.3381   -0.4814    0.2783   -0.02661   0.1335    0.3445    0.0691\n",
      " -0.2651   -0.539    -0.4744    0.774     0.851    -0.3613   -0.485\n",
      " -0.08057   0.1844    0.8726   -0.0996   -0.643    -0.1282   -0.1526\n",
      "  1.037     0.1709  ]\n",
      "\tgoiás está para goiânia (ou ['cuiabá', 'aracaju', 'macapá'])\n",
      "brasil está para feijoada assim como...\n",
      "query_embedding = [-0.09937  -1.17      0.8896    1.022     0.858     0.788    -0.083\n",
      "  0.6587   -0.1299   -0.013916 -0.5186    0.0718    0.0648   -0.3936\n",
      " -1.076     0.5737   -0.4028   -0.1721   -1.053    -1.288    -0.758\n",
      "  0.07367   0.0564    1.597     0.1348    0.8096    0.285     0.782\n",
      " -0.9595   -1.454    -0.4297    0.239     0.706    -0.665    -1.029\n",
      "  1.08      0.01978  -0.542     0.3743    0.4746   -0.448    -1.531\n",
      " -0.04724  -0.6865   -1.421    -0.3496   -1.144    -0.544     0.2727\n",
      "  0.2451    1.159     1.006     0.744    -0.339    -0.3022    0.6777\n",
      " -0.201    -1.031     0.4844   -0.2583    0.955     0.3147   -0.4783\n",
      " -0.2172   -0.11743  -0.7295    1.447    -0.706     0.602    -0.7363\n",
      "  0.1494    1.234    -0.546     0.835    -1.861    -0.073     0.6514\n",
      " -0.1936   -0.2251   -0.8794   -0.0217    0.3333   -0.983     0.69\n",
      " -0.761     0.7363    1.2705    0.0759    0.05005  -0.3762    0.7686\n",
      " -0.9595   -0.3972   -0.372     0.8154    1.377     0.1934   -0.444\n",
      " -1.479     0.659   ]\n",
      "\titalia está para esparguete (ou ['via-crúcis', 'negundo', 'pana', 'taxifolia'])\n",
      "query_embedding = [ 0.732   -1.168    0.825    1.622    0.2588   0.3003  -0.4365   2.012\n",
      "  0.3098   0.2576  -0.3757   0.639   -0.5117  -0.992   -1.34     0.03662\n",
      " -0.5537   0.2275  -0.959   -0.724   -0.8135   0.6445   0.405    1.377\n",
      " -0.509    0.3755   0.85     1.091   -0.4138  -1.316   -0.4106   0.4402\n",
      "  0.9746  -0.1997  -0.735    1.078    0.756   -0.0598   0.914    1.293\n",
      "  0.03735 -1.169    0.92    -0.1013   0.063   -0.3904  -0.3396  -0.812\n",
      " -0.37     0.2461   1.043   -0.285    0.674    0.0929   0.3564   0.8687\n",
      "  0.295   -0.5874   0.616   -1.157    0.9473   0.505   -0.1957   0.4058\n",
      "  0.5864  -0.4302   1.473   -0.1882   0.6895  -0.5684   0.03955  0.966\n",
      " -0.786    0.2842  -2.611   -0.327    0.05005  0.0884  -0.334   -0.907\n",
      "  0.3865   0.773   -2.178   -0.1494   0.1233   0.4104   0.635    0.02722\n",
      " -0.546   -0.0249   0.4204  -1.87    -0.5396   0.449    0.2942   1.242\n",
      "  0.1729  -0.675   -2.367   -0.1643 ]\n",
      "\testados-unidos está para cebolada (ou ['portugas', 'atribuindo-os', 'bróculos', 'surtida'])\n",
      "query_embedding = [ 0.811   -1.662    0.4192   0.042   -0.0897   1.447    0.1719   1.014\n",
      "  0.763    0.4817  -0.2827   0.9253  -1.18    -0.503   -1.07     0.7183\n",
      " -0.841   -0.6543  -1.338    0.1091  -0.7495   0.4644   0.2844   0.4094\n",
      "  0.1553  -0.3608   0.3984   0.739    0.1543  -0.914    0.03613  0.1699\n",
      "  0.998   -0.4976  -0.0753   0.4146   0.5664   0.3096   1.099    0.6016\n",
      "  0.634   -0.2988  -0.2744  -0.1958   0.6143  -0.374   -0.4607  -1.42\n",
      "  0.4077   0.2324   0.8135   0.7227   0.9863  -0.12476  0.2344   0.7505\n",
      "  0.596   -0.744    0.6377  -0.5303   0.2281   0.3047  -0.823    0.1875\n",
      " -0.6504  -0.2235   1.77    -0.9917   0.5527  -0.5425  -0.7544   0.8677\n",
      " -0.2021   0.8623  -1.388   -0.508    0.5234   0.0786  -0.8926  -0.5986\n",
      "  1.16    -0.1638  -1.74    -0.3894   0.6333   0.0989   0.421   -0.7734\n",
      " -0.5693  -0.09204  0.1267  -0.666   -0.2515   0.1626  -0.749    1.445\n",
      " -0.11914  0.431   -0.06335  0.06616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tinglaterra está para worcestershire (ou ['falmouth', 'lincolnshire', 'lowestoft'])\n",
      "query_embedding = [-0.4597   -0.9297    0.9194   -0.509     0.392     0.717    -0.67\n",
      "  0.796     0.4902   -0.07117  -0.2119    0.4614   -0.896    -0.875\n",
      " -0.7817   -0.611     0.02527  -0.276    -1.077    -0.9185   -0.3535\n",
      " -0.01569   0.1619    0.5234   -0.1855   -0.1272    0.2015    1.486\n",
      " -0.4736   -0.625    -1.227     0.644     0.3757   -0.1997   -0.4465\n",
      "  0.8965   -0.278    -0.347    -0.1218    0.66      0.342    -0.708\n",
      "  0.3774   -0.5356    0.013916 -0.0525   -0.9355   -0.6807    1.709\n",
      "  0.291     1.555     0.545     0.9443    0.1888    0.188     0.7803\n",
      "  0.2563   -0.8965    0.689    -1.025     0.4292    0.453    -0.5537\n",
      "  0.4712    0.663    -0.03638   0.585    -0.1138    0.1472   -0.61\n",
      " -1.03      0.614    -1.1455   -0.1733   -1.501     0.1597    0.5273\n",
      "  0.4207   -0.78     -0.65      0.1405    1.133    -1.425    -0.1387\n",
      " -0.2012    0.945     0.2974   -0.2334   -0.9717   -0.4946    0.504\n",
      " -0.4106    0.508     0.0735    0.3203    0.7554   -0.1953    0.2776\n",
      " -0.163     0.4148  ]\n",
      "\targentina está para retrete (ou ['esparguete', 'carnico', 'frita'])\n",
      "query_embedding = [-0.2126  -0.96     1.184    0.1289  -0.1488   0.5312  -0.0762   0.8486\n",
      "  0.2377   0.3992  -0.536    0.7505  -0.8706  -1.344   -1.086   -0.2817\n",
      " -0.3677  -0.225   -0.4216  -1.336    0.06226 -0.086    0.579    0.7256\n",
      " -0.4353  -0.7827   0.4185   1.456    0.1157  -0.698   -0.9014   1.144\n",
      "  1.286   -0.5107  -0.5234   0.6777  -0.02051 -0.358   -0.2742   0.658\n",
      "  0.6895  -0.997    0.8145  -0.716    0.3845   0.2305  -0.552   -0.78\n",
      "  1.229    0.07764  1.593    0.6006   0.1372   0.2458   0.063    0.925\n",
      "  0.627   -1.196    0.5747  -0.7646   0.4048   0.2117  -0.662    0.9683\n",
      "  0.5225  -0.625    1.232   -0.2004  -0.3596  -1.293   -0.8467   1.413\n",
      " -1.242    0.847   -1.449   -0.5654   0.703   -0.0979  -1.013   -0.714\n",
      "  0.4065   0.9004  -1.768    0.02002 -0.7065   1.354    0.3164  -0.06384\n",
      " -0.1812  -0.591    0.176   -0.1608   0.4495  -0.01099  0.629    0.8438\n",
      "  0.07275 -0.1357  -1.503    0.2957 ]\n",
      "\tperu está para frita (ou ['molho', 'guisado', 'assado'])\n",
      "homem está para mulher assim como...\n",
      "query_embedding = [-0.3313   0.2406  -0.5703  -0.888    0.3142  -0.712   -0.5586  -0.662\n",
      "  0.3777  -0.504   -0.1802   0.1907  -0.0752  -0.6606   0.3755   0.1431\n",
      "  0.1309  -0.2507  -0.6484   1.273    0.06775  0.3447   0.3428   0.9478\n",
      " -0.713   -0.1963  -0.505   -0.5376  -0.4302   0.8994   0.3716   0.1096\n",
      "  0.09973  0.4434   0.06836 -0.6523   0.4402  -0.2432  -1.422    0.5093\n",
      "  0.06396 -0.377   -0.948   -1.133    0.02954  0.672    0.2252  -0.276\n",
      "  1.555    0.2402   0.619    0.1481   1.025   -0.951   -0.956   -0.2815\n",
      "  0.0371  -0.552    0.04688 -0.01733 -0.6006   0.9233  -0.396   -0.44\n",
      " -0.8193  -0.1189   0.1879   0.366    0.4695   0.7173  -0.2822  -0.12463\n",
      " -1.038   -0.3728   0.04883 -0.2986   1.151   -0.617    0.24     0.4236\n",
      "  0.419   -1.965    0.296   -0.3867   0.614   -0.01978  0.02747  0.8486\n",
      " -0.4985   0.1167  -0.5493   0.4375   0.1929  -0.425    0.2249  -0.559\n",
      " -0.3552   0.6265   0.3115   0.09326]\n",
      "\tgaroto está para menina (ou ['garota', 'namorada', 'mãe', 'irmã'])\n",
      "query_embedding = [-3.2446e-01  1.2207e-02 -1.0693e-01 -1.0996e+00 -8.7891e-03  7.3438e-01\n",
      "  1.2178e+00 -1.1855e+00 -5.4883e-01 -6.1768e-02  2.4414e-04 -5.6348e-01\n",
      " -1.4248e+00 -4.8828e-03  4.1650e-01  1.5674e-01  5.1709e-01 -1.1904e+00\n",
      " -1.1992e+00  1.8711e+00 -5.0732e-01 -9.6875e-01  7.7539e-01  1.9189e-01\n",
      "  3.3691e-01  4.3896e-01 -1.0215e+00  2.5879e-01 -1.9360e-01 -5.9473e-01\n",
      "  6.0742e-01 -5.4102e-01  4.8730e-01  1.2295e+00 -3.8257e-01 -4.2139e-01\n",
      "  3.6621e-01 -5.0098e-01 -3.3301e-01 -5.6641e-02 -2.6465e-01  4.0771e-01\n",
      " -5.9668e-01 -8.2422e-01  3.8867e-01 -2.3401e-01 -1.0039e+00  5.7617e-01\n",
      "  6.5479e-01 -1.4551e-01 -5.8350e-01  1.8457e+00  4.8438e-01  1.4136e-01\n",
      " -6.9531e-01 -3.6060e-01  3.5107e-01 -6.9336e-01 -3.8770e-01 -2.8174e-01\n",
      " -6.6406e-01  5.6055e-01  1.6309e-01  4.8438e-01 -1.8857e+00  5.5078e-01\n",
      "  1.3857e+00  2.5537e-01  1.1133e+00  5.7617e-01 -5.2979e-01  1.8359e-01\n",
      " -1.5430e-01  5.1074e-01  2.5000e-01 -2.2559e-01  5.4980e-01 -5.8789e-01\n",
      "  5.7617e-02  1.2520e+00  1.1328e+00 -2.1523e+00 -1.8616e-01 -6.3574e-01\n",
      "  4.0820e-01 -3.5498e-01  5.2686e-01 -6.3281e-01 -5.3613e-01 -8.1396e-01\n",
      " -4.2017e-01  5.6396e-01  5.0293e-02 -1.3550e-01  5.0195e-01  4.8828e-04\n",
      " -7.0117e-01  1.0498e+00  1.7148e+00  1.0869e+00]\n",
      "\trei está para rainha (ou ['princesa', 'esposa', 'príncipe'])\n",
      "query_embedding = [-0.4932   0.2357  -0.1216  -0.9287   0.05322  0.0105   0.627   -1.093\n",
      " -0.3777  -0.1663  -0.2998  -0.7207  -1.292   -0.7227   0.1389   0.6987\n",
      "  0.3296  -0.4792  -1.132    1.195   -0.919   -0.2427   0.625    0.3672\n",
      "  0.169    0.0459  -1.443    0.2734  -0.3809  -0.3965   0.5317  -0.787\n",
      "  0.05426  1.024    0.476    0.3809  -0.2764  -0.676   -0.5713  -0.4507\n",
      " -0.4912   0.0626  -0.2812  -1.084    0.992   -0.2703  -0.4722   0.595\n",
      "  1.125   -0.03125 -0.27     1.621    1.012   -0.5396  -0.782   -0.00232\n",
      "  0.395   -0.6855   0.1487   0.0559  -1.101    1.291   -0.1113   0.04272\n",
      " -1.767    0.618    0.7764   0.4182   0.72     1.092   -0.4917  -0.1287\n",
      " -0.6406   0.408   -0.1318   0.271    1.254   -0.2473  -0.2798   1.057\n",
      "  1.435   -1.823    0.6455  -0.3691  -0.1162  -0.02246  0.546   -0.2334\n",
      " -0.1123  -0.611   -0.942    0.11816 -0.3423   0.1586   0.701    0.317\n",
      " -1.015    0.8906   1.359    1.132  ]\n",
      "\tpríncipe está para princesa (ou ['rainha', 'filha', 'esposa'])\n",
      "query_embedding = [-0.967   -0.01294 -0.4692  -1.151    0.2461   0.1418  -0.1592  -1.008\n",
      "  0.07227 -0.1931  -0.1199  -0.6797  -0.9946  -0.255    0.5186   0.3008\n",
      "  0.3848  -0.2854  -1.208    1.71     0.10046 -0.1001   0.6284   0.2954\n",
      " -0.2944  -0.1362  -0.996   -0.7334  -0.1255   0.478    0.652   -0.11743\n",
      "  0.07355  1.123   -0.0769  -0.379    0.947   -0.7217  -1.205    0.261\n",
      " -0.4766   0.07556 -0.6553  -0.983    0.4248   0.7705  -0.2734  -0.2426\n",
      "  1.32    -0.01196  0.1833   0.6875   1.368   -0.622   -0.955   -0.098\n",
      " -0.3384  -0.5527   0.486   -0.4036  -0.99     1.23    -0.3662   0.05542\n",
      " -1.468    0.1321   0.4104   0.715    1.198    0.6714  -0.2249  -0.131\n",
      " -0.5376  -0.2073   0.8574  -0.5117   1.197   -0.3545  -0.2136   0.2368\n",
      "  0.6914  -2.975    0.5977  -0.2544   1.105   -0.0681   0.7656   0.557\n",
      " -0.0703  -0.4985  -0.5312   0.3594   0.2874   0.2603   0.712   -0.3828\n",
      " -0.2852   0.8525   1.201   -0.147  ]\n",
      "\tpai está para filha (ou ['mãe', 'esposa', 'irmã', 'marido'])\n",
      "query_embedding = [-0.19     -0.1696   -0.405    -0.931     0.917    -0.05444   0.10376\n",
      " -1.123     0.2124   -0.282    -0.003174  0.2117   -0.4407   -0.794\n",
      "  0.7905   -0.487     0.2751   -0.7       0.06964   0.7793   -0.1959\n",
      " -0.1777    0.2114   -0.01611  -0.05518   0.0791   -0.1787    0.498\n",
      " -0.8      -0.1299    0.527     0.3884    0.6797    0.2285   -0.3926\n",
      " -0.3362    0.6445    0.07544  -0.5884    1.432    -0.3843   -0.01721\n",
      " -0.42     -0.3467   -0.546     0.444    -0.1229   -0.483     0.3662\n",
      " -0.124     0.337     0.6846    1.156    -0.2024   -1.272    -0.44\n",
      "  0.165    -0.7725    0.1532   -1.275     0.08435   0.699    -1.285\n",
      "  0.01776  -0.865     0.008545  0.8926    0.2505    0.9277    0.599\n",
      " -0.958    -0.3833   -0.1697    0.0996    0.67     -0.646     0.54\n",
      " -0.7407    0.1555    0.9883    0.3997   -1.035     0.2147   -0.5293\n",
      "  0.1677    0.0166    0.3315    0.582    -0.1382    0.3257    0.1549\n",
      "  0.4795   -0.10547  -0.2468    0.5713   -0.01904   0.00659   0.2095\n",
      "  0.1846    1.131   ]\n",
      "\tcavalo está para dama (ou ['égua', 'carruagem', 'irmã'])\n",
      "query_embedding = [ 0.0845  -0.3325  -0.1477  -0.1396   0.00903 -0.5283  -0.04712 -0.884\n",
      "  0.0515  -0.1394  -0.3801  -1.029    0.09717 -0.501   -0.27    -0.289\n",
      " -0.1475   0.07275  0.0965   0.2188  -0.01016 -0.1294  -0.3047   0.841\n",
      " -0.01855  0.562   -1.074    0.1807  -0.0718  -0.6465  -0.601   -0.2686\n",
      "  0.1531   0.01855 -0.2249   0.6445   0.294    0.2258  -0.2412  -0.1343\n",
      " -0.962   -0.8      0.06396 -0.8135  -0.1335   0.0442  -1.496   -0.0553\n",
      "  0.8955   0.2051   0.74     0.4426   0.946   -0.521   -1.106   -0.1122\n",
      " -0.167   -1.359    0.272   -0.3276   0.0741   0.2319  -1.51    -0.03406\n",
      " -0.3748  -0.228    0.2754   0.7295   1.379    0.965   -0.4854   0.6104\n",
      " -0.4883  -0.04688 -0.76    -0.6553   0.5503  -1.053    0.2578   0.133\n",
      " -0.295   -0.3965   0.55     0.5205   0.3748   0.2477   0.7554   1.061\n",
      " -0.4902  -0.3638   0.02747 -0.768    1.004    0.4272   0.666    0.333\n",
      "  0.3118   0.2329  -0.4795   0.4424 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgarçon está para cabeleireira (ou ['edna', 'pescadora', 'elisabetha'])\n",
      "grande está para pequeno assim como...\n",
      "query_embedding = [-1.2024e-01  8.6670e-03 -6.6064e-01  1.8213e-01  8.3008e-02 -6.0791e-02\n",
      " -4.9414e-01  5.2148e-01 -1.6956e-01 -2.5586e-01 -7.3633e-01  7.9736e-01\n",
      "  2.1399e-01 -1.0625e+00  7.5293e-01  2.2266e-01 -1.9604e-01  8.3105e-01\n",
      "  2.5806e-01  5.6030e-02 -1.0078e+00  3.6084e-01  1.0088e+00  9.5459e-01\n",
      " -3.3472e-01 -1.5439e+00 -1.4417e-01  1.9043e-02  6.0889e-01  1.6328e+00\n",
      "  4.9365e-01  1.4087e-01  7.1973e-01 -3.1226e-01  8.5205e-01  4.0356e-01\n",
      " -4.7943e-02 -8.2275e-02  1.2256e-01  6.3672e-01  6.2793e-01 -6.8848e-02\n",
      "  2.7246e-01  2.9688e-01  2.4390e-01  1.1104e+00  1.7651e-01 -4.2822e-01\n",
      "  7.3975e-01 -2.0044e-01 -5.5518e-01 -7.1045e-02 -1.6821e-01 -1.2427e-01\n",
      "  7.4170e-01 -3.9160e-01  1.4082e+00 -1.1406e+00  6.1621e-01  2.8247e-01\n",
      " -2.8174e-01 -2.7686e-01 -1.5112e-01 -2.5244e-01 -1.7236e-01 -3.3740e-01\n",
      " -8.2959e-01 -7.2900e-01 -2.2095e-02 -3.8794e-01 -7.2656e-01 -4.3408e-01\n",
      " -4.5972e-01  3.5303e-01  8.7109e-01  4.8828e-04  2.1875e-01  8.5449e-02\n",
      "  6.7383e-01 -7.2168e-01  5.3955e-02 -6.5625e-01  1.0498e-01 -4.4849e-01\n",
      " -4.2261e-01 -8.1641e-01 -1.4219e+00 -3.1323e-01  4.2529e-01  1.5605e+00\n",
      "  4.3433e-01  6.2451e-01 -5.7861e-01 -9.8730e-01 -5.8838e-02 -1.4587e-02\n",
      "  9.9414e-01  1.5247e-01 -1.1631e+00  5.2539e-01]\n",
      "\tcheio está para armário (ou ['saco', 'gato', 'minúsculo'])\n",
      "query_embedding = [-0.1311  -0.2842  -0.1973  -0.2554   0.0874  -0.2288   0.3745  -0.4902\n",
      " -0.3809  -0.7124   0.08276  0.8896   0.02332 -1.2705   0.703    0.8174\n",
      "  0.12317  0.7104   0.4255  -0.0985   0.01563  0.8193  -0.1582   0.2727\n",
      " -0.825   -0.2708   0.2583   0.986    0.568    1.027    0.2588   0.133\n",
      "  1.078    0.827    0.985    0.5596  -0.6133  -0.0902  -0.3428   0.2505\n",
      " -0.3398   0.07007 -0.3022  -0.1948   0.2688   0.931    0.7656  -0.7725\n",
      "  0.4976  -0.04932 -0.509    0.0887  -0.1738  -0.1609   0.7485   0.1846\n",
      "  1.393   -1.064    0.3906   0.6943  -0.2612  -0.00879 -0.1567   0.295\n",
      " -0.5986  -0.601   -0.8667  -0.6226  -0.4966  -1.056   -0.2465  -0.7065\n",
      " -0.2308   0.0437   1.381   -0.2734  -0.494   -0.1382   0.533    0.664\n",
      " -0.0371  -0.6387   1.176   -0.1389  -0.0877   0.1157  -1.125   -0.2024\n",
      "  1.004    1.239    0.2954   0.7593  -0.636   -0.759    0.2157   0.865\n",
      "  0.8037  -0.04663  0.824    0.825  ]\n",
      "\talto está para baixo (ou ['comprido', 'redondo'])\n",
      "query_embedding = [ 0.0807   -0.813     0.11523  -0.815     0.7266   -0.8643   -0.003418\n",
      " -0.1842   -0.1488   -0.846     0.2058    0.963    -0.161    -0.528\n",
      "  0.7275    0.7617   -0.3743   -0.08923  -0.3257    0.1779   -0.5273\n",
      "  1.052     0.591     0.4644   -0.2742   -0.2832    0.3574    0.41\n",
      "  0.4639    1.36      0.4192    0.1211    0.2878    0.298     1.256\n",
      " -0.1936    0.1328   -0.1034   -0.179     0.565     0.1938    0.01416\n",
      " -0.0216    0.03748   0.3486    0.711     1.364    -0.5586    0.4573\n",
      "  0.866    -0.39      0.5635    0.00406   0.4114    0.2783   -1.132\n",
      "  1.033    -0.1948   -0.0271    0.0989    0.5947    0.1531    0.1346\n",
      "  0.08093   0.1172    0.0737   -0.2476   -0.1667   -0.4062   -0.887\n",
      " -1.029    -0.5176   -0.0791   -0.3896    1.315    -0.4      -0.553\n",
      "  0.0083    0.5435   -0.0094    0.59     -0.4707    0.8438   -0.1345\n",
      " -0.2734   -0.1572   -0.583     0.0801    0.668     0.9775    0.02832\n",
      "  1.112     0.01013  -0.4282    0.193     0.5166    0.5044    0.293\n",
      "  0.581     0.1383  ]\n",
      "\tforte está para fraco (ou ['parecido', 'baixo'])\n",
      "query_embedding = [ 3.413e-01 -5.513e-01  7.310e-01 -7.808e-01  9.243e-01  4.590e-02\n",
      "  2.734e-01 -6.382e-01 -1.556e-01 -2.179e-01 -2.205e-01  8.833e-01\n",
      "  2.622e-01 -1.314e+00  5.830e-01  3.555e-01  4.080e-01  9.863e-01\n",
      "  3.499e-01 -2.888e-01 -8.057e-01  1.092e+00  3.052e-01  9.087e-01\n",
      " -2.097e-01 -8.213e-01  1.488e-01  8.066e-01  5.034e-01  8.340e-01\n",
      "  3.152e-01  1.821e-01  1.008e+00 -2.173e-02  7.886e-01  7.734e-01\n",
      "  3.550e-01 -2.830e-01 -1.370e-01 -1.421e-01 -2.422e-01 -1.194e+00\n",
      "  3.472e-01 -9.766e-04  6.733e-01  4.702e-01  1.890e-01 -2.905e-01\n",
      "  1.018e+00  2.380e-01 -9.248e-01  4.919e-01 -7.422e-02 -3.884e-01\n",
      "  1.195e+00 -2.764e-01  1.264e+00 -9.741e-01  4.116e-01  6.514e-01\n",
      "  2.166e-01 -1.346e-01 -9.268e-01  2.401e-01 -1.084e+00 -2.090e-01\n",
      " -1.045e+00 -7.939e-01 -2.048e-01 -1.605e+00 -8.203e-01 -2.545e-02\n",
      " -4.031e-01  2.715e-01  1.244e+00 -5.439e-01  1.384e-01 -5.225e-02\n",
      "  1.370e-01 -1.410e-01 -1.140e-01  2.812e-01  8.740e-01 -4.539e-01\n",
      " -4.265e-01 -5.439e-01 -7.158e-01 -1.194e-01  7.314e-01  2.000e+00\n",
      " -2.661e-02  3.906e-02 -6.646e-01 -8.154e-02  1.133e+00  6.748e-01\n",
      "  6.245e-01 -2.905e-01 -2.861e-01  7.695e-01]\n",
      "\tlargo está para beco (ou ['fronteiro', 'comprido'])\n",
      "pelé está para futebol assim como...\n",
      "query_embedding = [-0.05103  0.4414   0.0503  -1.352    0.5176   0.734   -0.3665  -0.6953\n",
      "  0.3545  -0.5254   0.6753   0.1357  -0.9893  -0.1416   0.00537  1.023\n",
      "  0.799    0.862    0.1157  -0.10596  0.7     -0.966   -1.152   -0.712\n",
      "  0.0639  -0.4817   1.165   -0.1672  -0.2559   0.747    1.443   -0.821\n",
      " -0.03113  0.6377   0.7964   0.3018   0.5947   0.00586  0.8438  -0.5317\n",
      " -0.02197 -0.0547  -0.3662   0.5723  -1.153    0.6484   1.33    -0.929\n",
      "  0.8203   0.4792   0.9556  -0.2935   0.0387   0.2744  -0.2202   0.01883\n",
      " -0.61    -0.02075  0.3677  -1.104   -0.2253  -0.2255  -0.2837  -1.033\n",
      " -0.2006   0.0845   0.524    0.392    1.167    0.6904  -0.255    0.06024\n",
      "  0.02222 -0.2812   0.6416  -0.85    -0.6025  -0.2377   0.5493   0.12274\n",
      "  0.5635  -0.3486   0.01367  0.6963  -0.824   -0.9814  -0.4575  -0.4746\n",
      " -1.75     0.2852   0.438    1.558    0.0498   1.318   -0.3809   0.2764\n",
      "  0.3254   0.7363   1.662   -0.655  ]\n",
      "\ttyson está para hóquei (ou ['basquetebol', 'campeão'])\n",
      "query_embedding = [-0.477     1.262    -0.2568   -1.395     1.04      0.4233   -0.3696\n",
      " -0.7026   -0.03564  -0.5547    1.322     1.149    -0.1655   -0.5054\n",
      "  0.04492   0.566     1.203     0.5063    0.7183   -0.7505    1.211\n",
      " -0.4458   -0.5703   -1.049     0.3286   -0.04175   0.782    -0.258\n",
      " -0.25      0.633     1.239     0.05743   0.1177    0.05518   0.4993\n",
      " -0.463     0.2822    0.95      0.536    -0.71      0.4902    0.7896\n",
      " -1.15      0.9976   -0.4468    0.7896    1.004    -1.002     0.358\n",
      " -0.4536    0.999    -0.6367    0.2179    0.3306   -1.576     0.239\n",
      " -0.5513   -0.3955    0.338    -0.2441   -0.00879  -0.568    -0.9414\n",
      " -1.189     0.4607   -0.1578    1.078     0.6855   -0.308     0.4722\n",
      " -0.3564   -0.1316   -0.3618    0.013916  0.05615  -1.24     -0.1917\n",
      " -0.593     0.524     0.2947    0.538    -0.7725    0.3652    1.071\n",
      " -0.661    -0.545    -0.671    -1.1      -1.074     0.1885    0.2063\n",
      "  0.868    -0.1333    0.9014   -0.268     0.1185    0.05225   0.269\n",
      "  1.489     0.04114 ]\n",
      "\tbolt está para hóquei (ou ['atletismo', 'ciclismo'])\n",
      "query_embedding = [-1.684    1.191    0.663   -1.721    0.8555   0.71    -0.543   -1.361\n",
      "  0.1338  -0.323    0.782    1.072   -1.182   -0.02661  0.4539   0.4275\n",
      "  1.064    0.4275  -0.2842   0.2573   0.6567  -0.851   -0.2793  -0.908\n",
      "  0.11127 -0.2026   1.129   -0.5674  -0.2393   0.3257   1.5205  -0.10626\n",
      " -0.2913   0.1509   0.1087  -0.543   -0.4263   0.05127  0.4092  -0.8105\n",
      " -0.612    0.2456  -0.697    0.818   -1.178   -0.2002   0.5947  -1.158\n",
      "  1.037    0.132    1.106   -0.6523  -0.5254  -1.039   -1.076    0.1611\n",
      " -0.9507   0.2288   0.837   -0.3364  -0.8203  -0.6865  -0.799   -0.843\n",
      "  0.1552   0.01563  0.714    0.10205 -0.6357   0.4258  -0.3667  -0.2168\n",
      "  0.2969   0.867    1.9795  -0.375   -0.574    0.0083  -0.177   -0.09784\n",
      "  0.195   -0.8774   0.5303   0.714   -0.647   -0.347   -0.1875  -0.5938\n",
      " -0.935    0.37     0.6064   0.7734  -0.4004   0.2085   0.02722 -0.1586\n",
      "  0.5483   0.2725   1.685   -0.2195 ]\n",
      "\tsenna está para ciclismo (ou ['liga', 'campeonato'])\n",
      "atena está para sabedoria assim como...\n",
      "query_embedding = [-0.1506    0.1072   -0.922    -0.2278   -0.2358   -0.03857   1.232\n",
      "  1.077     0.003418 -0.2688   -0.3513   -0.672    -0.2834    0.01123\n",
      "  0.6733    0.04993   0.2559   -0.38     -0.0786   -0.292     0.786\n",
      "  0.983     0.1589   -0.377     1.375     0.04395  -0.4492    0.297\n",
      " -0.9775    0.7446   -0.1476    0.2388   -0.48      0.5835    0.1853\n",
      " -0.6377    0.3152   -0.3623   -0.533     0.03613  -0.02869   0.3623\n",
      "  0.302    -0.12476   0.0785   -0.9546   -0.3665   -0.4194   -0.7095\n",
      " -0.8896    1.549    -0.4414    0.543     0.2178   -0.2783    0.4478\n",
      " -0.4055    0.352     0.8584   -0.1959   -0.7646    0.06183   0.3726\n",
      " -0.698    -0.1288   -0.573     0.5244    0.6504    0.651    -0.1377\n",
      " -0.416     0.5293   -0.2205    0.2095    1.153     1.068     0.5376\n",
      " -0.3833   -0.1519   -0.1692    0.1653   -0.7607   -0.827     0.03223\n",
      "  1.625    -0.5576   -0.71     -1.072     0.805    -1.031     0.1278\n",
      "  0.685    -0.9116    0.487     0.2332    0.1976    0.1179    0.4111\n",
      " -0.3252    0.3684  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tafrodite está para bondade (ou ['harmonia', 'compaixão', 'benevolência'])\n",
      "query_embedding = [ 0.6885    0.2993   -0.9297   -0.4226   -0.08984   0.158     1.227\n",
      "  0.3892    0.3767   -0.41      0.239    -0.1816    0.1685    0.7314\n",
      "  0.2964    0.366     0.8574   -0.1334    0.3347   -0.9863    0.4756\n",
      "  0.958     0.1094   -0.756     0.552    -0.004517 -0.4985    0.02295\n",
      " -0.75      0.2468    0.3086    0.5146    0.1268    0.05798   0.607\n",
      " -0.611     0.634    -0.3315   -0.4111   -0.02051   0.3145    0.1499\n",
      "  0.621    -0.881     0.624    -0.1836   -0.517    -0.5825   -1.232\n",
      " -1.16      1.316     0.595    -0.4446   -0.0603    0.0813    0.1018\n",
      "  0.1355    0.4673    0.02191  -0.2212   -0.831     0.10724   1.436\n",
      " -0.4788    0.05627  -0.1633    0.1719   -0.355     0.61      0.5034\n",
      " -0.7197    1.586     0.1824    0.9       0.909     0.598    -0.274\n",
      "  0.074     0.1338   -0.6543    0.1128   -0.7085    0.07715   0.003662\n",
      "  1.45     -0.8306   -0.1396   -0.9697    1.1      -0.4053    0.719\n",
      "  0.7563   -0.631     0.4832    0.2908    0.3975    0.3088   -0.2605\n",
      " -0.636    -0.442   ]\n",
      "\tposeidon está para inefável (ou ['sábio', 'bondade', 'intuição'])\n",
      "query_embedding = [ 0.0598    0.3357   -1.115    -0.1885    0.2773    0.2346    1.373\n",
      "  0.782     0.751    -0.2842   -0.2632   -0.51     -0.4387    0.8447\n",
      "  1.076     0.2578    0.5986   -0.717     0.2312   -0.4556    0.67\n",
      "  0.739     0.2683   -0.75      0.874    -0.575    -0.396     0.5\n",
      " -1.14      0.581     0.905     0.2317    0.0785    0.349     0.2803\n",
      " -0.1577    0.3528   -1.027    -0.3523    0.01855  -0.1843    0.8613\n",
      "  0.3953   -0.585    -0.0421   -0.11816  -0.01978  -0.2834   -0.4446\n",
      " -0.9536    1.249    -0.1172    0.504     0.5195    0.0637   -0.00659\n",
      "  0.04956   0.3306    0.1091   -0.308    -0.69      0.076     1.293\n",
      "  0.0537   -0.3481   -0.3315    0.1567    0.2954    0.4844    0.062\n",
      " -0.3132    0.841     0.        0.7285    1.34      0.875     0.2373\n",
      " -0.4458   -0.0913   -0.3335    0.7725   -1.4795   -1.11     -0.2798\n",
      "  1.496    -0.6514   -0.6025   -1.239     0.59     -0.7627    0.4321\n",
      "  0.957    -1.214     0.1823    0.01591   0.1876   -0.1914    0.1448\n",
      " -0.02002  -0.004333]\n",
      "\tzeus está para compaixão (ou ['bondade', 'alma', 'espiritual'])\n",
      "query_embedding = [ 0.1296   -0.0818   -0.6743   -0.0943    0.4111   -0.3005    1.326\n",
      "  0.3418   -0.3552   -0.2173   -0.2151   -0.5776    0.2766    0.647\n",
      "  0.3838    0.2235    0.4917   -0.3533    0.342    -0.01227   0.4087\n",
      "  0.553     0.3442   -0.3508    0.592     0.1877    0.1576   -0.2615\n",
      " -0.942     0.615    -0.02391   0.682    -0.2106    0.2842   -0.08954\n",
      " -0.435     0.4739   -0.2168   -0.3389   -0.6104   -0.2032    0.9634\n",
      "  0.757    -0.4758   -0.1862   -0.4448   -0.3      -0.08136  -0.472\n",
      " -0.871     1.064     0.1245    0.302     0.2825   -0.196     0.2786\n",
      "  0.2439    0.212     0.0627   -0.1626   -0.1986   -0.07513   0.01904\n",
      " -0.5854    0.0939    0.05203   0.6465   -0.2119    0.41      0.1328\n",
      " -0.4524    0.645    -0.388     0.2695    0.891     0.7725    0.521\n",
      " -0.3015   -0.4697   -0.127     0.4531   -0.887    -0.994    -0.1445\n",
      "  1.013    -0.356    -0.3413   -0.729     0.336    -0.5215   -0.0273\n",
      "  0.6445   -0.7812    0.3572   -0.009964 -0.2242   -0.1716   -0.11273\n",
      "  0.1527    0.0645  ]\n",
      "\tatena está para bondade (ou ['serenidade', 'generosidade', 'compaixão'])\n",
      "cruzeiro está para raposa assim como...\n",
      "query_embedding = [-0.5728    0.3945    0.3975   -0.8745   -0.03906   0.4233    0.4055\n",
      " -0.1206    1.102     0.3604    0.4866   -0.4233   -0.0913    0.1719\n",
      "  0.07825  -0.5327   -0.1782   -1.616    -0.2588    0.2214    0.318\n",
      " -0.716    -0.456     0.1289   -0.06696  -0.9824   -0.1504    0.07153\n",
      " -0.5547    1.09     -0.07764   0.912     0.5605   -0.5723   -0.6514\n",
      "  0.1501   -0.7637   -0.6157   -0.1692   -0.7583    0.2056    0.338\n",
      " -0.7275   -0.7056   -0.7983    0.1376    0.667    -0.0346    0.9663\n",
      "  0.2522    0.797    -0.01489  -0.4666    0.542    -1.057    -0.1057\n",
      "  0.8525   -0.149    -0.8525    0.538     0.1035    0.665    -0.02112\n",
      " -0.366     0.0752   -0.8823   -0.705    -0.271    -0.3542    0.55\n",
      " -0.669    -0.009766 -0.801     0.4832   -0.8223   -0.2059   -0.1558\n",
      " -0.2983   -0.09937   0.3213    0.0512   -0.1901   -0.2554    0.3403\n",
      " -0.9966   -0.2756   -0.626    -0.716    -0.8447   -0.834    -0.0637\n",
      "  0.65      0.02441  -0.528    -0.3813    0.181    -0.509     0.913\n",
      " -0.1798    0.3403  ]\n",
      "\tatlético está para galo (ou ['csa', 'azulão'])\n",
      "query_embedding = [ 0.279    0.2217   0.1593   1.049   -1.094    0.571    0.836    1.328\n",
      "  0.338    0.1517  -0.04834 -0.0913   0.03125  0.207    0.9443   0.1578\n",
      "  0.335   -0.8716   0.331   -0.2573  -0.2473  -0.0748  -0.05258  0.814\n",
      " -0.0704  -0.1947   0.06445  0.625   -0.786   -0.656    0.3154   0.3025\n",
      "  0.4878  -0.2059  -0.3606   0.552    0.469   -0.09985  0.4102  -0.173\n",
      " -0.3774  -0.4043  -0.625   -0.582   -0.671   -0.5513   0.936    0.2725\n",
      "  0.04053 -0.2003   0.995   -0.121   -0.8857  -0.3452   0.2446  -0.5083\n",
      "  0.86    -1.244   -0.3562  -0.02881  0.07544  0.6714   0.2922  -0.622\n",
      "  0.1425   0.0481   0.954    0.042    0.1738   0.1868   0.2905   0.157\n",
      " -0.689    0.1931  -1.065   -0.316    0.3765  -0.719   -0.3809  -0.363\n",
      " -0.01666 -0.00403 -0.1392  -0.2957  -0.88     0.4973   0.0696   0.675\n",
      " -0.7153   0.1528   0.219   -0.1362   0.5103   0.4243   0.1273   0.1002\n",
      " -0.0635   0.03125 -1.268    0.1467 ]\n",
      "\tgremio está para exquisite (ou ['arara-azul-de-lear', 'noreña', 'bifobia'])\n",
      "query_embedding = [-0.354     0.3809   -0.1569   -0.9      -0.9116    0.4746    0.4639\n",
      " -0.4863    0.646     0.732     0.2261    0.64      1.301     0.0991\n",
      "  0.2025   -0.1654   -0.1211   -1.531     0.1018    0.8965    0.831\n",
      " -0.0707    0.1217   -0.1035    0.3281   -0.5874    0.4873   -0.483\n",
      " -0.19      0.958    -0.851     1.035     0.291    -0.8545   -0.9775\n",
      " -0.622    -0.11255  -0.9795   -0.5747   -0.609    -0.3752    0.691\n",
      " -0.684    -0.3035   -0.5234   -0.2476    0.1521   -0.04486   1.084\n",
      " -0.1726   -0.05273   0.3755   -0.627     0.2399   -1.137     0.1952\n",
      "  1.024    -0.03772  -0.916     0.2664    0.2856    0.6655   -0.2032\n",
      " -0.2927    0.795    -0.993    -0.4148    0.1001   -0.3215    0.6895\n",
      " -0.71      0.4773   -0.967    -0.2625   -0.332    -0.4736    0.2507\n",
      "  0.1146   -0.1228   -0.009766 -0.3203   -0.3545   -0.624     0.4294\n",
      " -1.131    -0.796    -0.3079   -0.3525   -1.082    -0.2698    0.2756\n",
      "  0.6436    0.8564   -0.947     0.3862   -0.1515   -0.532     0.8643\n",
      " -0.679     0.3691  ]\n",
      "\tpalmeiras está para macaca (ou ['verdão', 'galo'])\n",
      "query_embedding = [-1.9604e-01  4.5557e-01 -1.5320e-01 -9.3701e-01 -8.8379e-01  4.8706e-02\n",
      " -1.3037e-01 -5.1074e-01  6.7822e-01  4.8853e-01  2.2705e-01  9.6924e-02\n",
      "  5.8594e-01 -1.0486e-01 -1.0425e-01  1.5617e-02  3.5449e-01 -1.4492e+00\n",
      " -6.4453e-02  8.3691e-01  2.2607e-01 -1.7944e-01 -2.8412e-02  2.4316e-01\n",
      " -1.3501e-01 -4.6069e-01  7.7344e-01 -4.3311e-01 -3.0664e-01  5.5225e-01\n",
      " -8.1152e-01  1.0029e+00 -1.0815e-01 -6.1475e-01 -8.2520e-01 -6.0791e-01\n",
      " -1.7065e-01 -6.2891e-01 -6.0352e-01 -7.9736e-01 -6.8787e-02  5.8740e-01\n",
      " -4.0918e-01 -6.9580e-01 -3.8721e-01  3.1885e-01  2.9443e-01  2.7661e-01\n",
      "  1.1152e+00  1.5015e-01 -1.2744e-01  4.2896e-01 -3.8062e-01  1.5674e-01\n",
      " -1.1904e+00  3.6621e-01  9.9414e-01 -1.2329e-02 -5.5615e-01  8.0566e-01\n",
      "  2.4756e-01  7.5684e-01 -1.6370e-01 -5.1758e-01  7.9492e-01 -6.3623e-01\n",
      " -2.7612e-01  8.4473e-02 -1.9592e-02  4.2969e-01 -7.6074e-01  2.0361e-01\n",
      " -1.2275e+00 -4.6204e-02 -4.4238e-01 -4.0527e-01  1.4355e-01  4.3335e-02\n",
      " -2.4414e-04 -1.0010e-02 -7.9834e-01 -5.2490e-01 -2.5830e-01  5.1239e-02\n",
      " -1.2188e+00 -8.8477e-01 -2.5830e-01 -5.6055e-01 -8.7305e-01 -6.6113e-01\n",
      "  4.5361e-01  8.4473e-01  2.8027e-01 -8.9795e-01 -2.8564e-02 -2.5488e-01\n",
      " -6.5283e-01  9.5312e-01 -1.3733e-01  5.2686e-01]\n",
      "\tcorinthians está para timão (ou ['galo', 'verdão'])\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import *\n",
    "dict_embedding = get_embedding( \"glove.pt.100.txt\",100)\n",
    "obj_analogy = Analogy(dict_embedding,\"kdtree.pt.p\")\n",
    "\n",
    "\n",
    "dict_analogias = {(\"brasil\",\"brasilia\"):[\"peru\",\"gana\",\"japão\",\"espanha\",\"india\"],\n",
    "                  (\"bahia\",\"salvador\"):[\"acre\",\"alagoas\",\"amapá\",\"amazonas\",\"ceará\",\"goiás\"],\n",
    "                  (\"brasil\",\"feijoada\"):[\"italia\",\"estados-unidos\",\"inglaterra\",\"argentina\",\"peru\"],\n",
    "                  (\"homem\",\"mulher\"):[\"garoto\",\"rei\",\"príncipe\",\"pai\",\"cavalo\",\"garçon\"],\n",
    "                  (\"grande\",\"pequeno\"):[\"cheio\",\"alto\",\"forte\",\"largo\"],\n",
    "                  (\"pelé\",\"futebol\"):[\"tyson\",\"bolt\",\"senna\"],\n",
    "                  (\"atena\",\"sabedoria\"):[\"afrodite\",\"poseidon\",\"zeus\",\"atena\"],\n",
    "                  (\"cruzeiro\",\"raposa\"):[\"atlético\",\"gremio\",\"palmeiras\",\"corinthians\"],\n",
    "                 }\n",
    "\n",
    "for (palavra,esta_para), arr_assim_como in dict_analogias.items():\n",
    "    print(f\"{palavra} está para {esta_para} assim como...\")\n",
    "    for assim_como in arr_assim_como:\n",
    "        palavras = obj_analogy.analogia(palavra,esta_para,assim_como)\n",
    "        print(f\"\\t{assim_como} está para {palavras[0]} (ou {palavras[1:]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Uma limitação desses embeddings é a dependencia de idioma e que palavras ambiguas não são tratadas. Por exemplo, Jaguar pode ser uma marca de carro ou animal, dependendo do contexto.  Para diminuir o problema de ambuiguidades, o [BERT](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) é um embedding que a representação da palavra é diferente de acordo com o seu contexto. O [MUSE](https://github.com/facebookresearch/MUSE) é um embedding multilingue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Representação textual usando embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Muitas vezes, precisamos de um único vetor para representar uma frase ou um texto ainda maior. Para isso, podemos usar a representação Bag of Words ou, ainda, representar por palavras chaves ou utilizarmos uma combinação de nossas representações por palavras. Neste tutorial, iremos mostrar como combinar embeddings de palavras e usar a representação por palavras chaves - podendo, inclusive, fazer uma expansão de palavras chaves por embeddings.\n",
    "\n",
    "Para isso, iremos usar o seguinte contexto: por meio de um dataset de revisões de produto da amazon, deseja-se prever automaticamente o sentimento do mesmo (positivo ou negativo). Utilizou-se uma amostra do [dataset do Kaggle para este exemplo](https://www.kaggle.com/bittlingmayer/amazonreviews). Veja abaixo o dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "df_amazon_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Em um método de aprendizado de maquina, cada instancia deve ser representada por um vetor numérico utilizando as representações ditas anteriormente. Iremos ilustrar cada exemplo utilizando uma pequena subamostra desta amostra com 5 exemplos positivos e 5 negativos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>NIV Bible: The NIV Bible is good, but I wish I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>pouch can be better: I recently bought it at A...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>Great book!: Dawn of a Thousand Nights is extr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>Disappointed: Very small wipes canister, not v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>The most horrible Blu-Ray: First, Night scenes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "101657  NIV Bible: The NIV Bible is good, but I wish I...  positive\n",
       "49225   pouch can be better: I recently bought it at A...  positive\n",
       "158265  Great book!: Dawn of a Thousand Nights is extr...  positive\n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "200048  Disappointed: Very small wipes canister, not v...  negative\n",
       "60933   The most horrible Blu-Ray: First, Night scenes...  negative"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"positive\"][:5]\n",
    "df_negative = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"negative\"][:5]\n",
    "df_amazon_mini = pd.concat([df_positive,df_negative])\n",
    "df_amazon_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bag of words: ** um exemplo simples, sem usar embeddings, é a representação em bag of words, **já discutido aqui**. Assim, podemos  usar a classe `BagOfWords` que está no arquivo `textual_representation.py`. Para as representações bag of words, usaremos a função bag_of_words abaixo. Usando esta representação o nosso dataset ficaria representado da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>70</th>\n",
       "      <th>absolute</th>\n",
       "      <th>actors</th>\n",
       "      <th>ahead</th>\n",
       "      <th>album</th>\n",
       "      <th>...</th>\n",
       "      <th>wooden</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.231984</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119602</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273879</td>\n",
       "      <td>0.13694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              13        16        18        20        21        70  absolute  \\\n",
       "id                                                                             \n",
       "208138  0.115992  0.231984  0.115992  0.115992  0.115992  0.115992  0.115992   \n",
       "157010  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "101657  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "57708   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          actors     ahead     album  ...    wooden   working     world  \\\n",
       "id                                    ...                                 \n",
       "208138  0.000000  0.115992  0.115992  ...  0.000000  0.000000  0.000000   \n",
       "157010  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.239204   \n",
       "101657  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "57708   0.129047  0.000000  0.000000  ...  0.129047  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  ...  0.000000  0.223607  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           worse     worst      wow  wrenching   written     years     class  \n",
       "id                                                                            \n",
       "208138  0.000000  0.000000  0.00000   0.115992  0.000000  0.000000  positive  \n",
       "157010  0.000000  0.000000  0.00000   0.000000  0.000000  0.119602  positive  \n",
       "101657  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "49225   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "158265  0.000000  0.000000  0.00000   0.000000  0.245462  0.000000  positive  \n",
       "204215  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "274316  0.283463  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "57708   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "200048  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "60933   0.000000  0.273879  0.13694   0.000000  0.000000  0.000000  negative  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import BagOfWords\n",
    "#o vocabulario, quando vazio, será considerado todas as palavra (menos stopwords)\n",
    "def bag_of_words(data, vocabulary=None):\n",
    "    #obtem stopwords\n",
    "    stop_words = set()\n",
    "    with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "        stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "    #instancia o bag of words, filtrando stopwords e considerando o vocabulario (se possivel)\n",
    "    bow = BagOfWords(\"bow\", stop_words=list(stop_words), words_to_consider=vocabulary)\n",
    "    \n",
    "    #o bag of words, é gerado separadamente a representação do treino e teste\n",
    "    #iremos usar apenas a representação considerando que \"data\" é o treino\n",
    "    data_preproc = bow.preprocess_train_dataset(data, \"class\")\n",
    "\n",
    "    #exibe apenas colunas não zedadas\n",
    "    m2 = (data_preproc != 0).any()\n",
    "    data_preproc = data_preproc[m2.index[m2].tolist()]\n",
    "    \n",
    "    return data_preproc\n",
    "bag_of_words(df_amazon_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bag of words (filtrado por palavras chaves e embeddings similares)** Como bag of words é uma representação com milhares de atributos, poderiamos fazer uma restrição por palavras chaves. Por exemplo, caso usássemos como vocabulário do bag of words baseado nas palavras obtidas da roda de emoções porposta por [Scherer K., (2005)](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guilt, remorse, depreciate, derision, worry, relief, gloom, tear, contempt, rage, recognition, happy, jittery, bliss, angry, optimistic, cheer, astonishing, ashamed, blame, surprise, comfortable, aversion, exaltation, hopeless, ardor, abhor, tedious, disgust, disdain, incense, enjoy, temper, boredom, shame, denigration, sad, anxiety, sick, infuriating, exhilarating, elation, fury, anguish, sadness, proud, joy, dejected, contrition, abashed, nausea, delight, resent, faith, thunderstruck, anger, acknowledgement, contentment, interest, curious, apprehensive, furious, hope, disrelish, embarrassing, animation, amazed, buoyancy, confident, nervous, melancholy, wrath, ecstatic, scorn, mad, euphoria, chagrin, dislike, pride, dumbfounded, happiness, hostile, satisfaction, respect, alert, indifference, enthusiasm, ennui, humiliating'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "\n",
    "vocabulary = []\n",
    "for emotion_group, set_keywords in emotion_words.items():\n",
    "    vocabulary.append(emotion_group)\n",
    "    for word in set_keywords:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "\", \".join(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O grande problema é que esse grupo de palavras é muito restrito. Veja como ficou a representação dos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>sick</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        interest  sick     class\n",
       "id                              \n",
       "208138       0.0   0.0  positive\n",
       "157010       1.0   0.0  positive\n",
       "101657       0.0   0.0  positive\n",
       "49225        0.0   0.0  positive\n",
       "158265       0.0   0.0  positive\n",
       "204215       0.0   0.0  negative\n",
       "274316       0.0   0.0  negative\n",
       "57708        0.0   1.0  negative\n",
       "200048       0.0   0.0  negative\n",
       "60933        0.0   0.0  negative"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lembre-se que eliminamos as palavras que não apareceram em nenhuma instancia. Assim, como pode-se observar, apenas duas palavras foram usadas e alguns documentos não possuiam nenhuma palavra. Para ampliar o vocabulário, poderiamos expandir esta representação usando palavras similares a estas de acordo com o nosso embedding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 6.4648e-01  4.8877e-01  6.9727e-01  8.6975e-03 -8.4961e-01  9.3359e-01\n",
      " -5.7764e-01  1.0876e-01  4.5996e-01  1.4441e-01 -2.5122e-01  1.0762e+00\n",
      " -4.1943e-01  4.6997e-01  3.6743e-01 -3.9600e-01  1.0040e-02  1.2097e-01\n",
      " -7.7441e-01  6.1816e-01 -1.9263e-01 -9.6191e-02 -9.1406e-01 -6.5674e-01\n",
      " -3.1592e-01  6.1035e-01  7.5391e-01 -6.1890e-02  1.5820e-01 -1.3647e-01\n",
      "  5.6201e-01 -5.1422e-02 -5.7666e-01 -1.2854e-01 -6.1035e-01  2.4948e-02\n",
      "  9.4141e-01  4.0991e-01  2.1927e-02 -1.0535e-01 -3.2349e-01  4.2725e-01\n",
      "  6.3086e-01 -6.2988e-01 -2.7374e-02 -8.5205e-01  1.0371e+00 -1.8201e-01\n",
      "  4.1901e-02 -6.3525e-01 -2.0215e-01 -3.0786e-01  1.1957e-01  1.0137e+00\n",
      "  6.5796e-02 -6.5576e-01  5.4932e-01 -3.3716e-01  7.5586e-01  2.6782e-01\n",
      "  7.0166e-01  3.3252e-01  3.0566e-01 -3.6108e-01  4.0625e-01 -4.3433e-01\n",
      "  7.1973e-01  1.7920e-01 -1.2861e+00 -8.8745e-02 -3.0688e-01  2.1924e-01\n",
      "  1.1234e-03  1.7371e-01  1.5442e-01  9.3945e-01  1.1768e-01 -9.7607e-01\n",
      " -8.8818e-01  5.7324e-01 -4.0356e-01  9.7363e-01  6.9824e-02  3.1055e-01\n",
      " -9.4043e-01 -2.2754e-01 -9.8291e-01  3.5840e-01 -8.6230e-01 -1.8848e-01\n",
      " -1.2744e+00 -2.9370e-01  4.4678e-01  4.3945e-01  1.2559e+00 -6.2439e-02\n",
      "  4.5190e-01 -7.1143e-01 -2.4902e-02 -7.6721e-02]\n",
      "query_embedding = [ 0.788     0.4233    0.3672    0.1035   -0.4678    0.6714   -0.974\n",
      " -0.1006    0.2046   -0.0326   -0.2642    0.821    -0.1945    0.1626\n",
      "  0.516     0.4485   -0.4473    0.365    -0.7686    0.529     0.159\n",
      "  0.2678   -1.609    -0.6787    0.0543    0.8965    0.657     0.4387\n",
      "  0.833    -0.1489    1.018     0.3535   -0.803     0.4753   -0.4314\n",
      "  0.207     1.081    -0.0897   -0.4424   -0.2896    0.0071    0.3962\n",
      "  0.917     0.1567   -0.3035    0.188     1.439    -0.552     0.48\n",
      " -1.162     0.04605  -0.1533    0.3398    0.823    -0.1366    0.13\n",
      "  0.3606   -0.545     0.2113    0.01573   0.3147    0.6367   -0.0979\n",
      "  0.006805 -0.006905 -0.1024    0.598     0.2783   -0.7495    0.2148\n",
      " -0.264    -0.2593    0.0918   -0.3186    0.6177    2.035    -0.0881\n",
      "  0.1981   -0.857     0.2067   -0.583     0.3813    0.59      0.736\n",
      " -0.4727   -0.2388   -0.467     0.4675   -0.712    -0.4849   -0.573\n",
      " -0.413    -0.0856    0.4329    0.5254    0.3345    0.1344   -1.048\n",
      "  0.01974   0.1658  ]\n",
      "query_embedding = [-0.256    0.31    -0.639    0.09344 -0.8237   0.2229  -0.8545  -0.3303\n",
      "  1.077    0.2815   0.1318   0.4197  -0.297   -0.6636  -0.1888  -0.5435\n",
      " -0.36     0.7085   0.321    0.11346  0.515    0.05008 -0.1649   0.906\n",
      "  0.1572   0.2118   0.05414 -0.8926   0.619   -0.02072 -0.2146  -0.2842\n",
      " -0.516   -0.01511 -0.1702   0.5454   0.6724  -0.526   -0.7305  -0.92\n",
      "  0.636   -0.3643  -0.737    0.1556  -0.10565  0.1592   0.8687  -0.4675\n",
      "  0.3403  -1.589   -0.1126   0.577   -0.2054   0.3875  -0.0226   0.8354\n",
      "  0.3738  -0.6685   0.08594 -0.283   -0.1343   0.7046  -0.204    0.8525\n",
      " -0.118    0.406   -0.575    0.132    0.089   -0.1514   0.03778 -0.2644\n",
      "  0.699   -0.00941 -0.2118  -0.6313  -0.323    0.7803   0.4846   0.652\n",
      "  0.3203  -0.1993   0.03119 -0.6284   0.799    0.1101   0.0673  -0.1224\n",
      "  0.6484  -0.4329  -0.4297   0.587   -0.1125  -0.565    0.1741  -1.07\n",
      "  0.772   -0.1897  -0.12354  0.543  ]\n",
      "query_embedding = [-0.0652    0.59      0.6104   -0.2388    0.1571    1.192    -0.36\n",
      " -0.9883   -0.1667    0.0213   -1.013    -0.1208   -0.1971   -0.762\n",
      "  0.7095   -0.0401   -0.4724    0.4207   -0.5264    0.06537  -0.3086\n",
      "  0.2803   -0.5703   -0.1932    0.3582    0.1315    0.0881    0.497\n",
      "  0.4368   -0.887     0.295    -0.1597   -0.3801    0.415    -0.227\n",
      "  1.065     0.3743   -0.03644  -0.647     0.0379   -0.349    -0.2034\n",
      " -0.3298    0.4893    0.11743  -0.249     0.7974    0.2954   -0.1577\n",
      " -0.0986   -0.03302  -0.4277    0.0697    0.3613   -0.2646    0.526\n",
      " -0.1311   -0.3384   -0.3538   -0.7773    0.5073    0.4531    0.127\n",
      "  0.6426    0.02135  -0.6333    0.947    -0.6826    0.1757   -0.1094\n",
      " -0.589     0.003244  0.2079   -0.5635   -0.76      0.6577   -0.1614\n",
      " -0.534    -0.8804    0.1461    0.0832   -0.1625    0.1514   -0.3547\n",
      " -0.5615   -0.05023  -0.2485    0.2776   -0.3877   -0.5327    0.11523\n",
      "  0.05597  -0.2898    0.719     0.7734    0.1914   -0.12115   0.798\n",
      "  0.2239    0.3503  ]\n",
      "query_embedding = [-0.002932  0.6304    0.5146   -0.4326   -0.904    -0.132    -1.177\n",
      " -0.65      0.61     -0.473    -0.4688    0.3901    0.0587   -0.2222\n",
      " -0.3772   -0.2747   -0.623     0.4426    0.0344    0.2107   -0.1442\n",
      "  0.6523   -0.3665   -0.2986   -0.353     0.2856   -0.1688   -0.4194\n",
      "  0.3513   -0.1219   -0.1079    0.1501    0.1152   -0.1509   -0.3354\n",
      "  0.1256   -0.01484   0.3003    0.613    -0.2347   -0.5625   -0.1309\n",
      "  0.04034  -0.653     0.0771   -0.4097    0.709     0.03098  -0.3376\n",
      " -1.268     0.547    -0.01569  -0.689     0.503     0.2864   -1.489\n",
      "  0.347    -0.2229    0.594     0.2155   -0.0726    0.6772   -0.1102\n",
      " -0.1436    0.5776    0.7324    0.5303    0.0627    0.00829  -0.181\n",
      "  0.0822   -0.1611   -0.3967   -0.3645   -0.3306    0.402     0.1827\n",
      "  0.0538   -0.505     0.2296    0.537    -0.5054   -0.623    -0.1736\n",
      " -1.388    -0.4417   -0.3748    0.3289   -0.7163   -0.3213   -0.04895\n",
      "  0.0921   -0.09814  -0.641    -0.6836    0.1475    0.3245   -0.1547\n",
      "  0.2452    0.2935  ]\n",
      "query_embedding = [-0.2219   1.158   -0.346   -0.4084   0.0871  -0.2131  -0.4421   0.04092\n",
      " -0.2815  -0.5605  -0.255   -0.4639   0.1085  -0.0869   0.01455 -0.371\n",
      " -0.1567  -0.04465 -0.2834  -0.0874  -0.1177   0.4744  -0.3523  -0.01671\n",
      " -0.2205   0.7407  -0.3455   0.4763  -0.4265  -0.6807  -0.0493  -0.62\n",
      " -0.2893   0.5283  -0.7554  -0.4385  -0.2947   0.7207  -0.06885  0.1556\n",
      " -0.2522   0.1356   0.412   -0.2283   1.16    -0.3867  -0.4287  -0.508\n",
      " -0.02756 -0.3901  -0.3196   0.6177   0.3997   0.69    -0.10693 -1.573\n",
      "  0.0945  -0.803    2.15     0.9272  -0.2203   0.2019  -0.05716  0.1329\n",
      "  0.3743  -0.2413  -0.453    0.1294   0.3464  -0.6587   0.1844  -0.6445\n",
      " -0.9937  -0.397    1.725    1.158   -0.433    0.9263  -0.5083  -0.1173\n",
      "  0.1722   0.6997  -0.2942  -0.10095 -1.332    0.745    0.2107  -0.2252\n",
      " -0.6035  -0.644    0.526   -0.7837  -0.1759  -0.3936   0.2644   0.2037\n",
      "  0.3188   0.6895   0.3645  -0.1575 ]\n",
      "query_embedding = [-0.562    0.304    0.7197  -0.7104  -0.3936  -0.2151  -0.8564  -1.163\n",
      " -0.03592 -0.2556  -0.3875   0.3108   0.2366  -0.4666   0.741   -0.1721\n",
      "  0.03348  0.3826   0.2101  -0.562   -0.2028   0.01657 -0.0607  -0.265\n",
      "  0.2183  -0.0998   0.5137   0.2368  -0.8564  -0.653   -0.2483  -1.395\n",
      " -0.7905   0.2808  -1.322    0.4353   0.351   -0.3398   0.1185  -0.4683\n",
      " -0.838   -0.7886  -0.2598   0.192    0.3784  -0.5103   0.8438   0.582\n",
      "  0.8325  -1.04    -0.703    0.0729  -0.1742   0.714   -0.1722  -1.346\n",
      "  0.3303   0.3384  -0.575    0.4875   0.2124   0.2922  -0.585    0.03598\n",
      " -0.627    0.3928  -0.0652  -0.3582  -0.06903 -0.9062   0.02802 -0.0939\n",
      " -0.7705  -0.2742  -0.1448   0.6636   0.4065  -0.4548  -0.336   -0.02571\n",
      "  0.2717   0.1566  -0.4746   0.8237  -0.3386  -0.6797  -0.4365  -0.163\n",
      "  0.4148  -0.0846   0.8125   0.7056  -0.2861   0.0665   0.4998   0.0746\n",
      "  0.5566  -0.011   -0.202    0.3528 ]\n",
      "query_embedding = [-0.2056    0.1475   -0.94     -0.7183   -0.8164    1.722     0.1299\n",
      "  0.938     0.4807    1.08     -0.4116   -0.3281   -0.1145    0.5005\n",
      " -0.4731   -0.1383   -0.4924    0.1738    0.0501   -0.7607    0.7915\n",
      " -0.8022   -0.711     0.5327    0.8936    0.10864  -0.298    -0.495\n",
      "  0.886     0.1638    0.1058    0.1694   -0.4456    0.2229   -0.2678\n",
      " -1.508     0.215    -0.6743    0.1595    0.706    -0.4102   -0.4297\n",
      " -0.425    -0.4084    0.0053   -0.03052  -0.5513    0.4675   -0.7026\n",
      " -0.7944    0.006214  0.5176    0.0792    0.5864   -0.1786   -1.246\n",
      " -0.2377   -0.1128    0.9585    0.7637    0.2634    1.291    -0.633\n",
      "  1.056    -0.1731    0.1462    1.011    -1.12     -0.3193   -0.685\n",
      "  0.4082    0.252     0.2686   -0.6387   -0.2542   -0.1116    0.428\n",
      "  0.4827   -0.1985    0.953     1.144    -0.3381   -0.3982   -0.03848\n",
      " -0.274    -0.1152    0.3794    0.951    -0.3843    0.61      0.443\n",
      " -0.963    -0.747    -0.3347   -0.1763    0.2278    0.0969    0.6396\n",
      "  0.05585   0.519   ]\n",
      "query_embedding = [ 0.615   -0.4116   0.5786  -0.0366   0.263    1.32    -0.702   -0.03705\n",
      " -0.3718   0.04218 -0.794    0.6953  -0.2058  -0.4272  -0.5127  -0.03824\n",
      "  0.024    0.3584  -0.7583   0.2139  -0.1013  -0.4624  -0.2295  -0.663\n",
      "  0.989    0.462    0.4653  -0.4255   0.361   -0.3982   1.366   -0.08344\n",
      " -0.9854  -0.2695  -0.08026  0.9497   0.4448  -0.68    -0.2402   0.832\n",
      " -1.014   -0.2023   0.02216 -0.4312   0.03888 -0.2502   0.593   -0.4219\n",
      " -0.2458  -0.5444   1.081   -0.705    0.5107   1.09    -0.4287   0.2404\n",
      "  0.6235  -0.404    0.873   -0.628   -0.11017  0.1366  -0.498    0.0171\n",
      "  0.3833  -0.2      1.128    0.2074  -0.372    0.4849  -0.2502   0.4414\n",
      " -0.2483  -0.321   -0.1938   1.115   -0.2693  -0.446   -0.821    0.0643\n",
      " -0.1332   0.1915   0.0851   0.6206  -0.3928   0.1259  -0.1388   0.2129\n",
      "  0.4758  -0.3057  -0.2094  -0.5596  -0.2866   0.3474   1.094    0.5435\n",
      " -0.0362  -0.4775   0.4805   0.1992 ]\n",
      "query_embedding = [-0.07745  -0.2456    0.5176   -0.848    -0.1692    1.742    -0.1448\n",
      " -0.311     0.4       0.202    -0.2896   -0.1813    0.2135   -0.5664\n",
      "  0.138     0.03114  -0.5073    0.6753   -0.565     0.10675   0.75\n",
      " -0.2351    0.02118   0.1481    0.3535    0.689     0.7227    0.1566\n",
      "  0.6367    0.02756  -0.1948    0.2175   -0.1688    0.1971   -0.7085\n",
      "  0.1616    0.3716   -0.1677    0.7397   -0.0388   -0.047     0.07104\n",
      " -0.4807   -0.6333    0.508    -0.2427    0.417     0.4382    0.4458\n",
      " -0.643    -0.378     0.0371   -0.005066  1.13      0.656    -1.192\n",
      "  0.2678    0.2917    0.01912  -0.0771    0.682     0.609     0.2803\n",
      " -0.5195    0.1155    0.5903    0.7524   -0.7427   -0.0924    0.03543\n",
      "  0.04623  -0.4866   -0.4314   -0.0183   -0.1991    0.5186    0.541\n",
      " -0.2815   -0.4238    0.162     0.2527    0.2795    0.1605    0.405\n",
      " -0.3115   -0.2554   -0.1921   -0.4426   -0.428    -0.04758  -0.303\n",
      "  0.2377   -0.641     0.7603    0.4426    0.1316   -0.301    -0.1796\n",
      "  0.72      0.1487  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 5.0000e-01 -1.8661e-02 -7.4646e-02  9.6387e-01  6.0547e-01 -1.3232e-01\n",
      " -3.7329e-01 -7.4805e-01  5.5762e-01  2.7905e-01 -1.0293e+00 -4.4702e-01\n",
      "  7.4561e-01 -7.6782e-02 -1.7285e-01  2.2095e-01  6.2354e-01  9.9365e-02\n",
      " -1.0864e-01  8.5254e-01 -8.4375e-01  2.5940e-02 -1.4478e-01 -3.8239e-02\n",
      "  6.7529e-01  2.3254e-01  3.5083e-01 -2.7954e-01  7.4365e-01 -9.3604e-01\n",
      "  4.9658e-01  4.5459e-01 -7.8369e-02  4.8364e-01  5.8545e-01 -6.6455e-01\n",
      "  3.2861e-01  4.7656e-01 -1.3457e+00 -2.4487e-01 -3.0884e-01 -1.0747e-04\n",
      "  4.4678e-01 -1.5186e-01 -2.3376e-01 -4.5117e-01  5.8057e-01  1.0876e-01\n",
      " -2.7295e-01 -7.2461e-01  5.7281e-02  2.5415e-01  1.6998e-02  9.3555e-01\n",
      "  2.7393e-01 -1.4473e+00  4.5776e-01 -7.9004e-01  1.5469e+00 -2.6074e-01\n",
      "  1.4746e-01  5.0537e-01 -1.6895e-01  3.3081e-01  2.4072e-01 -3.1836e-01\n",
      " -3.5815e-01 -5.1384e-03  3.8184e-01  9.2041e-02 -3.8910e-02  2.0630e-01\n",
      "  3.1799e-02 -3.4009e-01  5.4785e-01  3.2227e-01 -2.7295e-01 -1.2732e-01\n",
      " -8.4912e-01 -3.4595e-01  2.9712e-01 -4.2480e-01  4.4165e-01 -3.0420e-01\n",
      " -1.0195e+00  5.9863e-01  2.0679e-01 -6.4062e-01  3.7537e-02 -5.5713e-01\n",
      " -1.3513e-01 -6.0120e-03 -4.9316e-01  9.4434e-01 -1.4514e-01  1.3025e-01\n",
      " -2.4146e-01  3.4363e-02  3.0518e-01  5.2588e-01]\n",
      "query_embedding = [-0.09045  0.1964   0.2947  -0.477   -0.804    0.3079  -0.5522   0.5845\n",
      " -0.1705  -0.8486   0.1953   0.2367   0.4683  -0.59    -0.12164 -0.247\n",
      " -0.07294  0.1726  -0.0485   0.9526   0.5063   0.585   -0.1937  -0.4546\n",
      " -0.0311   0.516   -0.2405  -0.1007   0.536    0.02423 -0.5015   0.737\n",
      "  0.4946  -0.3474   0.8936   0.05743 -0.1913   0.3933   0.2118  -0.8984\n",
      "  0.0787  -0.1635   0.4526  -0.411   -0.195   -0.1349  -0.01631 -0.02185\n",
      "  0.1714  -1.241    0.0795  -0.9116   0.357    0.3628  -0.2494  -2.12\n",
      "  0.1454   0.53     0.9014   0.0336   0.02281  0.706   -1.036   -0.598\n",
      "  0.706   -0.0728   0.6704   0.528   -0.478   -0.674    0.3662  -0.3828\n",
      " -0.1035  -0.64     0.181    0.8257   0.0664  -0.408   -0.0838  -0.365\n",
      "  0.04535 -0.07355 -0.2012   0.3745  -1.402   -0.256   -0.4707  -0.1615\n",
      " -0.8794  -0.3633  -0.1736  -0.078    0.4326   0.00893 -1.031   -0.1159\n",
      " -0.3452   0.1151  -0.4082   0.202  ]\n",
      "query_embedding = [ 2.1265e-01 -1.8079e-01  1.0518e+00 -1.9678e-01 -6.4502e-01  3.4741e-01\n",
      " -9.6729e-01 -4.3457e-01  5.2197e-01 -6.7725e-01 -5.5957e-01  1.9455e-02\n",
      "  1.3184e-01 -7.1143e-01 -5.6671e-02  1.7273e-01 -7.8223e-01 -4.9243e-01\n",
      "  8.5352e-01 -5.8008e-01  3.3203e-01  9.3384e-02  2.7588e-01 -1.4514e-01\n",
      " -1.7975e-02 -2.5830e-01  2.4146e-01  7.4463e-01 -4.4830e-02  6.0107e-01\n",
      "  1.1826e-02 -5.3174e-01 -3.3350e-01  5.7422e-01 -1.8689e-01  1.1462e-01\n",
      "  1.8066e-01  6.8848e-02  7.7588e-01 -3.4814e-01 -2.2681e-01 -5.7959e-01\n",
      "  3.6865e-01  1.4075e-01  5.6580e-02 -1.3379e-01  5.7861e-01  2.6196e-01\n",
      "  9.3115e-01 -7.5684e-01 -9.5801e-01 -2.1326e-01 -2.3270e-04  6.7627e-02\n",
      " -2.2864e-01 -8.7939e-01  1.4795e-01 -6.8408e-01  1.0266e-01 -2.0801e-01\n",
      "  1.5442e-01  7.0996e-01 -3.2446e-01  4.1284e-01 -4.9146e-01  6.1096e-02\n",
      "  2.6758e-01  1.6248e-01  6.7432e-01 -6.5088e-01 -3.7012e-01 -4.0991e-01\n",
      " -4.7339e-01 -1.2384e-01 -9.8206e-02  2.5146e-01 -8.1177e-02  2.6562e-01\n",
      " -1.9434e-01 -5.4443e-01  1.0420e+00 -1.1749e-02 -6.2402e-01  7.7295e-01\n",
      " -2.9907e-01 -6.4404e-01 -2.0300e-01  8.1299e-01 -1.2646e+00 -6.4648e-01\n",
      " -7.8308e-02  7.5049e-01 -2.4292e-01 -5.9180e-01 -4.6875e-01  5.7422e-01\n",
      "  5.4785e-01  4.0356e-01 -8.8037e-01  8.3594e-01]\n",
      "query_embedding = [-0.1735   -0.7227    0.2502    0.3013   -0.4631    0.02011   0.6826\n",
      " -0.1582   -0.2068   -0.2817    0.2065   -0.3018    0.1942    0.5796\n",
      " -0.2256   -0.7573    0.3179    0.5244   -0.292     0.379    -0.1326\n",
      " -0.1127    0.335     0.10583   0.01973   1.033     0.693    -0.563\n",
      "  0.01645  -0.548    -0.04755  -0.2568   -0.02687   0.298    -0.3538\n",
      "  0.3206   -0.2727    0.4685    0.5938   -0.2394    0.3403   -0.11804\n",
      " -0.4316   -0.1688   -0.4727    0.08453  -0.04407  -0.006203  0.2057\n",
      "  0.19     -0.07007  -0.48      0.1345    0.001672 -0.01724  -0.6284\n",
      "  0.2698   -0.1772    0.236     0.508     0.72     -0.1982   -0.2532\n",
      " -0.7524    0.1731   -0.1871    0.5337   -0.7036   -0.2031    0.3208\n",
      " -0.3015   -0.09607  -0.11584  -0.1879   -0.0859    0.6714    0.8076\n",
      " -0.3525   -0.2896    0.374    -0.1772    0.459     0.2013    0.4207\n",
      "  0.193     0.575     0.2861   -0.4407   -0.1215   -0.5728   -0.7476\n",
      " -0.1323   -0.06665   0.5024   -0.1882   -0.428    -0.1148   -0.651\n",
      " -0.4836    0.2435  ]\n",
      "query_embedding = [ 0.3184   0.11646  0.05933 -0.8193  -0.946    1.549   -0.7085  -0.02975\n",
      "  0.6587  -0.3984  -0.4753  -0.1791   0.08954 -0.4722  -0.1965   0.116\n",
      " -1.161   -0.4036  -0.2037   0.3064   1.252   -0.07104  0.1809  -0.09894\n",
      "  0.633   -0.2072  -0.3606   0.85     0.855    0.505    0.337    0.6206\n",
      "  0.3818  -0.0724  -0.2898  -0.422    0.1428  -0.1533   0.551    0.2075\n",
      " -0.917   -0.5      0.3176  -0.403    0.3328  -0.379   -0.11584  0.893\n",
      "  0.4927  -0.2173   0.4883  -0.552    0.4192   0.512   -0.3323  -1.22\n",
      "  0.307    0.1565   0.726   -0.2122   0.4866   0.5205  -0.2686  -0.2898\n",
      "  0.6816  -0.3574   0.864    0.3542   0.1918  -0.3252  -0.0383  -0.2129\n",
      " -0.62    -0.7847  -0.2578   1.111    0.7397   0.10876 -0.8726  -0.0843\n",
      "  1.127   -0.4248  -0.5645  -0.08466 -1.342   -0.6216  -0.492    0.2339\n",
      " -0.604   -0.694    0.581   -0.2927  -0.5635   0.1719  -0.3188   0.1174\n",
      " -0.7173   0.9683  -0.06555  0.2827 ]\n",
      "query_embedding = [-0.599    0.03084  0.472    0.3154  -0.4683  -0.8823  -0.2249  -0.3242\n",
      " -0.03683 -0.4548  -0.0893   0.338    0.4604  -1.022   -0.05197 -0.1895\n",
      " -0.6016  -0.2074   0.9893  -0.02028  0.585    0.1743  -0.2058   0.105\n",
      " -0.9404  -0.02734  0.05313  0.02542 -0.371    0.253   -0.2646  -0.0265\n",
      " -0.3733  -0.08026  0.1779   0.3918  -0.1871   0.3818   0.2207  -0.5366\n",
      " -0.7217  -0.1447   0.676    0.10376 -0.2002  -0.925    0.603   -0.1658\n",
      " -0.1558  -1.179    0.6313  -0.6987   0.3835   0.3777  -0.1201  -1.682\n",
      "  0.425    0.1328   0.3835  -0.2391   0.03705  0.2742  -1.157   -0.4539\n",
      " -0.416   -0.2002   0.03687  0.9785  -0.373   -0.6416   0.736    0.0739\n",
      " -0.531   -1.096    0.266   -0.0171  -0.02007  0.08954 -0.557   -1.333\n",
      "  0.1521   0.3418  -0.911    0.3354  -0.828   -0.455   -0.1181   0.1604\n",
      " -0.8354  -0.5386   0.2817   0.1527  -0.4333  -0.471   -0.7344   0.6333\n",
      " -0.575    0.098   -0.1342   0.1674 ]\n",
      "query_embedding = [ 3.6035e-01  9.3457e-01  1.6028e-01 -8.4961e-01 -7.3828e-01  9.7754e-01\n",
      " -5.1855e-01 -4.7266e-01 -2.4902e-01 -6.4453e-01 -3.4448e-01  2.5244e-01\n",
      " -2.9639e-01 -4.8438e-01  3.0493e-01  1.2341e-01 -5.9509e-02  5.0391e-01\n",
      " -2.4451e-01  1.9321e-03  2.0483e-01 -2.1680e-01 -2.1411e-01 -3.1616e-01\n",
      " -1.1206e-01  1.2634e-01 -7.7441e-01  4.2065e-01  9.5312e-01 -5.9668e-01\n",
      " -1.9861e-01 -1.1792e-01  2.5073e-01  6.3379e-01 -4.1748e-02 -4.7231e-04\n",
      " -1.3931e-02  5.0049e-01  1.0205e-01 -9.1309e-01  2.1448e-01 -1.2195e-01\n",
      " -4.1699e-01 -5.4834e-01  4.0234e-01 -4.8462e-02  5.8740e-01  1.9348e-01\n",
      "  3.9380e-01 -1.2119e+00 -5.1465e-01 -1.5686e-01  4.5728e-01  3.0090e-02\n",
      "  2.2720e-02 -1.2129e+00  6.4026e-02  2.4817e-01  3.2495e-01 -4.4946e-01\n",
      " -2.0850e-01  6.0059e-01 -1.3599e-01  2.3035e-01 -6.0645e-01  7.1228e-02\n",
      " -1.7426e-02 -2.7466e-01  2.2681e-01 -2.5317e-01  9.9609e-02 -6.9141e-01\n",
      " -5.8447e-01 -7.9248e-01  3.7811e-02  1.3164e+00  1.5881e-01 -3.3765e-01\n",
      "  2.7539e-01 -4.3066e-01  3.9331e-01 -4.2285e-01 -5.7770e-02 -6.6406e-01\n",
      " -5.5225e-01 -7.1045e-01 -6.3086e-01  4.1113e-01  5.4817e-03  7.6477e-02\n",
      "  5.1074e-01 -1.6858e-01 -6.8408e-01 -1.1481e-01 -2.6465e-01 -5.0293e-01\n",
      " -1.6870e-01  8.0518e-01  2.6172e-01  1.0693e+00]\n",
      "query_embedding = [ 0.1492   0.3652   0.431    0.297   -0.1501   0.03973  0.2047   0.4993\n",
      " -0.2256   0.0961   0.3403   0.1542  -0.6387  -1.084    0.1207   0.596\n",
      " -0.4226   0.4397   0.1282   0.6636   0.535    0.0815  -0.2932  -0.3684\n",
      "  0.685    0.0987   0.2432   0.397   -0.1439  -1.096   -0.2128  -0.315\n",
      " -0.503   -0.3044  -0.2878  -0.1232  -0.0818   0.1328  -0.3333  -0.0635\n",
      "  0.168    0.7524  -0.1682  -0.12085  0.11707  0.1365   1.127   -0.27\n",
      " -0.01317 -0.319    0.1831  -0.1971   0.602    0.4424   0.01872 -1.182\n",
      "  0.234    0.1295   0.01894  0.02744 -0.293    0.295   -0.4072  -0.2195\n",
      "  0.1428  -0.3315   0.3762  -0.1813   0.3103  -0.01883  0.2391   0.3523\n",
      " -0.0644   0.1508   0.4077   0.1718  -0.3826  -0.391   -0.1285  -0.0545\n",
      "  0.462    0.545    0.4287  -0.1002  -0.5454  -0.52     0.6816  -0.2123\n",
      " -0.575   -0.0713   0.2435   0.4016  -0.0808   0.11926 -0.4072  -0.7134\n",
      " -0.7227  -1.08    -0.02348  0.5225 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-1.962e-01  3.347e-01  4.839e-01 -3.767e-01 -1.544e+00  1.088e+00\n",
      " -7.905e-01  4.519e-01  7.383e-01  1.865e-01  2.150e-01  9.116e-01\n",
      "  2.360e-01 -5.966e-02 -3.123e-01 -1.436e-01 -7.574e-02  4.282e-01\n",
      " -2.637e-01  6.523e-01 -6.061e-02  7.568e-01 -2.274e-01 -4.214e-01\n",
      " -4.548e-01  2.366e-01  7.660e-02  8.282e-02  1.334e+00  3.408e-01\n",
      "  4.109e-01  3.252e-01  1.680e-01 -4.807e-02 -8.301e-02  1.847e-01\n",
      " -2.908e-01  2.512e-01  7.251e-02 -4.436e-01 -4.949e-01  1.333e-01\n",
      "  1.057e-01 -3.193e-01 -7.393e-01  2.659e-01  1.186e+00  5.303e-01\n",
      " -2.338e-01 -1.153e+00 -2.757e-02 -1.714e-01  3.655e-01  2.389e-01\n",
      " -9.604e-01  2.664e-02  1.921e-01  8.600e-02 -8.978e-02 -3.662e-01\n",
      "  1.925e-01 -3.311e-02 -5.112e-01 -3.921e-01  5.441e-02 -4.565e-01\n",
      "  1.028e+00  8.911e-01 -3.021e-02 -3.599e-01  2.313e-01 -4.456e-01\n",
      " -1.635e-01  1.020e-01  1.219e-02  4.719e-01  2.581e-01  5.425e-01\n",
      " -2.472e-01  1.350e-03 -9.971e-01 -9.418e-02 -2.017e-01  9.663e-01\n",
      " -9.165e-01 -4.307e-01 -3.718e-01  3.687e-01 -8.340e-01 -5.303e-01\n",
      "  5.823e-02 -4.695e-01  3.560e-01  8.507e-03  9.564e-02 -1.497e-01\n",
      " -2.020e-01 -2.246e-01 -4.226e-01  5.164e-02]\n",
      "query_embedding = [-0.599    -0.1183    0.5356   -0.1494   -0.629    -0.0756   -0.9106\n",
      " -0.5654    0.3801   -0.2485   -0.226     0.483     0.4365    0.10065\n",
      " -0.2524   -0.1599   -0.6997   -0.3044   -0.4707    0.3997    0.01842\n",
      " -0.0566    0.00661  -0.52     -0.679    -0.1211   -0.5303    0.1829\n",
      "  0.5093   -0.4915    0.754     0.4026   -0.2181    0.322    -0.842\n",
      " -0.177     0.2423    0.05304   0.1594    0.1017   -0.6562    0.485\n",
      "  0.1538   -0.1829   -0.3062   -0.1148    0.1252    0.2595   -1.134\n",
      " -0.8203    0.2617    0.216    -0.469     1.113    -0.2544   -1.483\n",
      "  0.2167    0.0383    1.153    -0.10065   0.1288   -0.09454   0.01471\n",
      " -0.1164    0.6323    0.02237   0.3967   -0.01255  -0.6772    0.2338\n",
      " -0.841    -0.01808  -0.5723    0.03464   0.010345 -0.173     0.1259\n",
      "  0.096    -0.6226    0.654     0.4612    0.223    -0.918    -0.1952\n",
      " -1.043    -0.733    -1.012     0.549    -1.021    -0.264    -0.3992\n",
      " -0.2546   -0.648    -0.4883    0.11676  -0.3677   -0.2424    0.2593\n",
      "  0.4172   -0.011345]\n",
      "query_embedding = [ 1.7960e-02 -1.1316e-01  6.1084e-01 -3.6646e-01 -3.2837e-01 -2.1008e-01\n",
      "  1.5637e-01  3.6743e-02 -6.6711e-02 -4.2212e-01 -3.5303e-01  3.9429e-02\n",
      " -5.4492e-01 -9.1675e-02  1.9080e-01  7.2656e-01 -7.5488e-01  3.0688e-01\n",
      " -2.0801e-01  6.0449e-01  5.0537e-01 -5.3009e-02 -9.5654e-01  1.7126e-01\n",
      " -2.2375e-01  2.8613e-01  3.8727e-02  6.3721e-01  7.9736e-01 -9.9365e-01\n",
      " -5.2490e-01 -1.5625e-01 -1.8274e-01 -1.6125e-01  3.8379e-01  1.8738e-01\n",
      " -1.3428e-01  7.1228e-02 -2.2083e-01 -4.1235e-01 -2.5781e-01 -3.2043e-02\n",
      "  5.5469e-01 -1.8848e-01  7.6074e-01  4.3335e-02  5.1025e-01  1.0901e-01\n",
      "  4.7363e-02 -9.6484e-01  3.9307e-01 -4.0283e-01  4.9927e-01  5.5176e-01\n",
      " -2.3407e-02 -2.4219e+00 -6.9702e-02 -5.9906e-02  8.9453e-01 -1.5869e-01\n",
      " -5.0391e-01  3.5376e-01 -4.9652e-02  8.8745e-02  4.9414e-01 -5.0293e-01\n",
      "  5.4230e-02  2.5171e-01 -1.7456e-01  7.0679e-02 -1.2671e-01  9.7290e-02\n",
      " -4.7461e-01 -5.3369e-01 -2.1252e-01  8.6963e-01  1.1786e-01 -3.0225e-01\n",
      " -4.4312e-01  1.7065e-01  4.6997e-01 -3.1519e-01 -2.3376e-01 -1.4830e-03\n",
      " -1.1064e+00 -1.2861e+00  3.5474e-01  3.8623e-01 -1.4893e-01 -8.9990e-01\n",
      "  5.5115e-02  1.1310e-01 -1.3940e-01  3.6621e-01 -6.1035e-01 -7.2327e-02\n",
      "  2.0032e-01  7.5195e-02  6.1279e-02  3.2959e-01]\n",
      "query_embedding = [-3.3862e-01  2.5146e-01  8.2324e-01  2.3254e-01 -2.6978e-01  5.8887e-01\n",
      " -1.5393e-01  4.9658e-01 -3.2104e-01  1.0853e-03  1.2537e-01 -3.6719e-01\n",
      " -1.8945e-01 -2.0142e-01  4.0234e-01 -6.6357e-01 -6.5088e-01  1.5894e-01\n",
      "  5.8301e-01  1.9302e-02  3.4790e-01  1.0967e+00  3.9551e-01 -7.9639e-01\n",
      " -2.0154e-01 -5.0684e-01 -1.8457e-01 -8.0908e-01  1.0433e-03 -3.4180e-01\n",
      " -3.1519e-01  3.0176e-01 -1.9421e-01 -4.5630e-01  1.1641e+00  7.1436e-01\n",
      " -4.6240e-01  5.9912e-01  1.1017e-01 -1.3989e-01  3.3521e-01 -4.2236e-01\n",
      "  8.6963e-01 -3.4814e-01 -3.3862e-01 -4.8169e-01  4.6265e-01  4.3188e-01\n",
      "  4.1162e-01 -1.0791e+00 -4.3060e-02 -6.4062e-01  8.4839e-03  7.0312e-01\n",
      "  2.5171e-01 -2.2988e+00  8.4521e-01  3.8379e-01  7.9785e-01 -1.5710e-01\n",
      " -4.7314e-01 -1.1072e-01 -4.6973e-01  1.9849e-01  2.4194e-01  2.8564e-01\n",
      "  1.0010e+00  1.9373e-01 -5.4248e-01 -1.5002e-01  8.2764e-02 -3.4271e-02\n",
      "  2.4878e-01 -6.2256e-01  1.4319e-01  3.3691e-01 -6.4941e-01 -5.2734e-01\n",
      "  3.6450e-01 -4.0283e-01 -2.3352e-01  3.3765e-01 -5.5420e-01  2.9688e-01\n",
      " -5.3467e-01 -2.2803e-01  1.0370e-01  1.3257e-01 -1.3721e+00 -3.2568e-01\n",
      "  4.4556e-01 -4.6313e-01  1.1621e+00 -5.2588e-01 -4.6753e-01  1.9055e-01\n",
      " -4.8877e-01  1.3657e-02  4.5044e-01  2.6221e-01]\n",
      "query_embedding = [ 0.5938   0.4973   0.2566   0.1631  -0.791    0.01602 -0.666   -1.013\n",
      "  0.06494  0.1925  -0.7656   0.397   -0.05392 -0.4465   0.6973   0.0676\n",
      " -0.5454   0.09735  0.3853  -0.4355   0.05667  0.3079  -0.79     0.177\n",
      "  0.2496   0.5884   0.3777  -0.5503  -0.2356  -0.27    -0.0996   0.1444\n",
      " -0.771    0.2632   0.03314  0.2417   0.4216   0.1617  -0.09485 -0.1993\n",
      " -0.1697  -0.09064 -0.193   -0.3665   0.2198  -0.08655  0.3098   0.303\n",
      " -0.0893  -1.122   -0.05106 -0.2195  -0.1149   0.4548   0.3938  -0.5625\n",
      "  0.8003  -0.774   -0.2264  -0.2224   0.4915  -0.1545   0.641    0.273\n",
      " -0.1631   0.11743  0.0939  -1.293    0.38    -0.2783  -0.674   -0.3157\n",
      "  0.2372  -0.1757  -0.5396  -0.1718  -0.2817  -0.0697  -0.6426  -0.03543\n",
      "  0.3098  -0.3223  -0.04126  0.2476  -0.1079  -0.3538  -0.591    0.2693\n",
      " -0.6016  -0.9404  -0.712   -0.0996   0.05084  0.2219   0.562   -0.0454\n",
      "  0.2625  -0.5815  -0.0688   0.631  ]\n",
      "query_embedding = [ 0.572   -0.1411   0.301    0.1075   0.3154   0.371   -0.761   -0.547\n",
      "  0.1487   0.1587  -0.06085 -0.2974   0.825    0.05948  0.3481   0.02576\n",
      "  0.2439   0.4224   0.05432  0.231   -0.1438  -0.2219  -0.543   -0.121\n",
      "  0.3145   0.449    0.01657  0.3564   0.0517  -0.689   -0.0588  -0.6553\n",
      "  0.23    -0.1812  -0.1914  -0.10004  0.2074   0.04977 -0.4104   0.3616\n",
      "  0.246    0.5537  -0.00846 -0.3528  -0.03122 -0.2883   0.2788   0.2898\n",
      "  0.3875   0.03946  0.6826   0.1558  -0.0829   0.02533 -0.09045  0.1617\n",
      "  0.8887  -0.4546  -0.093   -0.597   -0.1771  -0.09534  0.1236  -0.3225\n",
      " -0.5405   0.0525   0.4639  -0.596    0.1431  -1.019    0.2152  -0.733\n",
      "  0.233    0.1761   0.245    0.568    0.2747  -0.572    0.2307  -0.3066\n",
      " -0.4285   0.2108   0.04138  0.6055   0.4531  -0.198    0.3564  -0.3923\n",
      "  0.745   -0.5176  -0.5493   0.3682  -0.1343   0.564    0.262   -0.8726\n",
      " -0.02629 -0.4187  -0.3586   0.1227 ]\n",
      "query_embedding = [-0.2896   -0.0405    0.6855    0.05887  -0.876     0.781     0.2307\n",
      " -0.6387    0.9185   -0.2391    0.3328    0.4424    0.04498  -0.858\n",
      " -0.3374   -0.2954   -0.2961    0.11346   0.7305    0.03702   0.6606\n",
      "  0.3647    0.4546   -0.00636   0.0143    0.5547    0.2091   -0.08496\n",
      " -0.098    -0.4993    0.1067   -0.1565   -0.01624  -0.4377   -0.57\n",
      "  0.1274   -0.5225    0.10504   0.8525   -0.4446   -0.1458    0.9033\n",
      " -0.09796  -0.753     0.1821   -0.00412   0.5435    0.7554   -0.6084\n",
      "  0.005924  0.3208   -0.0683    0.0265    0.3975    0.371    -0.6973\n",
      "  0.927    -0.05984  -0.2427    0.03442  -0.1276    0.03738  -1.004\n",
      " -0.4856    0.1942   -0.1708    0.727     0.251    -0.518    -0.7363\n",
      "  0.2612   -0.84     -0.4185    0.1351    0.3933    0.274    -0.3577\n",
      "  0.2418    0.0375   -0.3257   -0.285     1.157     0.04294   0.814\n",
      " -1.019    -0.1833   -0.1224    0.03497  -0.753    -0.3896   -0.1464\n",
      "  0.431     0.11896   0.5767   -0.07227   0.252    -0.311     0.3064\n",
      " -0.5967   -0.1648  ]\n",
      "query_embedding = [ 0.2544   0.5186   0.3633   0.044   -0.3433   0.392    0.1018  -0.6455\n",
      " -0.0168   0.5225  -0.3096   0.1163  -0.2094  -0.4912  -0.04474 -0.1084\n",
      " -0.337    0.3218   0.4739   0.01974 -0.2761  -0.2644  -0.2751   0.1798\n",
      "  0.1603   0.4114  -0.0666  -0.05612 -0.352   -0.67     0.3982   0.01884\n",
      " -0.512    0.2695  -0.2335   0.473    0.387   -0.196    0.0399  -0.289\n",
      "  0.4517   0.02545  0.0593  -0.4033   0.2466  -0.3508   0.5425  -0.414\n",
      "  0.73    -0.1741  -0.06396  0.3093  -0.225    0.4385   0.1112   0.2078\n",
      "  0.4675  -0.3125  -0.687   -0.2401   0.172    0.1138   0.272    0.04538\n",
      " -0.3188   0.00583  0.659   -0.7363   0.2485  -0.1165   0.12305 -0.5557\n",
      "  0.138   -0.498   -0.4785   0.5864  -0.215   -0.1571   0.2294  -0.182\n",
      " -0.4175   0.3486   0.3176   0.4094   0.02016  0.02773  0.2349   0.482\n",
      " -0.2964  -0.7783  -0.11395  0.259   -0.2347   0.3557   0.3953  -0.3997\n",
      " -0.2803  -0.4778  -0.224    0.736  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-6.7627e-01  6.5576e-01 -2.2888e-01  1.3257e-01 -7.6367e-01  3.5205e-01\n",
      " -1.1631e+00 -4.1479e-01 -2.2247e-02  7.9773e-02 -3.5034e-02 -9.3079e-03\n",
      "  1.5759e-01 -3.2788e-01  1.9153e-01  7.4951e-01 -3.2812e-01  6.3135e-01\n",
      " -2.3694e-01  4.1919e-01  1.0864e-01  3.2910e-01  8.5205e-02 -4.0991e-01\n",
      " -3.8110e-01 -6.7139e-02  3.0933e-01 -1.7236e-01  3.7646e-01 -9.9023e-01\n",
      "  2.3267e-01  4.2212e-01 -2.8735e-01  1.7432e-01  3.6530e-02  1.2164e-01\n",
      " -2.1216e-01  3.6987e-02  3.5767e-01 -6.4795e-01 -5.7007e-02  7.1045e-01\n",
      "  2.1863e-01 -5.0684e-01 -7.6025e-01  7.6172e-02  8.0078e-01  2.9932e-01\n",
      " -3.1177e-01 -2.0178e-01  3.3472e-01  8.2275e-01 -2.5146e-01 -2.2290e-01\n",
      " -2.8418e-01  6.5527e-01  1.0859e+00 -7.5867e-02 -6.9678e-01  7.4280e-02\n",
      " -5.1416e-01 -5.1483e-02 -3.0835e-01 -3.3203e-01 -7.2632e-02  2.3328e-01\n",
      "  9.4531e-01  2.0886e-01 -2.3010e-01 -6.9580e-01  1.6675e-01 -7.7979e-01\n",
      "  7.2815e-02  3.4131e-01 -8.2324e-01 -6.2256e-01 -1.6992e-01 -1.5845e-01\n",
      " -6.2842e-01 -2.6196e-01  1.6370e-01 -2.1216e-01 -6.2500e-02  7.8760e-01\n",
      "  2.8101e-01 -2.4329e-01 -4.3297e-03 -6.8970e-02 -6.7383e-01  3.3855e-05\n",
      " -1.6772e-01 -3.2837e-01 -1.0590e-01 -1.8591e-01 -3.4448e-01 -9.2102e-02\n",
      " -5.4901e-02 -5.2917e-02  3.3020e-02  2.9614e-01]\n",
      "query_embedding = [-9.1797e-01 -5.0842e-02 -2.9663e-02 -1.3843e-01 -7.3633e-01  2.9321e-01\n",
      " -2.2253e-01  9.0527e-01  5.5908e-01 -2.2314e-01  2.9053e-01  2.9077e-01\n",
      " -9.9121e-02 -4.7485e-01  3.9276e-02  5.0964e-03  5.2704e-02  5.7220e-02\n",
      "  1.0654e+00  3.5864e-01  7.6660e-02  2.5024e-01  2.6074e-01 -2.5635e-01\n",
      "  5.3027e-01  2.8397e-02 -5.2295e-01  3.2153e-01  9.8991e-04 -2.7734e-01\n",
      " -8.3984e-01 -6.9482e-01 -2.1606e-01 -1.3574e+00 -3.3813e-01  2.9004e-01\n",
      " -5.9521e-01 -7.7588e-01  5.8154e-01  2.5122e-01 -6.3667e-03  2.5171e-01\n",
      " -1.6064e-01  1.1816e-01 -8.5010e-01  2.5574e-02  4.9121e-01 -2.2247e-02\n",
      "  5.2295e-01 -1.1597e-02  6.1963e-01 -8.6670e-01 -6.0699e-02  3.7207e-01\n",
      " -4.9255e-02 -7.8796e-02  1.3245e-01  5.5298e-02  2.2253e-01 -3.3716e-01\n",
      " -5.0146e-01  4.6875e-01  9.8877e-02 -5.7227e-01  3.3032e-01 -5.3955e-01\n",
      "  6.1475e-01 -2.0117e-01 -2.8229e-02 -3.6353e-01 -2.9590e-01 -2.2717e-01\n",
      "  5.8154e-01  4.5972e-01 -6.7932e-02 -9.3323e-02 -1.1318e+00 -2.3792e-01\n",
      " -4.6539e-02 -4.7046e-01  2.0199e-03  4.1821e-01 -2.2913e-01  3.2568e-01\n",
      " -1.7920e+00  6.8311e-01  2.3694e-01  1.5479e-01 -5.9326e-01 -4.6191e-01\n",
      "  2.6245e-01  4.8926e-01 -2.4280e-01  1.9739e-01  7.7637e-01 -4.2798e-01\n",
      "  1.1896e-01 -1.0590e-02 -1.4915e-02  7.3242e-01]\n",
      "query_embedding = [ 0.1251    0.241     0.388    -0.4353   -0.7065    1.341    -0.733\n",
      " -0.4995    0.239     0.0792   -0.591     0.3896   -0.038    -0.2031\n",
      "  0.4875    0.4175   -0.4524    0.698    -0.3213   -0.1268   -0.277\n",
      "  0.672    -0.3518   -0.5615    0.4155    0.3882    0.765     0.5225\n",
      "  0.9546   -0.512     0.4263    0.387    -0.438     0.2484   -0.4888\n",
      "  0.588     0.5044   -0.1252   -0.242    -0.1411   -0.3403   -0.2917\n",
      " -0.001363 -0.1598   -0.3738   -0.02968   0.6445    0.3374   -0.3555\n",
      " -0.668    -0.01588  -0.11786   0.2357    0.9146   -0.2256   -0.557\n",
      "  0.4504   -0.01485  -0.0267   -0.1492    0.303     0.251     0.5127\n",
      "  0.1932   -0.44      0.1742    0.728    -0.7437   -0.01945   0.2795\n",
      " -0.1671    0.39     -0.355    -0.5303   -0.337     1.106    -0.10376\n",
      "  0.10516  -0.437     0.4966   -0.1924    0.3142    0.2583    0.1893\n",
      " -0.2922   -0.823    -0.3457    0.3276   -0.572    -0.54     -0.148\n",
      "  0.01108  -0.655     1.109     0.7686    0.176    -0.0359   -0.02278\n",
      "  0.342     0.2333  ]\n",
      "query_embedding = [ 0.2411    0.3696    0.6465    0.0761   -0.3486    0.7163   -0.6494\n",
      " -0.7544   -0.07263   0.0964   -0.9653    0.1844    0.1831   -0.5137\n",
      "  0.362     0.2625    0.003628  0.2717   -0.2817   -0.0801   -0.2006\n",
      "  0.4175   -0.628    -0.7314    0.255     0.2036    0.3235   -0.1993\n",
      "  0.4438   -0.905     0.6094    0.5986   -0.3408    0.1652   -0.1672\n",
      "  0.9756    0.4172   -0.01118  -0.2866    0.3735   -0.1587   -0.1439\n",
      " -0.1248    0.368     0.0661   -0.379     0.5874    0.2522   -0.2256\n",
      " -0.3962    0.2133    0.07404   0.2993    0.508    -0.03616  -0.2974\n",
      "  0.5933   -0.321    -0.188    -0.636     0.2544    0.1089    0.1562\n",
      "  0.2927   -0.1959   -0.0677    0.9155   -0.4558   -0.06537   0.1675\n",
      " -0.5483    0.1138    0.004032 -0.4983   -0.4321    0.4397   -0.2737\n",
      " -0.1625   -0.3052   -0.04205   0.02718   0.3093    0.2788    0.7456\n",
      " -0.6763   -0.06247  -0.339     0.371    -0.3987   -0.4333   -0.0792\n",
      " -0.10376  -0.3237    0.4695    0.1451   -0.1543   -0.5474   -0.291\n",
      "  0.11316   0.7837  ]\n",
      "query_embedding = [-0.304     0.812    -0.9683   -0.3936    0.613     0.8735   -0.5903\n",
      " -0.3247    0.428    -0.1088   -0.324     0.002815  0.359     1.089\n",
      " -0.6313   -0.719     0.1926    0.394    -0.1954   -0.6743   -0.04633\n",
      " -0.567    -0.568     1.044     0.385     0.92     -0.3591    0.9946\n",
      " -0.646     0.1514   -0.665    -0.11365  -0.5195    0.7046   -0.8706\n",
      "  0.413    -0.0669   -0.2137    0.7354   -0.5913   -0.1948   -0.7754\n",
      "  0.1501   -0.499     0.11176  -0.3867    0.00969   0.7573   -0.2206\n",
      " -0.4307    0.0735    0.04044  -0.3738    0.2969   -0.5215   -0.0672\n",
      " -0.388     0.11194   0.309    -0.672     0.6694    0.7437    0.3733\n",
      "  0.3833   -1.264    -0.6494    1.289    -0.9194   -0.5376   -1.62\n",
      "  0.03903  -0.672     1.282     0.0923   -0.3472    0.646     0.3567\n",
      " -0.4272   -0.446    -0.314     0.1962   -0.4016   -0.0955    0.0297\n",
      "  0.3481   -0.165    -0.07745  -0.0646    0.4775    0.2605   -0.01753\n",
      " -0.729    -0.1921    0.03134  -1.027    -1.257    -0.748    -0.02707\n",
      "  0.5415    0.3762  ]\n",
      "query_embedding = [-0.2522    0.2908    0.2393    0.3533   -0.04895   0.259    -0.8877\n",
      "  0.7715    0.08606  -1.018    -0.532    -0.1211    0.2113   -0.2203\n",
      " -0.2532   -0.7534    0.228     0.2296   -0.4211    0.6855   -0.06366\n",
      "  0.6846    0.24     -0.6704   -0.11285   0.03308  -0.342    -0.6816\n",
      "  0.285    -0.665    -0.5425    0.6924   -0.04407  -0.3784    1.123\n",
      "  0.2355    0.2291    0.2454   -0.4026   -0.7046    0.08344   0.068\n",
      "  0.4927   -0.3562   -0.006603 -0.139     0.3677   -0.129     0.553\n",
      " -1.477    -0.07     -0.367    -0.3035    0.6577    0.214    -1.421\n",
      "  0.943     0.004696  1.184    -0.321    -0.53      0.7964   -0.777\n",
      " -0.7505    0.259     0.725     0.6636   -0.597    -0.0959   -0.2957\n",
      "  0.1678   -0.1628    0.5073   -0.1967   -0.55      0.4822   -0.5396\n",
      " -0.5996   -0.02371  -0.1035    0.11304  -0.1052    0.01593   0.2328\n",
      " -0.499     0.1094   -0.3828    0.03983  -0.1743   -0.1417    0.3574\n",
      " -0.4321    0.552    -1.107    -0.786    -0.2925   -0.2036   -0.448\n",
      "  0.631     0.523   ]\n",
      "query_embedding = [ 0.2123   -0.5874    0.2766   -0.158    -1.157     0.3303   -0.502\n",
      " -0.6455   -0.2869    0.4282   -0.8833    0.096    -0.3225   -0.01225\n",
      " -0.4023    0.566    -0.881     0.005257  0.0677   -0.4375    0.7485\n",
      " -0.4297   -0.2595    0.3792    0.2191    0.2769   -0.887     0.3167\n",
      "  0.03226  -0.02034   0.1343    0.6094   -0.2472    0.1881   -0.6665\n",
      " -0.1997    0.06726  -0.1048    0.334    -0.02768   0.05463  -0.2896\n",
      "  0.1359   -1.327    -0.2426   -0.1048    0.8438   -0.4614    0.5386\n",
      " -1.265     0.1523    0.5884    0.3464    0.7656    0.9795   -0.9883\n",
      "  0.3813    0.09235  -0.5093    0.0786    0.5215    0.4792   -0.2944\n",
      "  0.1433    0.31      0.00824   1.274    -0.9976    0.8833   -0.6626\n",
      "  0.1294    0.2878    0.1902   -0.0722   -0.6494    0.205    -0.2369\n",
      " -0.4956    0.2262    0.1311    0.5977    0.008835 -0.192    -0.3652\n",
      " -0.3423   -0.7095   -0.03403   0.753     0.05783  -0.677    -0.308\n",
      "  0.1833   -0.00939   0.1829    0.0686    0.05447  -0.2083    0.00762\n",
      " -0.2041    0.2266  ]\n",
      "query_embedding = [-0.519   -0.01668  0.618    0.10913 -0.62     0.9077  -0.833   -0.11633\n",
      "  0.2788   0.1144  -0.1494   0.4495  -0.4998  -0.305    0.712   -0.278\n",
      " -0.6265   0.2183   0.1344  -0.322   -0.03854  0.04526  0.03223 -0.327\n",
      "  0.5825   0.5674   0.03525 -0.3953   0.2576  -0.9194  -0.4563   0.00424\n",
      " -0.146   -0.1744  -0.4355   0.3022   0.1901   0.0748   0.205   -0.02599\n",
      " -0.02261  0.2393  -0.7314  -0.05414  0.2081  -0.2717   0.3794   0.1615\n",
      "  0.0914   0.1677  -0.3113  -0.4587  -0.3462   0.578    0.606   -0.1416\n",
      "  0.3894  -0.1292  -0.2421   0.4534   0.1865   0.1598   0.2534  -0.5273\n",
      " -0.064    0.09155  0.896   -1.119   -0.1131  -0.10187 -0.364   -0.1803\n",
      " -0.4062   0.1566  -0.1164   1.141    0.0876  -0.3787  -0.2202   0.3845\n",
      " -0.4705  -0.04034 -0.4712   0.6123  -1.109    0.12103 -0.2415   0.2625\n",
      " -0.806    0.03268 -0.4612   0.1401   0.416    0.4312   0.2256  -0.3594\n",
      "  0.574   -0.3594   0.09155  0.2144 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.306    0.5776   0.714   -0.874   -0.4973   1.26    -1.343   -0.4404\n",
      "  0.39    -0.184   -0.02434  0.544   -0.11584  0.2595  -0.1625   0.07043\n",
      "  0.2507   0.5825  -0.9805   0.1732   0.02791  0.5146  -0.906   -0.54\n",
      "  0.2448   0.739    0.1812   0.3662   1.047   -0.4355   0.6177   0.06198\n",
      " -0.08997  0.2688  -0.532    0.2144   0.5317   0.0689  -0.2507  -0.1913\n",
      " -0.1519   0.2522   0.1135  -0.5034  -0.08    -0.08514  0.987    0.6377\n",
      " -0.688   -0.758   -0.1285  -0.3306  -0.1604   0.7085  -0.43    -1.2\n",
      "  0.0405   0.4167   0.2844  -0.0719   0.06946  0.2025  -0.6094  -0.197\n",
      "  0.3745   0.4482   0.796   -0.1348  -0.03032 -0.2123   0.2832   0.2354\n",
      " -0.2151   0.2374   0.3562   1.199    0.2351  -0.1649  -0.4822   0.6504\n",
      " -0.2069  -0.0685   0.1165   0.3506  -1.055   -0.2686  -0.6875  -0.01877\n",
      " -0.2507  -0.3623  -0.2186  -0.333    0.1488   0.5015   0.557   -0.2751\n",
      "  0.2335   0.11383 -0.0409   0.4875 ]\n",
      "query_embedding = [ 1.6882e-01 -2.2400e-01 -2.4597e-01 -2.5955e-02 -1.9852e-02  1.3506e+00\n",
      "  2.9932e-01 -5.8838e-01 -1.4343e-01  4.4409e-01 -2.6587e-01  4.0869e-01\n",
      "  6.6895e-02 -3.4570e-01  2.2986e-01  4.1504e-01 -1.0712e-01 -1.8234e-02\n",
      " -1.1298e-01  6.4941e-02 -1.1462e-01  2.9199e-01 -4.7070e-01  5.4626e-02\n",
      " -1.2939e-02  4.1382e-01  3.9124e-02  1.6516e-01 -6.0693e-01 -1.1123e+00\n",
      "  2.5879e-01 -3.8965e-01  3.0884e-01  1.8555e-01 -4.2267e-02 -1.6833e-01\n",
      "  1.1908e-01 -2.2839e-01 -2.9492e-01  1.1070e-02  1.6382e-01 -1.7432e-01\n",
      " -8.7646e-02  5.8984e-01 -1.5747e-01 -3.4888e-01  4.4409e-01  1.7914e-02\n",
      " -1.5222e-01 -6.3672e-01 -9.4666e-02  2.8662e-01 -2.5903e-01 -1.1481e-01\n",
      "  1.4458e-02  7.2412e-01  5.2393e-01  8.8978e-04 -3.7598e-01 -7.4756e-01\n",
      " -1.9058e-02 -3.3252e-01  5.9668e-01  2.5903e-01  3.4961e-01  1.2695e-01\n",
      "  9.6283e-03 -5.1025e-01  3.9551e-01 -4.2285e-01 -1.0266e-01 -1.4661e-01\n",
      " -1.6565e-01  2.9150e-01 -2.9443e-01  6.1475e-01 -3.4204e-01  2.6562e-01\n",
      " -4.1895e-01  5.1709e-01 -1.5625e-01  3.0493e-01  4.2407e-01  1.0186e+00\n",
      " -4.1553e-01 -8.2474e-03 -8.1104e-01  7.7637e-02  6.3135e-01 -2.5806e-01\n",
      "  1.8768e-02  1.0437e-01  4.5483e-01  2.3523e-01  8.8428e-01 -3.4692e-01\n",
      " -1.4795e-01 -7.4768e-02 -5.0928e-01  6.4502e-01]\n",
      "query_embedding = [-0.1953    0.2023    0.612    -0.29     -0.8657    0.756    -0.6465\n",
      "  0.815     0.8486   -0.449     0.1123   -0.1542    0.003717 -0.3325\n",
      " -0.2842    0.412     0.02948  -0.3782    0.4824    1.28      0.6807\n",
      "  0.7227   -0.4895    0.2333    0.09906   0.2036    0.08984   0.645\n",
      "  0.744     0.2805   -0.1528   -0.2023    0.4304   -0.6978   -0.822\n",
      " -0.377     0.2495    0.654     0.6143   -0.785     0.2487    0.1873\n",
      "  0.4248   -0.7324   -0.2191    0.242     1.149     0.721     0.00767\n",
      " -0.701     0.4382   -0.714     0.545     0.4382   -0.3853   -1.345\n",
      "  0.5063    0.647     0.4612    0.0931    0.1488    1.039    -1.124\n",
      " -1.06      0.614    -0.557     0.8457    0.3442   -0.6704   -0.333\n",
      "  0.3406   -0.4053    0.1561   -0.1154    0.7607    0.585     0.4692\n",
      "  0.1109    0.01868  -0.1814   -0.3433    0.3208    0.0948    0.263\n",
      " -1.747    -0.08026  -0.707    -0.384    -0.2401   -0.1685    0.746\n",
      "  0.4714   -0.511     0.572    -0.4465    0.3396   -0.224    -0.1465\n",
      " -0.7607    0.591   ]\n",
      "query_embedding = [ 0.00663   0.1827    0.679     0.1063   -0.981     0.5186   -0.806\n",
      " -0.8843    0.147     0.0146   -0.862     0.04276  -0.5166   -0.377\n",
      "  0.821     0.01197  -0.7617    0.04282  -0.2496   -0.579    -0.11957\n",
      " -0.1257   -0.6714    0.02599  -0.542    -0.0336    0.688    -0.2888\n",
      "  0.08655  -0.1257   -0.0941    0.107    -0.4182    0.2106   -0.6763\n",
      "  0.2328    0.9365    0.3652    0.2169    0.1415   -0.321    -0.2087\n",
      " -0.2507   -0.6904    0.328    -0.1785    0.61      0.338     0.0895\n",
      " -0.8247   -0.1394   -0.3875   -0.3198    0.6694    0.553    -1.486\n",
      "  0.7104   -0.1721   -0.002619  0.218     0.736     1.145     0.6665\n",
      " -0.3916   -0.10016   0.7295    0.6997   -1.138     0.81     -0.2957\n",
      " -0.2974   -0.2046   -0.718     0.2366    0.09625   1.155     0.1271\n",
      " -0.2231   -0.965     0.01315   0.1653   -0.04498  -0.4246    0.243\n",
      " -1.353    -0.1553   -0.2352    0.02849  -0.871    -0.506    -0.7485\n",
      "  0.331    -0.0751    0.2324    0.3484    0.778     1.129    -0.662\n",
      "  0.2034    0.1807  ]\n",
      "query_embedding = [ 0.3154   0.6733   0.4294  -0.3232  -1.085    0.612   -0.1209   0.1702\n",
      "  0.3271  -0.9434  -0.2417   0.5747   0.707   -0.0826   0.4705  -0.524\n",
      " -0.3174  -0.4055  -0.02411  0.6357   0.2446   0.7686   0.01755 -0.1543\n",
      " -0.1431   0.3833  -0.501   -0.3027   0.3262   0.976    0.2515   0.4739\n",
      "  0.464   -0.305   -0.5146   0.4604  -0.729    0.2163   0.0992  -0.1561\n",
      " -0.348    0.2183  -0.0541  -0.8496   0.3801   0.5566  -0.521    0.4832\n",
      " -0.1058  -0.607    0.3064  -0.693   -0.1022   0.3645  -0.5483  -0.882\n",
      "  0.344   -0.2181   1.049    0.1343   0.7563   0.974   -0.2507  -0.668\n",
      "  0.02473  0.623    0.3193  -0.1665   0.1891  -0.5557  -0.272   -0.6074\n",
      " -0.1378   0.5356   0.756    0.8647  -0.2676   0.3508  -0.408   -0.1362\n",
      "  0.3086   0.3503  -0.2996   0.5747  -2.139   -0.295   -0.0335   0.51\n",
      " -1.187    0.02914  0.0738  -0.502    1.056   -0.05762 -0.725   -0.03516\n",
      "  0.1534   0.1885  -0.2362  -0.699  ]\n",
      "query_embedding = [-0.2717  -0.10077  0.828   -0.2913  -0.4084   0.493   -0.6353  -0.1858\n",
      "  1.029    0.048   -0.1025  -0.142    0.3025  -0.704    0.2869   0.2896\n",
      " -0.478   -0.587    0.2394   0.28     0.2988  -0.03293 -0.13     0.02788\n",
      "  0.6865  -0.1538  -0.3691   0.622    0.2522  -0.3943  -0.112    0.0173\n",
      "  0.1708  -0.3604  -0.6255   0.2262   0.155   -0.1168  -0.3652  -0.2944\n",
      " -0.2725   0.3027   0.1284  -0.2256  -0.302    0.1284   1.352    0.02151\n",
      " -0.1306   0.4888   0.1658  -0.2913   0.1632   0.1729  -0.5854   0.07587\n",
      "  0.572   -0.251   -0.3967  -0.261   -0.5635  -0.0865   0.651    0.2522\n",
      " -0.05222 -0.4739   0.615    0.2578   0.461   -0.574   -0.398   -0.2905\n",
      " -0.2334   0.1214  -0.3284   0.2367   0.1552   0.1735   0.2347  -0.1472\n",
      "  0.05432  0.1803   0.03069  0.1798  -0.5166   0.3718  -0.1622   0.501\n",
      " -0.458   -0.416    0.2336  -0.02525 -0.07446 -0.3196  -0.11694  0.0696\n",
      " -0.1561   0.2146  -0.2166   0.5776 ]\n",
      "query_embedding = [-4.1016e-01  2.9004e-01  4.0210e-01  1.4636e-01 -2.9199e-01  2.4646e-01\n",
      "  1.7651e-01  9.6729e-01 -1.7212e-01 -2.7148e-01  2.2290e-01 -7.0679e-02\n",
      " -8.3447e-01 -1.0000e+00  6.2402e-01  3.3472e-01 -1.4246e-01  3.0716e-02\n",
      "  1.0052e-01  5.8838e-01  9.9609e-01  1.0779e-01  6.7017e-02 -4.3530e-01\n",
      "  7.8223e-01  2.9858e-01 -1.5625e-01  7.6562e-01  8.6365e-02 -7.8760e-01\n",
      " -5.2734e-01 -2.8540e-01 -6.8457e-01 -3.5474e-01 -1.8042e-01 -3.5596e-01\n",
      " -2.3840e-01  2.0679e-01  7.6843e-02 -4.4727e-01  9.7046e-02  6.4600e-01\n",
      " -7.9834e-02 -1.0376e-01 -1.4075e-01  7.8809e-01  1.3174e+00 -3.9209e-01\n",
      "  3.7451e-01 -1.4258e-01 -7.9163e-02 -4.3262e-01  5.7520e-01  1.9897e-01\n",
      "  2.4438e-01 -5.0488e-01  5.8740e-01  4.3921e-01 -5.9113e-02 -2.4780e-01\n",
      " -3.5620e-01  4.0210e-01 -5.8594e-03 -2.5342e-01 -5.6549e-02 -7.7197e-01\n",
      "  7.9932e-01 -6.3525e-01 -1.5686e-01 -6.0303e-01  2.7612e-01 -2.7563e-01\n",
      "  3.9722e-01 -4.0436e-03 -1.0848e-05  7.3303e-02 -5.0830e-01 -5.7617e-01\n",
      "  1.4124e-01 -6.4111e-01 -5.0977e-01  3.6426e-01 -3.6285e-02  7.8369e-01\n",
      " -4.3481e-01  1.6541e-01  1.7871e-01  8.2458e-02 -8.9990e-01 -6.4551e-01\n",
      " -3.1494e-02  4.3188e-01 -1.9434e-01 -1.1652e-01  1.6504e-01 -7.5562e-02\n",
      " -1.8616e-01 -2.7344e-01  8.2169e-03  1.0146e+00]\n",
      "query_embedding = [ 0.3694    0.2659    0.5596   -0.02826  -0.832     0.5107    0.1764\n",
      " -0.2664   -0.1128    0.004486 -0.2542    0.5215    0.001541 -0.7993\n",
      "  0.3286   -0.801    -0.8       0.4033    0.2847   -0.2642   -0.11426\n",
      "  0.2468   -0.198    -0.5576    0.2134    0.6924    0.3271    0.5635\n",
      "  0.4185   -1.132    -0.1484   -0.427    -0.601     0.3508   -0.5845\n",
      "  0.3208    0.565     0.3386    0.01001  -0.4863   -0.1021   -0.1025\n",
      "  0.08124  -0.2208    0.5273   -0.06665   0.5703    0.1523    0.5215\n",
      " -0.1505   -0.2081    0.1771    0.5845    0.4023    0.2004   -0.2314\n",
      "  0.33      0.0334   -0.2356    0.1769    0.1537    0.08356   0.394\n",
      "  0.2637   -0.768     0.2455    0.3977   -0.5356   -0.2844   -0.4216\n",
      " -0.401     0.01582  -0.2954   -0.4263    0.2537    1.254     0.05978\n",
      " -0.3735   -0.3218   -0.05856  -0.5757   -0.02206   0.211     0.683\n",
      " -0.3171   -0.1127    0.27     -0.3086   -0.9136   -0.8228   -0.1516\n",
      "  0.2167   -0.4998    0.562     0.4944   -0.02405   0.1932   -0.0849\n",
      " -0.1699    0.5596  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.005566  0.1321    0.608    -1.029    -0.149     0.925    -0.3281\n",
      " -0.338    -0.02237  -0.02914  -0.523    -0.1656    0.1388   -0.3433\n",
      " -0.155     0.317    -0.5728    0.4846   -0.404     0.1761    0.657\n",
      " -0.2942   -0.4238   -0.4485    0.5176    0.967     0.1631    0.6914\n",
      "  0.8584    0.0803    0.095     0.1432   -0.4849    0.23     -0.47\n",
      " -0.07245   0.2299   -0.4377    0.2142    0.1284   -0.5425    0.04324\n",
      " -0.2379    0.1862    0.75     -0.11115   0.3562    0.2137    1.013\n",
      "  0.07996  -0.3855    0.11523   0.1891    0.882     0.0662   -1.222\n",
      "  0.704     0.07184  -0.5312    0.142     0.9224    0.7046   -0.2083\n",
      "  0.005276  0.2625   -0.0347   -0.4062   -0.2961    0.02415  -0.0339\n",
      "  0.1144   -0.4744   -0.4941   -0.0707   -0.03317   0.8955    0.748\n",
      " -0.1741   -0.9604    0.6836    0.586     0.02336   0.2084    0.2297\n",
      "  0.243    -0.1493   -0.0863    0.04047   0.2115   -0.3203   -0.3123\n",
      "  0.06216  -0.8623    0.83      0.3562    0.1912   -0.2211    0.01491\n",
      "  0.4915   -0.4683  ]\n",
      "query_embedding = [ 0.2358    0.3962    0.2786   -0.1858   -0.6035    1.015    -0.9385\n",
      " -0.557     0.0641   -0.3337   -0.3281    0.523    -0.02011  -0.374\n",
      "  0.05402  -0.556    -0.736     0.2458   -0.3408    0.446    -0.1763\n",
      "  0.04944  -0.4185    0.2336    0.3477    0.3955    0.4507    0.2305\n",
      "  0.59     -0.517     0.335    -0.2822   -0.4895    0.307    -0.9185\n",
      " -0.03897   0.494     0.634     0.264     0.04373  -0.06854  -0.014854\n",
      "  0.03168  -0.4036   -0.2231    0.4324    0.8926    0.5186    0.279\n",
      " -0.2842   -0.1779   -0.4458    0.6       0.9873    0.556    -0.4048\n",
      "  0.3428   -0.6016   -0.3982    0.382     0.7085    0.7275    0.375\n",
      " -0.624    -0.05072   0.11584   0.6006   -0.665    -0.09375  -0.504\n",
      " -0.09735   0.4087    0.08673  -0.3054    0.5874    1.537     0.1791\n",
      " -0.048    -0.677     0.4924   -0.3105    0.5054    0.4563    0.621\n",
      " -1.023    -0.1318   -0.557    -0.0649   -0.7256   -0.4946    0.0382\n",
      "  0.1709   -0.1135    0.2634    0.511     0.2491    0.716    -0.3704\n",
      "  0.1932   -0.007317]\n",
      "query_embedding = [-0.1389    0.4202    0.834    -0.008484 -0.5635    1.038    -0.979\n",
      " -0.3008    0.1846   -0.3318   -0.536     0.561    -0.01805  -0.2388\n",
      "  0.3232   -0.385    -0.973     0.3972   -0.5303    0.477    -0.405\n",
      "  0.3718   -0.8726   -0.2367    0.2045    0.8994    1.063     0.998\n",
      "  1.255    -0.597     0.04843  -0.07336  -0.776     0.2316   -1.091\n",
      "  0.2104    0.8735    0.7173    0.1267   -0.2156   -0.09393  -0.2363\n",
      "  0.1239   -0.2542   -0.04425   0.5527    0.771     0.5767    0.0903\n",
      " -0.8335   -0.122    -0.5337    0.8247    0.782     0.1852   -1.019\n",
      "  0.3186    0.286    -0.06287   0.01894   0.575     0.564     0.2223\n",
      " -0.695    -0.2345   -0.06354   0.65     -0.7764   -0.1366   -0.5786\n",
      "  0.0355    0.1444   -0.2695   -0.2605    0.5825    1.708     0.336\n",
      " -0.09155  -0.539     0.6367   -0.4675    0.10297   0.41      0.673\n",
      " -0.8047   -0.57     -0.1622   -0.3225   -0.719    -0.1917   -0.2042\n",
      "  0.1924   -0.5215    0.831     0.002758  0.2461    0.1218   -0.8145\n",
      " -0.4485    0.3582  ]\n",
      "query_embedding = [-0.1554   0.3953   0.1969  -0.629   -0.761    0.3538  -0.6465   0.12103\n",
      " -0.299   -0.0392   0.01974 -0.277    0.5195  -0.7     -0.956   -0.704\n",
      "  0.63    -0.0088  -0.3      0.8286   0.191    0.81    -0.5347  -0.6416\n",
      "  0.3945   0.1395  -0.3396   0.02588  0.932   -0.3135  -0.3247   0.7217\n",
      "  0.911    0.1252   0.6577  -0.358   -0.4976   0.3193   0.3599  -0.4282\n",
      "  0.07404  0.10065  0.72    -0.1511  -0.05612 -0.0632   1.139    0.5786\n",
      " -0.1738  -1.108   -0.2373  -0.5723   0.66     0.406   -0.2815  -1.051\n",
      "  0.1686   0.285    0.651   -0.431    0.2097   0.3599  -0.689   -0.4226\n",
      "  0.4119  -0.2133   0.546    1.153   -0.1192  -0.1582   1.008    0.0953\n",
      " -0.1561  -0.2411   0.648    0.3777  -0.09576 -0.2126  -0.3423  -0.342\n",
      " -0.399   -0.05154 -0.02853  0.5093  -1.015   -0.385   -0.6787  -0.2081\n",
      " -0.7964  -0.621   -0.124   -0.2627   0.628    0.1398  -1.047   -0.1302\n",
      " -0.9976  -0.58    -0.3635   0.7383 ]\n",
      "query_embedding = [ 0.508     0.178    -0.02151  -0.4321    0.0788    0.792    -0.1808\n",
      " -0.054     0.0792   -0.999    -0.2312    0.0859   -0.371    -0.2717\n",
      "  0.1724   -0.883    -0.3003    0.6255   -0.781     0.5186   -0.1307\n",
      "  0.1287    0.07745  -0.4006    0.3608    1.306     0.4973    0.1886\n",
      "  0.873    -0.3328   -0.3462   -0.194    -0.3938    0.1996   -0.3242\n",
      " -0.3562    0.4436    1.045     0.2351   -0.4797    0.06287   0.10504\n",
      " -0.1311   -0.0883    0.0879    0.2825   -0.1222   -0.237     0.4512\n",
      " -0.3428    0.01926  -0.2273    0.3572    0.5283    0.3467   -1.851\n",
      " -0.1721    0.4875    0.1294   -0.5977    0.823     0.5605   -0.2205\n",
      " -0.1986   -0.3872    0.1447    0.882    -0.499    -0.1342   -0.5093\n",
      "  0.38      0.3154   -0.3394   -0.402     0.851     1.84      0.11566\n",
      " -0.466    -0.5625   -0.03665  -0.2576    0.06903   0.599     0.3638\n",
      " -0.731    -0.1741   -0.08624  -0.5967   -0.345    -0.3572    0.0684\n",
      " -0.3806   -0.0866    0.9614   -0.3308   -0.326     0.001855 -0.4653\n",
      "  0.0346    0.693   ]\n",
      "query_embedding = [-0.1846    0.1555    0.4805   -0.532    -0.7373    0.857     0.010086\n",
      "  0.15      0.6216   -0.1936   -0.2053   -0.01187   0.2349   -1.2\n",
      "  0.0735    0.1062   -0.537     0.032     0.7725   -0.161     0.013016\n",
      "  0.532    -0.1251    0.1892    0.6665   -0.3135    0.1973    0.04358\n",
      "  0.1886    0.12006  -0.1737   -0.519    -0.423    -0.2123   -0.3118\n",
      "  0.09015  -0.33      0.371     0.364    -0.154    -0.1495   -0.1877\n",
      "  0.5938   -0.11676  -0.12     -0.1295    0.4172    1.167     0.12146\n",
      "  0.1583    0.1743   -0.4888    0.2751   -0.1835   -0.08777  -0.06647\n",
      " -0.04468   0.1418   -0.5703    0.122    -0.1106   -0.1445   -0.3818\n",
      "  0.3179   -0.4536   -0.0564    0.8467    0.677    -0.10614  -0.1517\n",
      "  0.2189   -0.3276   -0.0528   -0.8594    0.02893   0.589    -0.01241\n",
      "  0.1576    0.601    -0.3135   -0.05426   0.02713  -0.10406   0.2267\n",
      " -0.0854   -0.908    -0.11206   0.3196   -0.9937   -0.1027    0.2471\n",
      " -0.2725    0.1824    0.4036    0.01743   0.0779    0.003368  0.9478\n",
      " -0.773     0.3596  ]\n",
      "query_embedding = [ 0.646     0.3984    0.2798    0.02727  -0.2844    0.734    -0.68\n",
      " -0.1738   -0.3853    0.00487  -0.3218    0.4958   -0.1399    0.1081\n",
      "  0.4746    0.2764    0.1807    0.502    -0.2454    0.1566    0.12366\n",
      " -0.0858   -1.143    -0.515     0.1031    0.555     0.1637    0.6357\n",
      "  0.2096   -0.798     0.5566    0.2108   -0.8887   -0.03363  -0.1597\n",
      "  0.8145    0.5483   -0.5166   -0.387     0.0529    0.1012    0.3943\n",
      "  0.4941    0.4502   -0.913    -0.3909    1.188    -0.708     0.1263\n",
      " -0.864    -0.564    -0.1951    0.4678    0.287    -0.3289    0.003073\n",
      " -0.0934   -0.4124   -0.06305  -0.1184    0.1276   -0.125     0.11566\n",
      " -0.1382   -0.586    -0.296     0.2369   -0.335    -0.01192  -0.5986\n",
      " -0.3489   -0.1296    0.2306   -0.532     0.7256    0.856     0.3557\n",
      " -0.00476  -0.4163   -0.1852   -0.7407   -0.1797    0.7607    0.3743\n",
      " -0.329     0.04428   0.2267    0.8496   -0.4724   -0.4941   -0.1392\n",
      "  0.08777  -0.2056    0.549     0.865    -0.05313   0.1757   -0.6885\n",
      " -0.005608  0.354   ]\n",
      "query_embedding = [-0.307     0.2644    0.717     0.01052  -0.2272    0.7485   -0.3337\n",
      " -0.346     0.5005    0.1766   -0.2242    0.6855    0.1611   -0.961\n",
      "  0.5234    0.523    -0.721     0.4058    0.0645   -0.415    -0.3018\n",
      "  0.2076    0.051    -0.3662   -0.0745   -0.1327   -0.2512   -0.07904\n",
      " -0.5435    0.002161 -0.692    -0.3765   -0.378     0.00851  -0.548\n",
      " -0.1067    0.04425  -0.3882    0.3972   -0.12134   0.1344   -0.2332\n",
      " -0.2499    0.3904   -0.1985    0.2722    0.9307    0.2942   -0.1946\n",
      " -0.28     -0.1179   -0.2347    0.4705   -0.1653   -0.10455   0.6665\n",
      "  0.2925   -0.476    -0.4668   -0.637    -0.0226   -0.3157   -0.1874\n",
      "  0.3337   -0.8623   -0.586     0.0972    0.4058    0.5176   -0.1389\n",
      " -0.1149   -0.5415    0.1707    0.03384  -0.4111    0.3071   -0.2076\n",
      "  0.001322  0.2876   -0.893    -0.04562  -0.605     0.0852   -0.1694\n",
      " -0.004486 -0.1561    0.419    -0.09863  -0.648     0.06744   0.1404\n",
      " -0.497    -0.1646    0.0976    0.03455   0.0817   -0.2603   -0.2145\n",
      " -0.9653    0.522   ]\n",
      "query_embedding = [-0.4253   -0.5503    0.2942    0.519    -0.33      0.8755    0.3909\n",
      "  0.452    -0.2546   -0.429    -0.9004   -0.644    -0.00995  -0.2355\n",
      "  1.04     -0.1642   -1.053    -0.9844    0.4644   -0.895     0.1049\n",
      " -0.1744   -1.068     1.408    -0.1427    1.226     0.587    -0.2537\n",
      "  0.4143   -1.034     0.1483    0.088    -0.713    -0.6304   -0.7915\n",
      " -0.02429   0.89      0.0889    0.5405    0.4055   -0.04385   0.5405\n",
      " -0.424    -1.633     0.1549    0.4038    0.2191   -0.0285    0.10266\n",
      "  0.0208   -0.005207 -0.1974   -1.505     0.2257    0.725    -0.2194\n",
      "  0.4448    0.4119   -0.7534   -0.1428    0.1384    1.088     0.2805\n",
      "  0.4512   -0.644    -0.01401   1.206    -1.534     0.843    -0.4539\n",
      " -0.4814   -0.3958    0.4495    0.3347    0.04782   0.8525   -0.6787\n",
      "  0.5103   -0.6733    0.3433    0.07166  -0.9067   -0.564     0.62\n",
      " -1.931    -0.517     0.2966    0.56     -1.051     0.115     0.005287\n",
      " -0.584     0.184     0.296     0.6846    0.563     0.4958   -0.4707\n",
      "  0.5015   -0.705   ]\n",
      "query_embedding = [ 4.1772e-01  1.1926e-01  5.7422e-01  1.0632e-01 -1.3855e-01  9.2139e-01\n",
      " -3.1396e-01 -3.3374e-01 -1.2549e-01 -3.4546e-01 -5.2979e-01 -8.9417e-02\n",
      " -3.1128e-01 -7.4805e-01 -5.8777e-02 -8.8196e-02 -3.5376e-01  4.3652e-01\n",
      " -2.5049e-01  3.5797e-02  4.7882e-02 -3.1067e-02 -3.5254e-01 -6.8848e-01\n",
      "  2.9688e-01  8.8965e-01  1.7993e-01  6.5137e-01  6.1572e-01 -1.0127e+00\n",
      " -2.4573e-01 -6.7810e-02 -2.3328e-01  2.5586e-01  5.4703e-03  1.9458e-01\n",
      "  9.3750e-02  5.7812e-01 -9.8724e-03 -6.3965e-01  2.8491e-01 -9.2773e-02\n",
      " -3.1494e-01 -2.6123e-01  5.1611e-01  1.3135e-01  2.5073e-01  3.4515e-02\n",
      "  2.3926e-01 -1.2048e-01  2.4658e-02 -3.8501e-01  2.3669e-01  7.1240e-01\n",
      " -3.7567e-02 -8.8330e-01 -4.2297e-02 -3.7567e-02  1.7807e-02 -4.8975e-01\n",
      "  5.7373e-01  7.5439e-01 -9.5367e-03 -2.8564e-02 -1.1981e-01 -1.3171e-01\n",
      "  4.7388e-01 -3.1030e-01 -6.3095e-03 -3.9819e-01 -2.4707e-01  2.2827e-01\n",
      " -2.1271e-02 -7.7002e-01 -2.5464e-01  1.3262e+00  8.2703e-02 -4.1431e-01\n",
      "  1.0529e-01  1.4694e-02 -1.5771e-01 -1.9238e-01 -1.2042e-01 -1.4746e-01\n",
      " -6.3623e-01 -8.2471e-01 -2.3267e-01  1.1841e-01 -4.4604e-01 -2.2583e-01\n",
      " -1.2016e-03  1.2817e-01  1.1377e-01  2.4255e-01 -3.4473e-01 -3.8281e-01\n",
      " -3.3398e-01 -1.7603e-01  3.1104e-01  4.7437e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.5264    0.4827    0.2927   -0.2766   -1.205     0.594    -1.33\n",
      " -0.1131    1.064     0.11365  -0.457    -0.0744    0.5464   -0.5186\n",
      " -0.0902   -0.189    -0.25     -0.477    -0.094     0.56      0.002058\n",
      "  0.3757    0.2622    0.02283  -0.2155   -0.193     0.02556  -0.3262\n",
      "  1.212    -0.6978    0.1196    0.3267   -0.04794  -0.377    -0.2389\n",
      "  0.11365   0.19      0.4558    0.3599   -0.2598   -0.697     0.05548\n",
      "  0.093     0.185    -0.1047   -0.03958   0.8203    0.149     0.2218\n",
      " -0.6045   -0.0894    0.362     0.234     0.25     -0.10187   0.2421\n",
      "  0.7896   -0.836    -0.1788   -0.3962   -0.2291   -0.09863   0.1969\n",
      "  0.1442   -0.1313   -0.05115   0.4614    0.12085  -0.3103    0.11084\n",
      "  0.0394   -0.5146   -0.4377   -0.545    -1.04     -0.04333  -0.1517\n",
      " -0.2217   -0.3438    0.1355   -0.1781   -0.4978   -0.6377    0.3643\n",
      " -0.7266   -0.11206  -0.6694   -0.268    -0.931    -0.6035    0.1837\n",
      " -0.3904   -0.371    -0.6597   -0.2705   -0.5107   -0.762    -0.2266\n",
      " -0.0978    0.5825  ]\n",
      "query_embedding = [ 5.8057e-01  5.7520e-01 -9.9976e-02 -2.9980e-01 -1.2317e-01  7.0166e-01\n",
      " -4.0601e-01 -7.9102e-01 -9.8877e-02 -1.4929e-01 -3.1323e-01  2.6953e-01\n",
      "  7.9004e-01  3.9600e-01 -4.4775e-01 -7.5586e-01  2.1362e-01  8.3887e-01\n",
      " -1.3086e+00  8.9844e-01 -1.8982e-01 -2.2546e-01 -4.2773e-01 -1.6016e-01\n",
      " -8.7280e-02  4.1919e-01  2.2461e-01 -2.6904e-01 -1.3025e-01  4.1284e-01\n",
      "  3.7354e-01  2.3206e-01 -5.3369e-01  7.4097e-02 -4.8145e-01 -6.4758e-02\n",
      " -5.2002e-02  3.4302e-01  1.1237e-01 -2.3254e-01 -9.8779e-01  2.6587e-01\n",
      "  1.0098e+00  4.2603e-01 -8.8770e-01 -1.2734e+00  2.7246e-01  2.1652e-02\n",
      "  4.3018e-01 -9.0283e-01 -1.0147e-03  5.9717e-01 -3.8605e-02  5.8740e-01\n",
      " -1.0150e-01 -2.2656e+00  4.3945e-01 -3.1982e-01  8.9258e-01 -2.1643e-01\n",
      "  8.2129e-01  5.6183e-02 -4.6783e-02 -7.1826e-01  1.2115e-02  1.5662e-01\n",
      "  2.4390e-01  8.5144e-03  1.6162e-01 -7.2021e-01  2.6660e-01  4.2139e-01\n",
      "  1.4001e-01 -6.1035e-01  4.2456e-01  6.3416e-02 -3.1934e-01 -4.7266e-01\n",
      " -7.4072e-01 -4.3604e-01 -5.6836e-01  6.7480e-01  5.3375e-02  5.5518e-01\n",
      " -6.3818e-01  1.9189e-01 -5.6982e-01  1.5808e-01  1.1986e-02 -2.4414e-01\n",
      " -6.7529e-01 -4.9951e-01 -2.3010e-01  6.4941e-01 -3.9526e-01 -5.2832e-01\n",
      "  9.2590e-02 -7.9443e-01  2.2290e-01  9.0479e-01]\n",
      "query_embedding = [-0.3066   -0.06323   0.03488  -0.1909   -0.7354    0.00856   0.1096\n",
      "  0.3506   -0.01727  -0.1555    0.166     0.548     0.0995   -0.793\n",
      "  0.005028 -0.1854   -0.4702    0.1768    0.3542   -0.4429    0.05655\n",
      " -0.0721   -0.0948   -0.089     0.1793    0.00867   0.3093    0.5933\n",
      "  0.2896   -0.09656   0.10016  -0.4019   -0.1934    0.3723   -0.02042\n",
      "  0.3005   -0.2382   -0.07135   0.05432  -0.5586    0.3428    0.439\n",
      " -0.1652    0.397    -0.2627   -0.4888    0.1287    0.799    -0.0392\n",
      "  0.1985   -0.3123   -0.2142    0.4116   -0.2343   -0.1995    0.3694\n",
      "  0.4587    0.202    -0.844    -0.5967    0.1337   -0.1814   -0.456\n",
      " -0.2452    0.02249  -0.2664    0.4875    0.437     0.0174   -0.23\n",
      " -0.05627   0.3435    0.2397    0.09033  -0.4534    0.5547    0.406\n",
      "  0.5806    0.0834   -0.4453   -0.0788    0.08044   0.04617   0.0667\n",
      " -0.002344 -0.646    -0.0887   -0.3157   -0.8823    0.1499   -0.4146\n",
      " -0.1073    0.2627    0.1058    0.7324    0.02605  -0.01859   0.1885\n",
      " -0.7344   -0.2445  ]\n",
      "query_embedding = [ 1.3702e-02  4.0503e-01  4.1064e-01 -7.0801e-01 -3.8428e-01  1.1641e+00\n",
      " -1.1514e+00 -7.4609e-01  3.7598e-01  3.1860e-01 -1.0332e+00  3.4448e-01\n",
      "  9.3262e-02 -3.0200e-01  3.4261e-04 -2.6318e-01 -1.3125e+00  1.2585e-01\n",
      " -8.3154e-01 -2.2229e-01  2.4646e-01 -1.8640e-01 -3.5449e-01 -3.0298e-01\n",
      "  4.8859e-02  3.7402e-01  4.2407e-01  5.1904e-01  7.9150e-01 -8.2947e-02\n",
      "  3.2501e-02  5.7812e-01 -1.8848e-01  3.4766e-01 -9.0674e-01  1.7725e-01\n",
      "  1.0684e+00  9.9792e-02 -1.2524e-01  7.3425e-02 -6.6162e-01 -4.5898e-01\n",
      " -1.9104e-01 -3.6182e-01  4.0918e-01 -3.9453e-01  6.7041e-01  3.5254e-01\n",
      "  4.3555e-01 -8.0713e-01 -1.5027e-01 -3.4106e-01 -2.0599e-02  1.2920e+00\n",
      "  2.1118e-01 -1.7139e+00  3.8208e-01 -9.3994e-02  5.1465e-01 -3.0298e-01\n",
      "  7.9346e-01  4.3921e-01  7.6953e-01 -2.4414e-01  3.7329e-01  3.8867e-01\n",
      "  7.5293e-01 -6.3525e-01  1.3977e-01 -2.2205e-01 -1.9932e-03  1.0040e-01\n",
      " -7.5732e-01 -3.8306e-01 -1.6357e-01  1.4189e+00  4.4067e-01 -6.0059e-02\n",
      " -1.0791e+00  7.1045e-01  5.7666e-01  1.3391e-01 -4.8248e-02  4.8279e-02\n",
      " -9.4873e-01 -6.3623e-01 -3.7354e-01  2.4341e-01 -2.5830e-01 -7.3096e-01\n",
      " -4.9878e-01  1.0896e-04 -9.2578e-01  6.8457e-01  4.3628e-01  3.3374e-01\n",
      "  2.5708e-01 -1.1932e-01  3.8770e-01  2.9688e-01]\n",
      "query_embedding = [ 0.3904    0.005074  0.6797    0.4858   -0.1644    0.1088   -0.3462\n",
      " -0.2725    0.4062   -0.06604  -0.4526    0.08105   0.3037    0.03027\n",
      "  0.4885    0.5396    0.2136   -0.04843   0.0797    0.0897   -0.5054\n",
      "  0.1942   -0.5513   -0.1483    0.274     0.0633    0.637     0.6387\n",
      "  0.003239 -0.872     0.079    -0.2004    0.3096    0.565    -0.2313\n",
      "  0.1807   -0.05423   0.3499   -0.5884    0.04425   0.02916   0.8096\n",
      "  0.465     0.471    -0.2219   -0.1914    0.8833   -0.1699   -0.1261\n",
      " -0.2952    0.4497    0.06216   0.581     0.08386  -0.01614  -0.1234\n",
      "  0.3296   -0.7173    0.3464   -0.1809    0.2888   -0.1262   -0.1936\n",
      "  0.595     0.2217   -0.7183   -0.002949 -0.18      0.1698   -0.546\n",
      " -0.1973   -0.1907    0.1973   -0.81      0.1881    0.839    -0.2401\n",
      " -0.2979   -0.781    -0.1898    0.0632   -0.2563    0.403     0.2224\n",
      " -0.5195   -0.0617    0.1444    0.1664   -0.07     -0.6245   -0.15\n",
      " -0.0042   -0.631     0.5073    0.3892   -0.3308    0.2123   -0.4336\n",
      " -0.3691    0.5225  ]\n",
      "query_embedding = [ 0.2783    0.4102    0.6494    0.2058    0.2935    0.1288   -0.1394\n",
      " -0.3645    0.1532   -0.1605   -0.853     0.788     0.5376   -0.1582\n",
      " -0.181    -0.1451   -0.11743   0.0497    0.1685    0.3928   -0.3242\n",
      " -0.1569   -0.128    -0.6274    0.3877    0.36      0.0738   -0.3933\n",
      " -0.2668   -1.122     0.0633   -0.299    -0.4338    0.02557   0.2375\n",
      "  0.5503    0.3042    0.336    -0.05353  -0.5444    0.1321   -0.1451\n",
      " -0.0678   -0.4138   -0.348    -0.3945    0.487     0.2167   -0.008514\n",
      " -0.02057  -0.1855   -0.3806    0.1666    0.602     0.167     0.1718\n",
      "  0.5405   -0.3406   -0.6504   -0.1427   -0.01825   0.1372   -0.5186\n",
      " -0.7607   -0.09064   0.578     0.843    -0.7905    0.421    -0.1365\n",
      "  0.1652    0.01994   0.2742   -1.062     0.3994    1.107     0.25\n",
      " -0.456     0.0938   -0.3062   -0.6465    1.046     0.3447    0.49\n",
      "  0.2568    0.2219    0.5293   -0.108    -0.7734   -0.235    -0.4949\n",
      " -0.0855   -0.1326    0.362     0.3337    0.2534    0.2338   -0.266\n",
      "  0.03198  -0.1023  ]\n",
      "query_embedding = [ 0.4614    0.1848    0.0897    0.405     0.1625   -0.605    -1.018\n",
      " -0.8086   -0.507    -0.5273   -0.3164    0.4404   -0.352    -0.1782\n",
      "  0.07776  -0.348    -0.6045    0.4287    0.37      0.481    -0.058\n",
      "  0.1225   -0.3054    0.1434    0.1026   -0.5483   -0.04843  -0.76\n",
      " -0.8145    0.003096 -0.2494    0.4717   -0.643     0.05743  -0.433\n",
      "  0.2484    0.5415    0.007133 -0.2751   -0.4822   -0.2081   -0.5273\n",
      "  0.03983  -0.004944  0.05453  -0.405     0.3757   -0.493     0.2009\n",
      " -1.664     0.2305   -0.01299   0.08795   0.7783    0.10114  -2.342\n",
      "  0.04025  -1.211     1.89      0.432     0.3435    0.5527   -0.2046\n",
      " -0.05676   0.947     0.2695    0.2683    0.2993    1.205    -0.0846\n",
      "  0.2847   -0.1781    0.1514   -0.3745   -0.5015    0.1737    0.1898\n",
      " -0.2131   -1.103    -0.3074    0.316     0.4592   -0.0178   -0.626\n",
      " -0.7236   -0.6147   -0.559     0.3074    0.2214   -0.8926   -1.095\n",
      "  0.013275  0.1493   -0.357    -0.7983    0.09424   0.2502   -0.589\n",
      "  0.7246    0.01014 ]\n",
      "query_embedding = [ 0.2377    0.2079    0.3872    0.0712   -0.7334    0.964    -0.07526\n",
      "  0.3093    0.5947   -0.9067   -0.1605   -0.1726    0.2856   -1.091\n",
      "  0.3662    0.2805   -0.1228    0.02213   0.258     0.3584    0.2925\n",
      "  0.128    -0.1042   -0.7314    0.6816   -0.08765   0.07605   0.708\n",
      "  0.1417   -0.2362   -0.3408    0.2556    0.1283   -0.2378   -0.015495\n",
      " -0.2563   -0.4512    0.325     0.7446   -0.4543   -0.4348   -0.1998\n",
      "  0.175    -0.3618   -0.2043   -0.271     0.3057    0.8564    0.3196\n",
      "  0.05908   0.8423   -0.6694    0.188     0.2742   -0.52     -0.9\n",
      "  0.2367    0.08154   0.4487   -0.513    -0.1122    1.236     0.1609\n",
      "  0.05685   1.025    -0.454     0.523     0.2299    0.2362   -0.3125\n",
      "  0.06915   0.1062   -0.2013   -0.2737   -0.3508    0.601     0.545\n",
      " -0.381    -0.02414  -0.5034    0.1669    0.0806   -0.2454   -0.1865\n",
      " -0.9424   -0.3726   -0.2554   -0.1289   -0.7017   -0.8867    0.0907\n",
      "  0.167     0.1323   -0.764    -0.4797   -0.03555  -0.979    -0.3376\n",
      " -0.272     0.6636  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 2.6733e-01 -3.1348e-01  5.1123e-01  1.1261e-01 -9.1064e-01  1.3782e-01\n",
      " -5.5566e-01 -1.1481e-01  8.6377e-01 -4.7852e-01 -1.7395e-01  5.9033e-01\n",
      "  3.8477e-01 -1.0713e+00  5.1221e-01 -6.4014e-01 -3.8794e-01  3.3112e-02\n",
      "  7.8369e-01 -5.8441e-02  2.8076e-01  7.1436e-01  1.8311e-02 -1.5173e-01\n",
      " -1.9507e-01  2.5342e-01  7.9004e-01  2.9834e-01  3.1494e-01  4.9341e-01\n",
      " -8.5059e-01 -2.8564e-01 -4.4159e-02 -3.4888e-01 -3.4351e-03  4.2920e-01\n",
      " -2.2793e-03  3.3350e-01  4.1089e-01 -3.8501e-01 -6.0889e-01  2.7954e-01\n",
      "  2.8564e-01 -1.0262e-03 -8.6365e-02 -4.4849e-01  9.4434e-01  3.7231e-01\n",
      "  5.5273e-01 -4.0381e-01  4.7607e-02 -6.2207e-01  3.4595e-01  1.9104e-01\n",
      " -3.8745e-01  1.2915e-01  6.3281e-01 -2.8711e-01 -7.2510e-01 -1.0712e-01\n",
      "  1.0175e-01  7.4902e-01 -4.7583e-01  7.5537e-01 -2.3840e-01 -5.6396e-01\n",
      "  3.8452e-01  4.5117e-01  2.2278e-01 -8.1299e-02  3.8037e-01 -1.0176e+00\n",
      " -4.6448e-02 -3.2764e-01 -6.0400e-01  7.2852e-01 -2.8247e-01 -1.2093e-02\n",
      " -2.9077e-01 -5.5371e-01 -8.6487e-02  6.7978e-03 -4.4214e-01  6.3867e-01\n",
      " -6.5967e-01 -1.7834e-01 -2.9648e-02  8.4656e-02 -5.2539e-01 -6.0840e-01\n",
      "  1.5112e-01 -1.3123e-01 -1.2805e-01 -2.5366e-01 -3.5083e-01  4.3640e-02\n",
      " -7.4768e-02  2.6642e-02 -5.5322e-01  3.4570e-01]\n",
      "query_embedding = [ 0.056    -0.3254   -0.5005   -0.3992   -0.688     0.9194   -0.3267\n",
      "  0.2328    0.2627   -0.3293   -0.17     -0.05515  -0.3591   -0.55\n",
      " -0.393    -0.012245 -1.238    -0.08624  -0.1409    0.3708    0.742\n",
      "  0.296     0.0493   -0.1359    0.571     0.2462   -0.4587    0.9985\n",
      "  0.632    -0.04904  -0.2844    0.317     0.03647  -0.08307  -0.0596\n",
      " -0.0568   -0.005924 -0.3252    0.48      0.342    -0.68     -0.1097\n",
      "  0.2368   -0.277     0.4995   -0.344     0.1581    0.4998    1.061\n",
      " -0.2449    0.5728   -0.3423    0.2133    0.451     0.1797   -1.127\n",
      "  0.04764   0.08795  -0.03656  -0.06525   0.2407    0.0663   -0.1458\n",
      " -0.01458   0.4585   -0.2317    0.2008    0.605    -0.003723 -0.1543\n",
      "  0.0941    0.0471   -0.5415   -0.1989   -0.732     0.6675   -0.02498\n",
      " -0.1787   -0.879    -0.1925    0.833    -0.1271   -0.3372    0.2769\n",
      " -0.59     -0.499    -0.5415    0.3572   -0.3455   -0.79      0.2365\n",
      "  0.2847   -0.786     0.3242    0.1748   -0.2593   -0.2688    0.4082\n",
      "  0.088     0.1074  ]\n",
      "query_embedding = [ 0.03488   0.4607    0.0684   -0.02898   0.165    -0.386    -0.511\n",
      " -0.11774   0.1462   -0.8013   -0.531     0.4165    0.0884   -0.3315\n",
      " -0.2747   -0.631     0.07794   0.3079   -0.643     0.429    -0.1758\n",
      "  0.0793   -0.3752   -0.4148   -0.05374   0.5254    0.02846  -0.4158\n",
      "  0.5654   -0.535    -0.4446    0.3228   -0.02986   0.3372   -0.01744\n",
      " -0.2185    0.063     0.1567    0.1141   -0.2786   -0.8486   -0.3013\n",
      "  0.3083   -0.6636   -0.1371   -0.446     0.262    -0.05334   0.02173\n",
      " -0.9517   -0.11115  -0.329     0.0724    0.9883    0.4082   -2.635\n",
      " -0.01332  -0.2432    0.8613   -0.0733   -0.4346    0.696    -0.4995\n",
      " -0.2063    0.1611    0.5024   -0.01152   0.897    -0.4536   -0.871\n",
      "  0.805    -0.12354  -0.544    -0.9136    0.4692    0.6885    0.199\n",
      " -0.1765   -0.9766   -0.188     0.318     0.403    -0.357     0.1891\n",
      " -1.253     0.2217   -0.0659    0.2957   -0.1396    0.008125 -0.0811\n",
      " -0.06586  -0.05566   0.1996   -0.857    -0.0279    0.1218   -0.279\n",
      "  0.1727    0.5107  ]\n",
      "query_embedding = [-0.2137   -0.3794    0.8965   -0.2147   -0.8403    0.4558   -0.331\n",
      "  0.1632    0.5317   -0.1818    0.4185    0.10724  -0.5566    0.0707\n",
      "  0.00971   1.014    -0.4006   -0.341     0.3975    0.5527    0.322\n",
      " -0.02164  -0.2854   -0.05722  -0.0437   -0.4756   -0.3586    0.881\n",
      "  0.449    -0.4036    0.2869   -0.1599   -0.11096  -0.4668   -0.654\n",
      " -0.08374  -0.6675   -0.3545   -0.626     0.0792   -0.11975   0.35\n",
      "  0.115    -0.563    -0.5254    0.521     1.273     0.702    -0.1138\n",
      " -0.6016    0.6797   -0.34      0.1493    0.3723   -0.2253   -1.529\n",
      "  0.8286    0.0807    0.5703   -0.006477 -1.197     0.2122   -0.01921\n",
      " -0.2769    0.6025   -0.8896    1.048     0.1436   -0.1698   -0.2695\n",
      " -0.1716    0.1909   -0.4138    0.1492   -0.03705   0.5996   -0.476\n",
      "  0.02107  -0.62      0.784     0.2917    0.2445    0.5103    0.241\n",
      " -1.399    -0.4463   -0.02902   0.985    -0.459    -0.806     0.01452\n",
      "  0.631    -0.08014  -0.03522   0.2952    0.01071   0.0398    0.178\n",
      " -0.0867   -0.1555  ]\n",
      "query_embedding = [ 0.4521    0.1543   -0.06714   0.0654    0.8564   -0.5347   -0.1417\n",
      " -0.1434   -0.5034    0.0998    0.04425  -0.7437    0.4836   -1.065\n",
      " -0.2954    0.913    -0.2465    0.2025    0.9326    0.505    -0.2788\n",
      "  0.02559   0.6772   -0.2686    0.3362    0.1962    0.551     0.7026\n",
      " -0.413    -0.1909   -0.402     0.8525   -1.166    -0.09735  -0.1454\n",
      " -0.1887   -0.3267   -0.0387   -0.04794  -1.201     0.377     0.5244\n",
      " -1.3545    0.7573   -0.2443    0.1285   -0.4849   -0.246     0.679\n",
      "  0.1545   -0.8755   -0.827    -0.0759    0.631     0.3245   -1.338\n",
      " -0.5664    0.1683    0.667    -0.1296    0.1272    1.543    -0.2842\n",
      "  0.1962    0.586    -0.1935    0.5522    0.3398   -0.4072    0.758\n",
      "  0.2708    0.771    -0.1251    0.2646    0.008545  0.4932   -0.2974\n",
      "  0.709    -0.3408    0.2329   -0.767    -0.4119   -0.04153   0.2327\n",
      " -1.697     1.04     -0.1925   -0.1754    0.6484   -0.3342    0.4563\n",
      " -0.3337    0.2451   -0.04385   0.5      -0.7715   -0.2307   -0.498\n",
      "  0.763     0.0307  ]\n",
      "query_embedding = [-0.551    0.2935   0.4      0.2786  -1.348    0.5044  -0.247    0.6416\n",
      "  0.5254  -0.5356   0.2725   0.3296   0.4822  -1.2705  -0.4424  -0.212\n",
      " -0.8726   0.6147  -0.3271   0.2893  -0.04367  0.529   -0.09204 -0.7646\n",
      "  0.563    0.1267   0.2487   0.292    0.5234  -0.291   -0.438    0.2742\n",
      " -0.1283   0.6245   0.03662 -0.03717 -0.531   -0.02058  0.4287  -0.6885\n",
      " -0.2664   0.2957  -0.2112  -0.357   -0.2119   0.0638   0.6143   0.264\n",
      "  0.314   -0.736   -0.02965 -0.6064   0.4587   0.2103  -0.5137  -0.453\n",
      "  0.12274  0.3564  -0.2073  -0.11475  0.2676   0.624   -0.418    0.0666\n",
      " -0.2573  -0.2976   0.5522   0.7188   0.3948   0.1929   0.662   -0.01741\n",
      "  0.1098  -0.128    0.02715  0.295    0.4995  -0.10583  0.03833 -0.538\n",
      "  0.1631   0.00762 -0.536   -0.0503  -0.924   -0.6323   0.05133 -0.2996\n",
      " -1.569   -0.4487   0.1558  -0.2473   0.0775  -0.1737  -0.2644  -0.3037\n",
      " -0.6387  -0.4126  -0.587    0.701  ]\n",
      "query_embedding = [ 0.0373    0.3972   -0.09247  -0.006264 -0.645     0.02327  -0.2141\n",
      " -0.09955  -0.2208    0.1218   -0.323     0.1179   -0.495    -0.3328\n",
      "  0.5186   -0.06287  -0.463     0.04565   0.4844   -1.023     0.2306\n",
      " -0.6       0.1713    0.12164   0.0695    0.413    -0.226    -0.2617\n",
      " -0.11115  -0.791     0.1592   -0.541    -1.453     0.3213    0.02643\n",
      " -0.2332    0.3013    1.231    -0.1393   -0.4783   -0.01817  -0.3853\n",
      " -0.45     -0.3064    0.6626    0.2988    0.03415  -0.1355   -0.1794\n",
      " -0.06696   0.271     0.8184   -0.1826    0.912     0.581    -0.5815\n",
      "  1.037    -0.85      0.007275 -0.4702    0.1198   -0.3452    0.266\n",
      "  1.586    -0.11127   0.3364   -0.189    -0.851     0.422    -0.4255\n",
      "  0.3413   -0.4917    0.3557    0.569     0.3818   -0.12396  -0.2068\n",
      " -0.3699    0.04617  -0.2097   -0.3464   -0.03433  -0.238     0.1306\n",
      "  0.1516    0.1482    0.6196    0.3071   -0.3582   -0.1658    0.1664\n",
      " -0.2277    0.549    -0.614     1.491     0.3445    0.05353  -0.324\n",
      " -0.1804    0.748   ]\n",
      "query_embedding = [-0.2625   -0.1592    0.666    -0.1243   -0.638    -0.341    -0.1027\n",
      "  0.02988  -0.3943   -0.02727   0.2345    0.1566    0.2683   -0.867\n",
      " -0.527    -0.3943   -0.4246    0.3381    0.1796    0.0926    0.1698\n",
      "  0.3313    0.01019  -0.4412   -0.6045    0.232     0.2646   -0.1864\n",
      " -0.09033   0.1771   -0.6733    0.3186   -0.06055  -0.3672    0.8564\n",
      " -0.12067  -0.2301    0.08167   0.03778  -0.2097   -0.54     -0.3909\n",
      "  1.09     -0.3164   -0.1039   -0.2216    0.688    -0.0953    0.237\n",
      " -1.338     0.1383   -0.5327    0.397     0.5806    0.1165   -1.822\n",
      "  0.1077    0.1398    0.6943   -0.088     0.0658   -0.05106  -1.112\n",
      " -0.06146  -0.004208  0.2394    0.2369    1.018    -0.531    -0.403\n",
      "  0.9277   -0.02171  -0.385    -0.9224    0.664    -0.01207  -0.4507\n",
      " -0.1292   -0.2064   -0.7935    0.5405   -0.02573  -0.541     0.2323\n",
      " -0.9316   -0.705    -0.05936   0.6406   -1.078    -0.5503    0.1759\n",
      " -0.1732   -0.0436   -0.004112 -0.2666    0.6113   -0.5674   -0.1771\n",
      " -0.1516    0.4058  ]\n",
      "query_embedding = [ 0.43     -0.3142    1.065    -0.2812   -1.058     0.3716   -0.6616\n",
      "  0.0546    0.5894   -0.9067   -0.424    -0.2231    0.1769   -0.3828\n",
      "  0.4548    0.4119   -1.22     -0.2466    1.131    -0.2585    0.2233\n",
      "  0.1787   -0.07477  -0.4946   -0.07776   0.2001    0.3118    0.0418\n",
      "  0.8384    0.707    -0.05215   0.432     0.05518  -0.3079    0.402\n",
      " -0.348    -0.1509    0.303     0.529     0.223    -0.1516   -0.5195\n",
      " -0.01372  -0.5117   -0.01627  -0.1013    0.8594    0.537     0.06903\n",
      " -1.054    -0.2253   -0.593     0.35     -0.0794    0.3164   -1.171\n",
      "  0.11456  -0.315     0.4148    0.758     0.2129    1.423    -0.4595\n",
      "  0.3342    0.02562   0.1415    0.4092    0.235     0.694    -0.347\n",
      " -0.1737   -0.3997   -0.1558   -0.5093   -0.0555    0.363     0.006466\n",
      "  0.1266   -0.1769   -0.5903    0.8193   -0.00743  -1.048     0.668\n",
      " -1.887    -0.10077  -0.3157    0.4463   -1.354    -0.519     0.292\n",
      " -0.02063   0.27      0.256    -0.275     0.67      0.6016   -0.3442\n",
      " -0.207     0.2937  ]\n",
      "query_embedding = [-0.1261   0.2817   0.9907   0.1128  -0.0631   1.166   -0.5063  -0.607\n",
      " -0.2179  -0.1423  -0.728    0.4846  -0.5503  -0.1062   0.407    0.392\n",
      " -0.06186 -0.1921   0.6953   0.424    0.476   -0.0742  -0.2053  -0.3057\n",
      "  0.359    0.531   -0.04694  0.4136   0.1353  -0.4312  -0.5513  -0.4563\n",
      " -0.4932  -0.346   -1.146    0.1864   0.1559   0.4197   0.3726  -0.633\n",
      "  0.6265  -0.32     0.02792  0.2186  -0.2467   0.3242   0.743    0.2327\n",
      "  0.501   -0.1531   0.1467  -0.3591   0.4192   0.3516  -0.07635 -0.7563\n",
      "  0.7163   0.2037  -0.2401   0.2742   0.9834   0.8564  -0.1886  -0.2761\n",
      "  0.041   -0.1483   0.795   -0.3918  -0.2467  -0.9224   0.08215 -0.0654\n",
      "  0.0795   0.2888   0.1362   0.5615   0.1527  -0.09357  0.372   -0.7354\n",
      " -0.7246   0.02582  0.298    0.693   -0.9414  -0.02812  0.04474 -0.1743\n",
      " -0.581    0.0592  -0.2328   0.08746 -0.1858   0.5664  -0.04886  0.384\n",
      " -0.7383  -0.7856  -0.9595   0.329  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.289   -0.4426   0.119   -1.423   -0.2834   0.66    -0.67    -0.6147\n",
      " -0.1088   0.2515  -0.5264  -0.2893   0.06134 -0.6     -0.1051  -0.537\n",
      " -0.3015   0.2666  -0.785    1.094    0.02994 -0.09283 -0.5654   0.18\n",
      "  0.669    1.3     -0.8413   0.1921   0.534   -0.987    0.5244  -0.3936\n",
      " -0.1246  -0.0979  -0.3806   0.02538  0.605   -0.4607  -0.2136   0.0981\n",
      " -0.5156   0.3103   0.2238   0.08075  0.776   -0.817    0.5654   0.4468\n",
      "  0.3965   0.02415 -0.2274   0.08325  0.3948   0.6353  -0.474   -0.4595\n",
      "  0.7983  -0.472    0.1431  -0.2654   0.9487   1.072   -0.2566  -0.2233\n",
      "  0.2617  -0.1785  -0.627   -0.536    0.0938  -0.04385  0.2637   0.03192\n",
      " -0.273   -0.404   -0.7725   0.4077  -0.03314  0.11505 -1.071    0.61\n",
      "  0.519    0.0956  -0.5503   0.415   -0.3286   0.02388 -0.1367   0.07227\n",
      "  0.3274  -0.6475  -0.4597   0.5317  -0.05533  0.6655   0.3884  -0.1658\n",
      "  0.05295  0.4563   0.0983  -0.08496]\n",
      "query_embedding = [ 0.462    0.3176   0.4058  -0.1084  -0.668    1.189   -0.1711   0.49\n",
      " -0.00401 -0.273    0.1588  -0.2766   0.1155  -1.144   -0.00368 -0.2595\n",
      " -0.4258   0.2725   0.489    0.2942   0.4548   0.56    -0.2477  -0.2742\n",
      "  0.4734   0.559    0.3171   0.877    0.822   -0.2913  -0.583   -0.2373\n",
      " -0.1349   0.635   -0.152   -0.3455   0.075    0.7114  -0.0649  -0.372\n",
      " -0.1004   0.2036   0.06006 -0.2612  -0.1068  -0.444    0.1267   0.2167\n",
      "  0.416   -0.2211   0.4     -0.5137   0.7046  -0.1538  -0.256   -0.1823\n",
      "  0.4836   0.2272  -0.0656  -0.038    0.5405   0.1663  -0.4966  -0.08527\n",
      " -0.3398  -0.3687   0.55     0.3184  -0.1959  -0.647    0.1901  -0.10315\n",
      "  0.10944 -0.7886  -0.411    0.7583   0.1603  -0.508   -0.1578  -0.7954\n",
      "  0.04654  0.1233  -0.07574  0.4548  -0.3586  -0.3645   0.0821  -0.186\n",
      " -0.921   -0.6343   0.2     -0.3252  -0.3105   0.4448  -0.08997 -0.4216\n",
      " -0.606    0.3228  -0.2351   0.9277 ]\n",
      "query_embedding = [-0.4812    0.983     0.4949   -0.2217    0.1339    1.057    -0.8477\n",
      " -0.683     0.2952    0.2292   -0.85     -0.3533   -0.2341   -0.4673\n",
      " -0.0954   -0.05124  -0.02985   0.1492   -0.724     0.4011   -0.1631\n",
      "  0.376    -0.71     -0.251     0.3228    0.5103    0.2363    0.5933\n",
      "  0.4353   -1.068     0.8374   -0.0769   -0.7573    0.534    -0.377\n",
      "  1.143     0.1396   -0.09515  -0.9653    0.2778   -0.6655    0.08575\n",
      " -0.2915    0.203     0.1143    0.00543   0.0565    0.513    -0.012924\n",
      " -0.367     0.1808   -0.1053    0.0894    0.801    -0.4932    0.06415\n",
      "  0.2942   -0.3523   -0.387    -0.6064    0.6006    0.08966   0.04648\n",
      "  0.4763    0.1074   -0.3408    0.588    -0.24     -0.242    -0.1357\n",
      " -0.499     0.1109    0.1542   -0.2234   -0.5215    0.611    -0.1293\n",
      " -0.605    -0.873     0.00306   0.1971    0.3564    0.03867  -0.08777\n",
      " -0.1271   -0.1876   -0.4807    0.484    -0.296    -0.728     0.02917\n",
      "  0.01888  -0.508     0.6763    0.605    -0.08636  -0.2966    0.4097\n",
      "  0.427     0.7407  ]\n",
      "query_embedding = [ 0.6987   -0.2341    1.359    -0.534    -0.8813   -0.1376    0.072\n",
      " -0.02542   0.2595   -0.9883   -0.4353    0.2366    0.1193    0.01544\n",
      "  0.3542    1.076     0.1334   -0.0357   -0.5107    0.3706    0.606\n",
      "  0.752     0.003366 -1.027     0.5967    0.85      0.497     0.2341\n",
      "  0.178     1.058     0.06192   0.7397    0.3591   -0.2245   -0.2444\n",
      "  0.06052  -0.5786   -0.2382   -0.1406   -0.1554   -0.468     0.5283\n",
      " -0.757    -0.876     0.3684    0.2754   -0.0851    0.01588   0.02878\n",
      " -0.6104    0.195     0.02744   0.0197    0.4255   -0.697    -1.495\n",
      " -0.2646    1.203     0.1525   -0.36      1.11      1.057    -0.1755\n",
      " -0.1311    0.906     0.6196   -0.2015   -0.1691    0.4749    0.1245\n",
      "  0.05536  -0.11383  -0.875     0.6606   -0.307     0.1621    0.312\n",
      " -0.075    -0.7275    0.1058    0.4814   -0.177    -0.2996    0.3481\n",
      " -1.267    -0.1227   -0.1869    0.2637   -0.7197    0.1746   -0.3645\n",
      "  0.04395  -0.1674    0.1771   -0.09064  -0.6943   -0.1594   -0.494\n",
      " -0.07904  -0.606   ]\n",
      "query_embedding = [ 4.0967e-01  5.8929e-02  8.1982e-01 -1.4673e-01 -4.5898e-01  3.2129e-01\n",
      "  3.6297e-03 -1.5466e-01 -1.4392e-01 -4.9731e-01 -2.9858e-01  5.2783e-01\n",
      " -1.5662e-01 -5.9473e-01  9.0479e-01  3.7872e-02 -5.2002e-01  2.3132e-01\n",
      "  5.1660e-01 -8.4180e-01  1.5967e-01  9.2173e-04 -3.4790e-01 -1.5771e-01\n",
      " -3.9600e-01  4.9731e-01  5.5566e-01  4.7266e-01  3.5864e-01 -9.2676e-01\n",
      " -4.8370e-02  2.8931e-02 -8.5498e-01  5.6396e-01 -1.8921e-01 -2.5415e-01\n",
      "  4.6899e-01  2.6016e-02 -3.4888e-01 -5.9912e-01 -2.9678e-02 -3.7915e-01\n",
      " -2.3636e-02 -3.2935e-01  3.5400e-01 -4.1309e-01  1.0938e+00  1.2427e-01\n",
      "  4.7217e-01 -8.2422e-01 -4.5923e-01 -1.2732e-01  4.0771e-01  1.1182e-01\n",
      "  5.4346e-01 -1.0469e+00  9.5337e-02  5.9521e-01 -7.7820e-02 -1.6003e-01\n",
      "  1.7114e-01  5.8301e-01  2.1228e-01  1.2335e-01 -1.2006e-01  5.4346e-01\n",
      "  3.3276e-01 -1.2432e+00  3.0322e-01 -6.6553e-01  8.3740e-02  5.5511e-02\n",
      " -7.0361e-01 -2.1411e-01 -3.3447e-01  1.0703e+00  5.3864e-02  6.4697e-03\n",
      " -7.1729e-01 -2.8516e-01  7.2571e-02 -3.3691e-01 -1.5186e-01  6.1426e-01\n",
      " -1.6785e-01 -6.9092e-02 -2.4170e-01 -2.8418e-01 -4.8682e-01 -6.3086e-01\n",
      "  1.1911e-03  1.1884e-01 -8.3838e-01  7.1094e-01  1.2244e-01 -1.7725e-01\n",
      "  7.1777e-01  1.5173e-01 -3.6255e-01  4.3652e-01]\n",
      "query_embedding = [ 0.142   -0.2379   0.796   -0.04666 -0.6436  -0.1108  -0.653   -0.1318\n",
      "  0.3325  -0.218   -0.3286   0.2852  -0.3523  -0.738    0.1947  -0.25\n",
      " -0.07666  0.1475  -0.03009  0.1638  -0.5317   0.3826  -0.4856  -0.1381\n",
      "  0.3242  -0.1807  -0.3733  -0.03293  1.128   -0.6997   0.2402  -0.278\n",
      " -0.2795  -0.01973 -0.6294   0.4102   0.5146   0.1132   0.526    0.04977\n",
      "  0.0993   0.0791  -0.2311  -0.2491  -0.04907 -0.00551  0.3767   0.25\n",
      "  0.3823   0.2133  -0.02243 -0.2375   0.1648  -0.1407  -0.1837  -0.06213\n",
      " -0.04468 -0.1711  -0.6826  -0.6064   0.2754   0.0939   0.556    0.5024\n",
      " -0.4043  -0.3264   0.3464  -0.4734   0.3328  -0.1976  -0.194    0.3242\n",
      " -0.1243  -0.2983  -0.02782  0.528    0.317   -0.3906  -0.5405   0.4001\n",
      " -0.3228   0.0647   0.395   -0.2063  -0.5977  -0.2292  -0.2384   0.1583\n",
      "  0.06433 -0.3887  -0.03928  0.2051  -0.2332   0.615   -0.645   -0.1526\n",
      " -0.55    -0.07733  0.2725   0.4048 ]\n",
      "query_embedding = [-0.0353    0.3635    0.1048    0.2194   -0.635     0.4978   -0.652\n",
      " -0.7764    0.3713   -0.408    -0.623    -0.0935    0.10767  -0.5947\n",
      "  0.5615   -0.006134  0.1879    0.1201    0.06647   0.7354   -0.03833\n",
      "  0.6035   -0.267    -0.336    -0.1526    0.02847   0.532    -0.1495\n",
      "  0.7446   -0.6147    0.4648    0.824     0.0743   -0.1924   -0.1598\n",
      "  0.709     0.641     0.09924   0.3792    0.0913   -0.1785    0.0213\n",
      "  0.3438   -0.3215   -0.439    -0.1836    0.3303    0.023     0.2754\n",
      " -0.818     0.4883    0.2184    0.09454   0.4658    0.1692   -0.416\n",
      "  1.009    -0.3633   -0.346    -0.6704    0.1082    0.1461    0.4937\n",
      "  0.206     0.3394    0.007286  0.4526   -0.1799   -0.10077  -0.0703\n",
      " -0.4114   -0.2834    0.02641  -0.2115   -1.109     0.5903   -0.1556\n",
      " -0.4517   -0.257     0.0486   -0.08527   0.007576 -0.3162    0.3132\n",
      " -0.723    -0.09174  -0.6143    0.5454   -0.615    -0.6084   -0.0849\n",
      " -0.1029   -0.3386    0.075    -0.01776  -0.2783   -0.8276   -0.255\n",
      "  0.1335    0.4631  ]\n",
      "query_embedding = [ 0.3958    0.4727    0.1644   -0.887    -0.2026    0.6694   -0.7173\n",
      " -0.3845   -0.06146  -0.07495  -0.966     0.02066   0.1122   -0.713\n",
      " -0.919    -0.2075    0.5737    0.7817   -0.4146    0.3943    0.3428\n",
      " -0.4436   -0.6416   -0.2942    0.1401    0.4177   -0.1768    0.1333\n",
      "  0.9927   -0.6016   -0.5884    0.3596    0.03128   0.156    -0.05933\n",
      " -0.525     0.1494    1.003    -0.5264    0.03586   0.2057   -0.749\n",
      "  0.2133   -0.2416   -0.006485 -0.03973   0.6587    0.4263    0.10016\n",
      " -0.615    -0.2644   -0.00934  -0.278     0.7466   -0.05814  -1.694\n",
      "  0.2822    0.0164    0.4539   -0.1825    0.4172    0.7017   -0.585\n",
      "  0.02936  -0.1063    0.325     0.4634   -0.4155    0.1786    0.2008\n",
      "  0.2275   -0.03287  -0.2493    0.1098    0.2615    1.065     0.1148\n",
      " -0.09845  -1.105     0.1442   -0.6353   -0.273     0.608     0.9067\n",
      " -0.2642   -0.342    -0.958     0.0819   -0.01773   0.00866  -0.4993\n",
      " -0.776     0.12024   0.5576   -0.5923    0.023    -0.04758  -0.3003\n",
      "  0.4275    0.91    ]\n",
      "query_embedding = [ 0.0298    0.1576    0.3062   -0.2925   -1.315     0.7144   -0.2451\n",
      "  0.4583    0.6074   -0.269     0.1086    0.482     0.6235   -1.28\n",
      " -0.02727  -0.0657   -0.6816    0.678     0.2472    0.09033   0.191\n",
      "  0.599     0.02216  -0.2203    0.702    -0.3801    0.722     0.513\n",
      "  0.7466   -0.557     0.1237   -0.2605    0.02812   0.256    -0.366\n",
      "  0.3389   -0.4329   -0.1901    0.5693   -0.4534   -0.2408    0.3694\n",
      " -0.09467   0.08154  -0.3816   -0.0674    0.5405    0.9243   -0.4446\n",
      " -0.1311   -0.2313   -0.2803    0.0815    0.074    -0.733     0.2522\n",
      "  0.08405   0.1792   -0.801     0.0867    0.09076   0.2294   -0.32\n",
      "  0.2351   -0.3513   -0.5747    0.4397    0.666    -0.01423   0.1979\n",
      "  0.4092    0.0965    0.4775   -0.3086   -0.3696    0.2622    0.303\n",
      "  0.2524   -0.2559   -0.786    -0.01088   0.1914   -0.0911    0.007675\n",
      " -0.2444   -0.7695    0.3884   -0.01665  -1.155    -0.5737    0.0945\n",
      " -0.1453   -0.0899    0.1547    0.3455    0.10925   0.2305   -0.1787\n",
      " -0.8164    0.4756  ]\n",
      "query_embedding = [ 0.348    0.517    0.6885   0.4668   0.2328   0.2494  -0.3057  -0.668\n",
      "  0.3386  -0.4138  -0.265    0.8174   0.1647  -0.02042 -0.3025  -0.357\n",
      " -0.0358   0.6055  -0.657    0.526   -0.0967  -0.1857  -0.6846  -0.317\n",
      "  0.352    0.8696   0.362   -0.3489   0.2732  -0.794   -0.4177   0.26\n",
      " -0.2231   0.3313   0.2393   0.1909   0.4387   0.949   -0.3413  -0.7993\n",
      " -0.1486  -0.0835  -0.1919  -0.277   -0.2213  -0.361    0.1327   0.2052\n",
      " -0.1995  -0.5176  -0.0343  -0.4949   0.1761   0.9106   0.2544  -1.191\n",
      "  0.724    0.481    0.2334  -0.4001   0.0889   0.4497  -0.572   -0.913\n",
      "  0.2241   0.619    0.747   -0.9795   0.63    -0.5454   0.1431   0.1841\n",
      "  0.2583  -0.368    0.5176   1.3955   0.1273  -0.9478  -0.7397  -0.04538\n",
      " -0.696    1.169    0.42     0.1014  -0.7905   0.1605   0.2366  -0.6465\n",
      " -0.346   -0.2021  -0.22    -0.1176   0.4683   0.286   -0.1705  -0.4016\n",
      "  0.0684  -0.4302  -0.10144  0.2311 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.3113    0.1504    0.2937    0.4075   -0.7124    0.608    -0.182\n",
      " -0.6353   -0.0778    0.623    -0.10565  -0.2686    0.187    -0.62\n",
      "  0.3992   -0.074    -0.717    -0.4265   -0.1714    0.3433    0.914\n",
      " -0.1191    0.2104   -0.07336  -0.2043    0.05853  -0.1527    0.911\n",
      "  0.05463  -0.02426   0.4563    0.739     0.05313  -0.9653    0.93\n",
      "  0.005802  0.3657   -0.962    -0.1981    0.3313   -1.064     0.139\n",
      " -0.3894    0.2102    0.3354    0.09283  -0.2686    1.085    -0.1727\n",
      " -0.631     0.4956    0.4746    0.8877    0.368     0.1406   -1.319\n",
      "  0.8657   -0.58      1.282    -0.547    -0.01816  -0.0355   -0.222\n",
      "  0.004448  0.01362  -0.2686    0.554     0.3508   -0.084    -0.1891\n",
      " -0.2147   -1.175    -0.4854   -0.313    -0.315    -0.5806    0.011475\n",
      " -0.2827   -1.082     0.6084    0.5615    0.04843  -0.0676    1.157\n",
      " -0.9185   -0.3462    0.04147   0.4707    0.353    -0.871    -0.02408\n",
      " -0.1691   -0.2086    0.2263    0.00947   0.9087   -1.135     0.4856\n",
      "  0.3044    0.292   ]\n",
      "query_embedding = [-0.04214  -0.1488    0.635     0.886    -0.6357   -0.2676   -0.2808\n",
      " -0.1622    0.01478   0.1167   -0.6104    0.1222    0.3452   -0.2194\n",
      " -0.1473   -0.0753   -0.3936    0.3342   -0.1309    0.5386   -0.4739\n",
      "  0.2325   -0.9395   -0.242    -0.1617   -0.10724   0.51      0.3604\n",
      "  0.2783   -0.7603    0.1062    0.2452   -0.2435    0.3635    0.4158\n",
      "  0.399     0.5776    0.77     -0.5747   -0.3762    0.1935   -0.1251\n",
      "  0.3784   -0.196    -0.3242   -0.1401    0.03647  -0.213     0.0043\n",
      " -0.9727    0.3345   -0.624    -0.1831    1.072     0.2014   -0.88\n",
      "  0.5303   -0.462     0.6406   -0.0244    0.0725   -0.489    -0.5977\n",
      " -0.5957   -0.2842    0.418     0.3574   -0.2267    0.5366    0.1779\n",
      " -0.00655   0.4548    0.2898   -0.8486    0.5605    1.454    -0.2083\n",
      " -0.7666   -0.649    -0.508     0.03084   0.7856    0.2146   -0.1039\n",
      " -0.997    -0.3625    0.04663  -0.3965   -0.857    -0.565    -0.42\n",
      " -0.1561   -0.6475    0.4343   -0.012825  0.623     0.0692   -0.5806\n",
      "  0.127     0.4565  ]\n",
      "query_embedding = [ 0.2219   0.3254   0.5146   0.336   -0.441    0.577   -0.8623  -0.3276\n",
      " -0.4934  -0.0634  -0.771   -0.272    0.998   -0.4116  -0.513   -0.504\n",
      "  0.11914  0.308   -1.134    0.6743  -0.1458  -0.3274  -0.525   -0.785\n",
      "  0.4824   0.11273  0.1484  -0.5425   0.906   -0.594    0.2869   0.646\n",
      "  0.11584  0.441    0.408    0.1678   0.5864   0.513   -0.7505  -0.2969\n",
      " -0.1371  -0.1573   0.9136  -0.0813  -0.6694  -0.3406  -0.10046  0.11786\n",
      " -0.4946  -1.139    0.574    0.08203 -0.2708   1.06     0.3005  -1.173\n",
      "  0.7886  -0.4128   1.378   -0.3398  -0.06555 -0.1735  -0.927   -0.5156\n",
      "  0.7354   0.368    0.03455  0.4714   0.3264  -0.0998   0.3523   0.0339\n",
      "  0.5874  -0.5264   0.796    0.3914  -0.4783  -0.4595  -0.7256  -0.0434\n",
      "  0.5444  -0.00317 -0.03586  0.2343  -1.117    0.71    -0.4675  -0.1216\n",
      " -0.1857  -0.4832  -0.515   -0.5967  -0.3909   0.4897  -0.2151   0.2051\n",
      " -0.4524  -0.4724  -0.08746  1.063  ]\n",
      "query_embedding = [-0.5703   -0.787     0.1293   -0.3064   -0.568     0.6074   -0.516\n",
      " -0.2617    0.04922   0.3337   -0.1354    0.1592   -0.01067  -0.05984\n",
      "  0.2043    0.01657  -0.083    -0.1406   -0.2925   -0.5435   -0.136\n",
      " -0.2773    0.09753  -0.4075    0.09894   0.594     0.2969    0.802\n",
      " -0.662    -0.1367    0.3018   -0.4656    0.1492    1.258     0.2477\n",
      "  0.002031 -0.2917   -0.6714    0.653    -0.1948   -0.1672    0.2037\n",
      "  0.06824  -0.5107    1.335     0.3206    0.3584    0.475     0.3108\n",
      " -0.967     0.7817   -0.06064  -0.357     0.8726   -0.345    -1.404\n",
      "  0.4895   -0.1356    1.88      0.332     0.1826    0.671    -0.02658\n",
      " -0.456     0.2686    0.4944   -1.281    -0.0666    1.082     0.3564\n",
      "  0.337    -1.062    -0.374    -0.4866    0.6255    1.203     0.7603\n",
      "  0.2556   -0.2947   -0.702     0.909    -0.9727   -1.094     0.1938\n",
      " -0.6675    0.08746   0.3657    0.584    -0.3071    0.1287   -0.2805\n",
      "  0.597    -0.1995   -0.2783   -0.2964    1.155     0.01823  -0.3186\n",
      " -0.7886   -0.3398  ]\n",
      "query_embedding = [ 0.8      -0.0982    1.108     0.501    -0.8037    0.985    -0.5317\n",
      " -0.337    -0.2798    0.1724   -0.8203    0.443     0.624    -0.523\n",
      "  0.422     0.573    -0.2471    0.4504   -0.3953   -0.007763 -0.05008\n",
      "  0.4314   -0.02367  -0.411     0.371     0.396     0.505    -0.014015\n",
      "  0.3628   -1.066     0.2499    0.311    -0.4622    0.3425   -0.5527\n",
      "  0.908     0.698    -0.02644  -0.1458    0.388    -0.4714    0.1025\n",
      " -0.4202    0.4294   -0.1404   -0.3813    0.6426    0.4211   -0.6187\n",
      " -0.2212    0.4358   -0.2964   -0.1029    1.154    -0.01701  -0.1183\n",
      "  0.548    -0.1267    0.0662   -0.672     0.1305   -0.5977   -0.1676\n",
      " -0.258     0.1141   -0.3235    0.3489   -0.84      0.2417    0.10895\n",
      " -0.874    -0.3916   -0.2365   -0.259     0.432     0.4243    0.3933\n",
      "  0.5205    0.06415   0.05637   0.1892    1.105    -0.01264   0.834\n",
      " -0.8867   -0.568    -0.5303   -0.04468  -0.2878   -0.524    -0.2339\n",
      " -0.142    -0.9546    0.6655    0.387    -0.293    -0.1503   -0.7207\n",
      " -0.0193    0.0834  ]\n",
      "query_embedding = [ 4.5166e-01  4.7412e-01  3.2153e-01 -1.2360e-01 -1.4136e-01  1.4763e-02\n",
      " -2.0679e-01 -6.5918e-01 -2.6782e-01 -6.2347e-02 -1.0127e+00  1.7212e-01\n",
      " -1.9666e-01 -1.2656e+00 -3.0298e-01 -3.4326e-01 -6.1084e-01  8.6426e-01\n",
      "  2.3718e-01 -4.7699e-02 -2.9736e-01 -3.0350e-02 -6.0791e-01 -8.5596e-01\n",
      " -1.1304e-01  3.6011e-01  3.0713e-01  1.9824e-01  1.0553e-01 -5.5518e-01\n",
      " -2.5610e-01  4.9683e-01 -3.2153e-01  2.0105e-01 -3.1177e-01 -4.5044e-02\n",
      "  2.5806e-01  1.2598e-01 -2.4048e-01 -1.0834e-01  1.8494e-02 -6.6162e-01\n",
      "  2.7985e-02 -1.1473e-03  6.8848e-02 -6.8750e-01  5.5713e-01 -4.0649e-01\n",
      "  8.1934e-01 -1.3936e+00 -1.3489e-01 -1.5900e-02  8.9417e-02  4.7046e-01\n",
      "  3.4644e-01 -1.4365e+00  2.1423e-01 -1.0791e-01  1.1780e-01 -4.8169e-01\n",
      "  4.3188e-01  6.0059e-01 -7.3486e-02  2.1582e-01  4.6631e-02  3.0347e-01\n",
      "  6.1865e-01 -6.9727e-01  8.1787e-01 -2.1802e-01  7.2852e-01 -5.3955e-02\n",
      " -2.6294e-01 -7.6318e-01 -1.1322e-01  1.0547e+00  7.6904e-02 -2.1973e-01\n",
      " -7.8027e-01 -2.1948e-01  2.2046e-01  3.0200e-01  1.9791e-02 -3.6426e-01\n",
      " -7.6904e-01 -6.0352e-01 -2.6392e-01  3.5352e-01 -4.5264e-01 -4.8926e-01\n",
      " -1.7236e-01 -1.4282e-01 -2.6831e-01  6.5576e-01 -5.3467e-01 -2.3950e-01\n",
      " -3.1470e-01 -4.1553e-01  3.1311e-02  8.1299e-01]\n",
      "query_embedding = [ 0.04892   0.04016   0.9053   -0.4426   -0.3713    0.637    -0.1176\n",
      " -0.4546    0.2478    0.6704   -0.6846    0.1754    0.04028  -0.582\n",
      "  0.4414   -0.2188   -0.3843    0.04852   0.02913  -0.1823   -0.373\n",
      " -0.0582   -0.0946   -0.396     0.6875    0.1624    0.06726  -0.202\n",
      " -0.2047   -0.7505   -0.1305   -0.4001   -0.2424   -0.2766   -0.2773\n",
      "  0.2125   -0.1526   -0.2047   -0.0755    0.1854    0.4329    0.0754\n",
      " -0.539    -0.1209    0.03256  -0.2893    0.5845   -0.04822   0.1292\n",
      "  0.3489   -0.3364   -0.00898  -0.1395    0.03574   0.2927    0.2812\n",
      "  0.5386    0.207    -0.4436    0.2668    0.3113    0.1447    0.3127\n",
      " -0.1863   -0.1107    0.0529    0.3965   -0.8394   -0.2363   -0.5356\n",
      " -0.3474   -0.1407   -0.264    -0.2162   -0.2338    0.3054    0.10425\n",
      " -0.133    -0.2482   -0.3909   -0.406     0.0386   -0.03485   0.8555\n",
      "  0.00972  -0.2001   -0.1959   -0.121    -0.5073   -0.471    -0.1726\n",
      "  0.3435   -0.0554   -0.010956  0.307     0.2162    0.522    -0.5176\n",
      " -0.2017    0.0733  ]\n",
      "query_embedding = [ 0.1404    0.174     0.577     0.2196   -0.9014    0.881    -0.1627\n",
      "  0.7617   -0.11914   0.02338   0.2474   -0.3171   -0.2286    0.1617\n",
      "  0.4385    1.118    -0.4087   -0.7876    0.08655   0.4094    0.835\n",
      " -0.1759   -0.11316   0.622    -0.2439   -0.404    -0.391     0.427\n",
      "  1.155    -0.638     0.5327   -0.403    -0.5234   -0.514    -0.2385\n",
      " -0.5225   -0.9907   -0.5337   -1.156    -0.659    -0.3076    0.341\n",
      "  0.41      0.1123   -0.3237    0.9424    0.708     1.005    -0.2498\n",
      " -0.0816    0.2673   -0.2876    0.04092   0.857    -0.004597 -0.8423\n",
      "  1.138    -0.3127    0.2605   -0.2852   -0.9644   -0.3608   -0.3235\n",
      " -0.4124    0.2428   -0.04675   0.763     0.011665 -0.793    -0.3396\n",
      " -0.592    -0.081    -0.3865    0.4194   -0.1844    0.3738   -1.146\n",
      "  0.5913   -0.3735    0.3503    0.616     0.233     0.477     1.265\n",
      " -1.105    -0.7324    0.3044    0.5796   -0.0406   -0.717     0.2367\n",
      " -0.06604   0.4539    0.6353    0.2683   -0.0877   -0.09235   0.3225\n",
      " -0.2002    0.186   ]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_expanded = []\n",
    "for word in vocabulary:\n",
    "    #obtem as 40 mais similares palavras de cada uma do vocab original\n",
    "    _,words = kdtree_embedding.get_most_similar_embedding(word,40)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Veja aqui as palavras usadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"despised, seductive, doom, consternation, bringing, orderliness, sweating, seeming, alerts, honor, patriotism, hoping, provide, ambuscade, affronted, degrading, unleashed, action, resenting, retaliation, petulance, termed, x.xx.xx.xx.x, looked, intense, liveliness, leave, unbelievable, aches, fearful, reminded, compliments, homoeroticism, rubber, insouciance, applaud, see, threats, humbling, independency, inquisitive, infused, cumbersome, theatricality, current, captivating, disconcerted, calamitous, timeless, summons, enjoyment, needs, desultory, sympathy, predictably, shouts, .0342, addition, tempted, reverie, much, apologize, interactive, emphasized, fluctuate, censer, twitchy, anguish, loyalty, liberty, dejected, hysteria, skepticism, kudos, smugness, wo, hoped, distracting, wisdom, affection, assistance, weary, fearlessness, aghast, sitting, empathy, healthy, hypocrisy, profess, fired, bafflement, antagonism, vomiting, hesitancy, belated, similarly, forget, disliking, gph04bb, committment, lethargy, abrade, betrayal, dislike, horrendous, miserable, wicked, culpability, stoicism, explode, furthermore, firecrackers, complacency, affectations, ballast, js04bb, regret, ironically, lg03, laborious, admiration, engendered, insult, howls, remorse, celebrating, curiously, alarms, passivity, retribution, kd94, rage, gentleness, affected, dreamworks, indestructibility, scary, lechery, anyway, piety, awesome, suspenseful, blame, abhor, status, stop-motion, deepens, encumber, demonstrators, tears, expressing, publicized, acquiescence, denouncement, darkening, courtesy, disgust, grandiosity, god, 3-d, revenge, hardly, cow, boredom, exasperating, thoughtlessness, engage, pleasantly, serenity, sake, creations, threat, confusion, decry, competence, worse, temperament, tellingly, informality, viewed, unfathomable, soon, mo95, endearing, shamed, grief, zeal, fluidity, insipid, monotonous, ethos, anticipating, evocative, unseemly, depreciated, occasion, didacticism, gone, rattled, constricted, mortifying, immediate, qualms, idiosyncrasy, crunch, fatalism, true, look, apprehensive, cheerily, enraged, cynicism, villainy, displeasure, frivolity, idealize, friendliness, maneuverability, unsettled, nonchalance, reassured, em96, uninterested, quietness, reticence, aggressiveness, euphoria, forsake, detest, malevolence, clumsiness, fervor, palpitations, staggering, rather, sloppiness, bravado, kd96, burned, curses, perplexity, mocks, familiarity, cheerfulness, brooding, heaped, pleading, increasing, thus, theatrical, pointless, oddness, jubilation, oblivious, vilification, latest, transference, excercise, odd, visualise, asserting, wake, individualization, hurry, amorality, respecting, outsiders, takeover, marvelous, sparks, everyone, sensuous, cheerleaders, surprising, counter, embarrassment, signal, votives, jubilant, blaming, hopes, selflessness, aversion, exaltation, mistake, intellectualism, blandness, respects, unexpected, clearly, perceived, discouragement, believes, recognising, fantastic, longing, disgrace, genuine, suggesting, upon, resents, shocking, warlike, unflustered, migraines, gleeful, miffed, apologise, aimlessness, nonetheless, alerting, came, whimsy, photography, k978-1, video, bereft, sort, cinematic, script, carnality, uniqueness, booing, delightful, anticipation, distraught, elegiac, anxieties, coarseness, resent, signifying, menacing, tiredness, anger, epically, pleasure, slander, airiness, impression, frustration, attacking, invective, worrying, skeptical, rallying, rollicking, nervous, loathe, response, fierce, postmodernists, overdressed, scare, dizzying, depress, arduous, irritation, likening, meekness, debasement, everybody, cheered, yet, though, acetylene, swings, despondent, happiness, ungenerous, although, rabbit, scorned, incensed, devaluate, worried, overjoyed, brominated, come, soothing, k587-1, unfairness, untruth, lately, scared, realization, disillusionment, wondering, dissapointed, know, dismay, livid, self-monitoring, loathing, fixated, immediately, cushioned, glory, suffering, glum, obsession, unsurprisingly, brokenhearted, antagonistic, flood, astonishing, rebelliousness, raucous, unsure, bulletinyyy, kroyts, monotony, worship, newborn, vruhl, say, thanks, farcical, breathtaking, vatten, luck, benefit, uselessness, documentary, disenchantment, tedious, elaboration, hesitant, contribution, horrifying, gobsmacked, give, stupefaction, antipathy, wondered, overlong, despondency, denial, offering, greeted, ironic, admirable, kindling, revelatory, diarrhea, exhilarating, backlash, disastrous, adore, sadness, unsatisfied, surveillance, wonderment, fool, prideful, lucky, insulted, honored, evocation, tedium, disturbing, guiltily, frankness, watching, weird, montanans, naborsind, faith, alarm, erratic, optimism, http://www.mediabynumbers.com, tihg, madness, exuberance, studios, aback, understanding, goodness, sight, symbolizes, mims, suggests, hopelessness, poignant, spraying, wanderlust, moral, heartburn, superlatives, sublime, discomfiting, spirit, regards, contend, irritated, believing, sluggishness, solemnity, distressing, payback, pervading, generalise, insanity, behest, http://www.nifc.gov/, vwahr, acceptance, explicitness, easy, thank, lament, excitement, treachery, stupid, jitters, ..., comfy, equate, engaged, unnerving, beeswax, innocence, undecipherable, approbation, fahnt, skillz, resented, contempt, unhappiness, goal_montreal, baffled, crispness, calumny, voluptuousness, depredations, cheering, prefer, applauding, resentment, artfulness, equanimity, _____________________________________________, doldrums, films, spirituality, sudden, dispirited, imagine, dishonour, indication, euoplocephalus, nasty, surprise, deflate, appreciating, esteem, relentless, destitute, signified, clueless, sinicization, deliriously, wonder, engagement, realize, speechless, startled, ill-informed, detractors, sprinkled, masterful, storyboard, questioning, aid, way, outraged, protesting, insufferable, academicism, flame, shame, gentility, commitment, onslaught, tranquillity, presume, demoralising, swoon, sending, contrarians, angered, demonstrate, proclivities, counteract, apparent, crankiness, joy, sympathize, tiresome, shock, earthiness, earnestness, irate, blurbed, usual, nervousness, survivability, hungry, contingent, unpleasant, lonely, attitude, popi, nowadays, comfortably, unleash, mind, frustrations, confused, pretty, headaches, celebration, ineptitude, complacent, disheartening, intemperate, likely, cannons, myrrh, wrath, else, loved, embarassing, soulful, desperation, resisting, nowhere, chagrin, aware, stupefied, hysterical, merriment, raised, disgusting, heartbreaking, suspicion, forlorn, panic, cramping, yearning, throwing, reverence, recklessness, fervour, directness, studio, intimation, dissatisfaction, ridicule, astounding, censers, hydrodynamic, bullets, diffident, explosiveness, sensational, propriety, happy, stress, investor, puzzlement, angry, compassion, engendering, shaken, js94bb, compounded, sanguine, love, reluctance, hopeful, malice, shyness, nerves, pity, suzuya, ruminative, unconvincing, allied, inability, expectations, hesitation, perceive, self-identity, passion, inconsequential, lifeline, guffaws, stressing, pretentiousness, stunned, boisterous, kid, astonishment, spared, frazzled, starving, ignored, higher, insomnia, restless, curiosity, recovery, bullet, embarrassed, stun, stay, overwrought, furor, insensitivity, cruel, skyrocket, patient, bdb94, sorry, wanted, neediness, heft, humiliated, chore, emasculation, spooked, rioters, copal, knowing, indignation, dying, pleased, throngs, denunciations, amidst, demoralizing, rejoicing, indignant, enigmatically, disappointed, feeling, eventful, rectitude, chafe, regrets, dreams, preoccupation, maddening, joyless, malevolent, hilarity, incredulous, circumspect, symbolize, sure, soothe, sparklers, inscrutably, snobbery, joke, pathos, wishing, pregnant, welcome, disheartened, beast, plodding, pleasurably, combativeness, ferocious, determination, constipation, rootlessness, tearing, candles, nightmare, overconfidence, retardants, fatuous, mothers, cackles, virtue, bewildered, elderly, turn, scornful, provided, desire, unconvinced, enchanting, abasement, bemusement, drubbing, lyricism, incense, speculation, sassacus, horrified, feature, unfortunately, sadly, animated, instream, reflected, claustrophobia, temper, searing, realizing, oly-2004-tennis, cushioning, candle, creativity, thrown, delirious, threatening, fortunate, stupendous, sensitivity, drowsiness, grenades, skylarking, david.lazarus@latimes.com, complaining, praise, particular, thrilled, wearisome, delighted, promise, soothed, deployed, apathy, exhilarated, .0170, reprisal, vitriol, reassurances, castigation, given, acknowledgement, drudgery, impotent, predicament, foolish, bhagya, clamor, .0163, poignancy, quietude, irreverence, neuroses, hope, embarrassing, filmmakers, animation, decent, solid, looks, buoyancy, warrant, shouting, abhorred, sent, pessimistic, ferocity, anxious, elicited, saw, thankless, rousing, quick, principle, pride, depletes, vulnerability, http://www.oklahomacitynationalmemorial.org, misfortune, canisters, cleverness, doubt, painstaking, ready, predilection, devaluing, worry, gloom, calmed, covetousness, camphor, garlands, inconsolable, mtow, greet, nosebleeds, spurred, here, emergency, perplexed, undeniable, debauchery, predominance, peacefulness, value, animations, compensators, egomania, integrity, wistful, mocked, lanterns, incredulity, crystallise, internalize, sanctimony, exuberant, unhappy, forcefulness, amazing, concur, disillusion, homeless, divine, disdain, notice, relaxed, devaluating, callousness, grateful, prompts, deplore, bullish, stressed, hostility, apologizing, entrancing, rebukes, elation, peppermints, inspiration, proud, eager, disbelief, inscrutable, firestorm, awe, astounded, fit, establishment, focus, judgment, loveliness, distasteful, antigravity, gloomy, cowardice, overanalyze, simple-minded, awkwardness, .000088, interest, coming, useless, interestingly, accuse, edgy, rates, greg.wilcoxdailynews.com, heartlessness, compensator, looking, devotion, storytelling, baffling, kalamity, prosecution, christian, hand-drawn, annoyance, recognizes, crazed, timelessness, mendacity, dumbfounded, repudiate, distaste, opportunity, enjoys, fearing, shout, brightly, aggressive, unfriendly, rejecting, revulsion, jumpy, jaded, cat, maybe, open-mindedness, ghastly, envious, puzzling, str95bb, depreciate, longevity, decisiveness, disperse, thing, apology, ripoffs, peculiar, detestation, .0206, bliss, thoughtfulness, inviting, understand, ashamed, gripped, spray, bittersweet, supportive, depreciating, undeterred, moralize, despises, conviction, thankful, engaging, panicked, mindful, newfound, noting, better, haunting, equated, blamed, southpaws, de-emphasize, ire, playfulness, vivacity, firing, expression, lucidity, dramatic, flummoxed, future, unrelenting, tendency, sad, lopsided, fascinating, pessimism, infuriating, regardless, believers, christ, assure, hatred, contrition, politeness, messy, cautiously, delight, sorrowfully, disorienting, agony, constrain, entranced, wariness, ominous, explain, hubbub, opposed, bad, strangeness, cuteness, patronize, enjoyable, suggestion, individuality, indulged, expecting, actions, announcement, frantic, seething, uncomfortable, adulation, bemused, broken, interesting, heartfelt, legacy, crowd, demonization, pitiful, glib, groggy, giddy, reflexively, convinced, uproar, magnanimity, potpourri, ethereal, defiance, prompted, reason, topside, boldness, shrilly, anticipate, kd97, talky, raise, inattention, cries, repentance, experience, permanence, canister, benignly, admitting, foreboding, treated, batons, recognition, plaudits, chance, leniency, disliked, evacuation, understandably, impishly, savage, ambivalent, omnipresence, idleness, alerted, bullishness, ardor, mercy, rashes, nevertheless, belief, bedraggled, why, stunning, dizziness, calling, mortified, paranoia, sociability, danger, triumph, glad, exactitude, depth-charged, guileless, passions, fear, rehabilitation, embittered, insisted, denigration, unfazed, catcalls, riot, caring, emotion, romanticize, greatness, fatigue, mournful, vengeance, uninteresting, burn, wishes, ornaments, depressing, blames, perfidy, cathartic, enraging, prosecuted, tiring, admonition, pains, letting, exhortation, celebrate, nausea, frustrating, scandalous, spellbinding, biotechtrst, good, embrace, puzzled, accomplished, sticks, believer, transcendence, continuing, obviously, enamored, furious, unleashes, bloodlust, strongly, tired, penchant, wonderful, leery, sorrow, privilege, bombast, shields, warning, joyful, lit, viciousness, surprised, vengeful, obstinate, buoyant, www.slarmy.org, bring, acasta, itching, indeed, dreadful, liking, demonstrates, grotesque, underwhelmed, townsfolk, marveled, enthusiasm, fxff, musculature, religion, perfunctory, clumsy, idiocy, illumined, grace, exasperation, beenz, buying, pig, tradition, manoeuvre, compelled, obsessed, heartbroken, rescue, implication, k977-1, intemperance, dispersing, enjoying, testifying, cheer, disrespected, imprimatur, surely, formulaic, prompting, assurance, presumption, regard, cautiousness, weekend, exoticism, comfortable, discomfited, appalling, appreciative, irritability, visual, wacky, libations, assist, ardour, nonplused, playful, headache, amazement, 65stk, pleasantness, unquestionable, frenzy, concerned, unambiguous, creative, stubbornness, cared, dismaying, downcast, mesmerized, distraction, affirmation, enthralling, votive, dubbed, discontent, regretful, crowds, unbothered, frankly, chagrined, morose, treat, beguiling, frustrated, propensity, disorientation, productions, arrogance, dumbstruck, doctrine, remarkable, fury, uncouth, concerns, bloating, distrust, parents, feelings, acknowledgment, arguing, incredibly, animator, reticent, terrifying, gratitude, belligerent, giddiness, annoyed, self-pity, scripts, loath, perturbed, fact, provoked, mystifying, lambasting, falseness, surreal, inclusion, misunderstand, folks, wretched, paradoxically, prospects, slovenliness, opprobrium, grumble, exhilaration, pranked, ignorance, unamused, achievements, ordeal, perfect, assured, wafted, convoluted, shadows, humaneness, catharsis, nostalgia, take, efforts, relief, incredible, delights, achieved, alienation, jittery, penury, turning, amused, greenness, mockery, unimpressed, uplifting, infuriates, besides, rest, anticipated, uninitiated, malaise, exasperated, hopefulness, accustomed, invigorating, redemption, excruciatingly, demeaning, 'm, dislikes, plausibility, afflicted, shocked, lobbing, reassure, recognized, remembered, exultant, anguished, enlightening, cautioned, ought, electrifying, helplessness, motivation, definitely, withering, mania, wanting, precocity, opposing, candor, satisfied, revelations, imperieuse, awed, unprompted, .0208, visuals, symbolized, purposefulness, jingoism, fretted, maelstrom, alabamians, marvellous, tantrum, fatigued, continue, shelter, exhausting, biodegrade, disinterest, absurdity, thrilling, wearying, emotionalism, despise, craving, heightened, acknowledge, accepted, destitution, nor, putting, advocation, filmmaking, hell, loving, unease, error-prone, think, disrespect, confident, teargas, resultant, inevitable, sentimentality, irked, depreciates, srivalo, callous, dullness, eschew, reaffirmation, resentful, .000105, unconcerned, manner, support, re-visited, stranger, concern, determined, loneliness, seeing, otherwise, upsetting, fulfillment, befuddling, …, evoked, dismayed, participation, bb94, humiliating, going, reconstruction, torment, cramps, derision, triumphalism, recognize, tear, insolence, euphoric, 'd, panache, multimedia, impressive, flamboyance, optimistic, doubles_biggio, lollipops, meant, recognizing, adventurousness, .0207, ugly, shellshocked, uneasiness, keeping, animators, liken, bizarre, intrigued, cowardliness, brashness, immortality, warn, helpless, surprises, disgruntled, dazzling, need, moment, inconstancy, heartache, bemoan, plenty, outbursts, panicky, tantrums, over-produced, truly, alarmed, demented, precaution, exclusivism, grossness, terrible, awful, burning, heartless, referring, expect, disregard, angst, mistaken, complain, oly-2004-fhockey, circularity, giving, ill-founded, appreciation, humility, fretful, crestfallen, flatten, coldness, clear, fend, bitterness, enjoyed, complained, rise, importance, interpenetration, always, eagerness, brave, care, outrage, uncertain, hauteur, immaturity, raising, pellets, crazy, encouragement, solitude, symptoms, injustice, time-consuming, thunderstruck, simply, mystified, jealousy, sprayed, equal, 're, dread, unassertive, curious, disconsolate, ingenuous, tankage, palpable, essence, abandon, awkward, sympathetic, amazed, inflexibility, graciousness, ecstatic, cartoon, menorah, rp-1, mad, selfishness, reassuring, neither, haughtiness, remorseful, recognised, despair, defensiveness, misunderstood, cannon, hurled, jolting, 28aou94, pathetic, help, sense, lust, unlikely, unworldly, dignity, disappointment, merits, indifference, want, condescension, sentimental, impress, burst, distracted, invoking, squeamish, whatever, strange, allusion, dispatched, unfocused, impatience, displeased, restlessness, coloradans, laziness, liberality, migraine, troubling, repetitious, flabbergasted, cry, stylish, upbeat, importantly, disgraceful, phenomenal, mayhem, surliness, despairing, edginess, str94, hopeless, emotions, ornery, risible, antsy, frankincense, distrusted, frightened, re-organise, munificence, solipsism, beliefs, agitate, comforted, debase, followed, despite, fiery, irksome, awestruck, success, pronouncement, find, editing, trepidation, nice, enthused, indigestion, reproach, price, saves_mrivera, stones, anxiety, dispiriting, betrayed, wondrous, lest, vigilant, benumbed, effervescence, shamefaced, eloquence, monster, certainly, joyous, dishonesty, rheology, self-love, doomed, stabiliser, dehumanization, astonished, drohs, bruising, torturous, confess, fears, discouraged, horrific, dreamy, minions, spend, evident, admit, js03, melancholy, clamoring, cue, hipness, soulfulness, splendid, avoid, reconsideration, terrified, vega@globe.com, hostile, agitated, peeved, bb97, laughter, mesmerizing, puppetry, discomfort, satisfaction, perturb, respect, visibly, raisonnable, quite, discombobulated, cacophony, pensive, remember, action-packed, dehydration, warned, magnificent, guilt, deprogrammed, dispersed, excellence, exoneration, unsettling, rw95, supplant, irritating, cautious, insularity, overabundance, apologies, exploding, stirred, merciless, oftentimes, bangkokians, proclivity, expressed, fevers, responding, indifferent, perfectionism, omission, believe, nuttiness, unsatisfying, sullen, certain, implying, unnerved, scapegoating, diminishes, torches, sandalwood, admired, patterson, introspective, trouble, lending, .0202, insist, jaundice, enjoy, frenzied, expectation, silliness, menace, physicality, melancholic, rebuilding, idolize, organgn, startling, sickness, clarity, http://www.opel.com, washingtonians, rudeness, unironic, geotagging, legitimacy, uncharacteristic, dejection, depressed, angering, appalled, boring, keep, constricting, deform, denunciation, shameful, dampened, pain, bewilderment, messiness, sorrowful, effort, moreover, coded, pictures, thrill, powerless, felicitously, displeasing, afraid, ignominious, irony, unprepared, insinuation, outcry, shrewdness, claiming, brutalization, needed, skittish, predicting, scorn, unfortunate, hypocritical, yearn, humble, precautions, mishandle, devalue, nonplussed, helping, preoccupied, unforgettable, check, tumult, perfection, disaster, wish, conscience, intimidated, nastiness, outspokenness, unexcited, overact, befuddled, humanitarian, distinction, alert, contrary, achievement, engrossing, apotheosis, reflecting, live-action, inexplicable, piyanart, frightening, vigilance, disgusted, stupidity, put, ingratitude, fabulousness, offended, brought, blunt, tragic, illness, fascinated, confronted, protectiveness, humiliation, freedom, breathless, languor, hug, impressed, ignore, misery, 24aou94, admiring, feel, truth, sleeplessness, really, exclamations, irrepressible, providing, youthfulness, uncertainty, excited, anti-clericalism, vulnerable, impatient, failure, burners, trivialize, bb96, 30-270, pixar, fun, peevish, suited, significance, affectation, ponderous, propitiation, overconfident, purifier, noticed, fondness, ridiculousness, getting, unremorseful, protection, perspire, lightheadedness, 'll, surprisingly, childlike, sick, envy, goofiness, diarrhoea, inadequacy, ambition, attentiveness, acknowledging, confidence, film, horrible, feistiness, wary, unserious, abashed, thermoregulation, hatter, omniscience, amusing, incomparable, spirited, contentment, dog, warnings, forgiveness, admire, cheers, disney, fascination, concision, licentiousness, treating, cgi, evildoers, homesickness, radiance, staying, shortness, canonicus, fergalicious, spite, timidity, appreciate, gyroscopic, ambivalence, borrowing, heartwarming, mellow, protestors, tempered, antics, protesters, weariness, flustered, wrong, upset, riveting, elated, detests, downsize, insistent, steadiness, ill, hypnotised, ascribe, scented, firebombs, ignoring, infuriated, vituperation, babies, intimacy, fading, ennui, defeats\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Algumas palavras podem não estar relacionadas à emoção, porém, o método de aprendizado de máquina ainda é capaz de considerar palavras mais relevantes para uma determinada instancia, ignorando algum ruído. Veja como ficou a representação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>canister</th>\n",
       "      <th>disappointed</th>\n",
       "      <th>experience</th>\n",
       "      <th>fact</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>find</th>\n",
       "      <th>give</th>\n",
       "      <th>god</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>put</th>\n",
       "      <th>putting</th>\n",
       "      <th>sense</th>\n",
       "      <th>sick</th>\n",
       "      <th>sympathetic</th>\n",
       "      <th>true</th>\n",
       "      <th>video</th>\n",
       "      <th>wanting</th>\n",
       "      <th>worse</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             bad  canister  disappointed  experience      fact  fascinating  \\\n",
       "id                                                                            \n",
       "208138  0.000000  0.000000      0.000000    0.606043  0.000000     0.000000   \n",
       "157010  0.000000  0.000000      0.000000    0.000000  0.490297     0.490297   \n",
       "101657  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "49225   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "158265  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "204215  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "274316  0.606043  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "57708   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "200048  0.000000  0.707107      0.707107    0.000000  0.000000     0.000000   \n",
       "60933   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "\n",
       "        find      give  god      good  ...       put   putting     sense  \\\n",
       "id                                     ...                                 \n",
       "208138   0.0  0.606043  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "157010   0.0  0.000000  0.0  0.324199  ...  0.416798  0.000000  0.000000   \n",
       "101657   0.0  0.000000  0.0  1.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225    0.0  0.000000  0.0  0.551556  ...  0.000000  0.000000  0.000000   \n",
       "158265   0.0  0.000000  0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215   0.0  0.000000  0.0  0.000000  ...  0.515192  0.000000  0.606043   \n",
       "274316   0.0  0.000000  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "57708    0.5  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "200048   0.0  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "60933    0.0  0.000000  0.0  0.313903  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        sick  sympathetic      true     video  wanting     worse     class  \n",
       "id                                                                          \n",
       "208138   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "157010   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "101657   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "49225    0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "158265   0.0          0.0  0.000000  0.000000      0.5  0.000000  positive  \n",
       "204215   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "274316   0.0          0.0  0.000000  0.000000      0.0  0.606043  negative  \n",
       "57708    0.5          0.5  0.000000  0.000000      0.0  0.000000  negative  \n",
       "200048   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "60933    0.0          0.0  0.474727  0.474727      0.0  0.000000  negative  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Poderiamos agrupar as palavras chaves em conceitos, por exemplo, \"happiness\" ser sempre contabilizado quando houver um conjunto de palavras, por exemplo, '\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"'. Porém, isso pode restringir muito o número de palavras e expandir com palavras usando embeddings, pode extrair palavras relacionadas com a emoção oposta (veja exemplo abaixo). Por isso, optamos por apresentar a representação usando bag of words. Mesmo assim, caso queira ver algum resultado dessa forma, a classe CountWords implementa expansão por grupos de palavras chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.09045  0.1964   0.2947  -0.477   -0.804    0.3079  -0.5522   0.5845\n",
      " -0.1705  -0.8486   0.1953   0.2367   0.4683  -0.59    -0.12164 -0.247\n",
      " -0.07294  0.1726  -0.0485   0.9526   0.5063   0.585   -0.1937  -0.4546\n",
      " -0.0311   0.516   -0.2405  -0.1007   0.536    0.02423 -0.5015   0.737\n",
      "  0.4946  -0.3474   0.8936   0.05743 -0.1913   0.3933   0.2118  -0.8984\n",
      "  0.0787  -0.1635   0.4526  -0.411   -0.195   -0.1349  -0.01631 -0.02185\n",
      "  0.1714  -1.241    0.0795  -0.9116   0.357    0.3628  -0.2494  -2.12\n",
      "  0.1454   0.53     0.9014   0.0336   0.02281  0.706   -1.036   -0.598\n",
      "  0.706   -0.0728   0.6704   0.528   -0.478   -0.674    0.3662  -0.3828\n",
      " -0.1035  -0.64     0.181    0.8257   0.0664  -0.408   -0.0838  -0.365\n",
      "  0.04535 -0.07355 -0.2012   0.3745  -1.402   -0.256   -0.4707  -0.1615\n",
      " -0.8794  -0.3633  -0.1736  -0.078    0.4326   0.00893 -1.031   -0.1159\n",
      " -0.3452   0.1151  -0.4082   0.202  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"happy, feel, glad, sure, everyone, 'm, definitely, 'd, 'll, remember, everybody, wish, proud, 're, really, always, maybe, excited, good, lucky, obviously, thrilled, pleased, pretty, wonderful, know, afraid, delighted, looking, want, thing, imagine, think, unhappy, satisfied, realize, knowing, going, tired, crazy\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distance, words = kdtree_embedding.get_most_similar_embedding(\"happy\",40)\n",
    "#veja que unhappy é relacionado com happy - além de outras palavras negativas e ruido\n",
    "\", \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pride</th>\n",
       "      <th>elation</th>\n",
       "      <th>happiness</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>relief</th>\n",
       "      <th>hope</th>\n",
       "      <th>interest</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>sadness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>shame</th>\n",
       "      <th>guilt</th>\n",
       "      <th>disgust</th>\n",
       "      <th>contempt</th>\n",
       "      <th>hostile</th>\n",
       "      <th>anger</th>\n",
       "      <th>recognition</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pride  elation  happiness  satisfaction  relief  hope  interest  \\\n",
       "208138      0        0         14             0       0     7         0   \n",
       "157010      0        0          5             0       0     5         2   \n",
       "101657      0        0          7             0       0     5         0   \n",
       "49225       0        0          4             0       0     3         0   \n",
       "158265      0        0          3             0       0     1         0   \n",
       "204215      0        0          0             0       0     1         0   \n",
       "274316      0        0          2             0       0     0         0   \n",
       "57708       0        0         10             0       0     7         0   \n",
       "200048      0        0          4             0       0     2         0   \n",
       "60933       0        0         12             0       0     2         0   \n",
       "\n",
       "        surprise  anxiety  sadness  boredom  shame  guilt  disgust  contempt  \\\n",
       "208138         0        4        0        0      0      0        0         0   \n",
       "157010         0        3        0        0      0      0        0         0   \n",
       "101657         0        0        0        0      0      0        0         0   \n",
       "49225          0        1        0        0      0      0        0         0   \n",
       "158265         0        1        0        0      0      0        0         0   \n",
       "204215         0        1        0        0      0      0        0         0   \n",
       "274316         0        0        0        0      0      0        0         0   \n",
       "57708          0        5        0        0      0      0        1         0   \n",
       "200048         0        0        0        0      0      0        0         0   \n",
       "60933          0       12        1        0      0      0        0         0   \n",
       "\n",
       "        hostile  anger  recognition     class  \n",
       "208138        0      0            0  positive  \n",
       "157010        0      0            0  positive  \n",
       "101657        0      0            1  positive  \n",
       "49225         0      0            0  positive  \n",
       "158265        0      0            0  positive  \n",
       "204215        0      0            0  negative  \n",
       "274316        0      0            0  negative  \n",
       "57708         0      0            0  negative  \n",
       "200048        0      0            0  negative  \n",
       "60933         0      0            0  negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import CountWords,InstanceWisePreprocess\n",
    "aggregate = CountWords(dict_embedding, emotion_words,max_distance=0.3)\n",
    "\n",
    "word_counter = InstanceWisePreprocess(\"word-counter\",aggregate)\n",
    "word_counter.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O max_distance é resposável por obter as palavras similares. Veja que diversos documentos negativos foram classficados com o grupo \"happiness\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Representação agregando embeddings das palavras: ** Conforme proposto por [Shen et al.](https://arxiv.org/pdf/1805.09843.pdf), dado que uma frase é representado por um conjunto de embeddings $\\{e_1, e_2, ..., e_n\\}$  uma forma simples e que geralmente obtém resultados **comparáveis a métodos mais complexos** é fazer operações em cada dimensão do embedding, tais como: média e máximo por dimensão do embedding. Por exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'green']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings de alguams palavras: \n",
    "dict_embedding = {'my':      [10, 11,14, 20, 15, 80],\n",
    "                  'house':   [11, 12,10, 24, 11, 30],\n",
    "                  'is':      [1,  3,  5, -1, 10, 20],\n",
    "                  'green':   [12,10, 20, 12, 10, 20]\n",
    "                   }\n",
    "#representação do texto \"my house is green\"\n",
    "arr_texto = \"my house is green\".split()\n",
    "arr_texto      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Usando a média de cada dimensão dos embeddings:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [8.5, 9.0, 12.25, 13.75, 11.5, 37.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula a média da iésima posição do embedding\n",
    "        sum_pos = 0\n",
    "        for word in arr_texto:\n",
    "            sum_pos += dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(sum_pos/len(arr_texto))\n",
    "    return representacao\n",
    "dim_embedding = 6\n",
    "representacao = average_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Usando o máximo de cada dimensão dos embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [12, 12, 20, 24, 15, 80]\n"
     ]
    }
   ],
   "source": [
    "dim_embedding = 6\n",
    "def max_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula o valor máximo de cada iésima posição do embedding\n",
    "        first_word = arr_texto[0]\n",
    "        max_pos = dict_embedding[first_word][i]\n",
    "        for word in arr_texto[1:]:\n",
    "            if max_pos < dict_embedding[word][i]:\n",
    "                max_pos = dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(max_pos)\n",
    "    return representacao\n",
    "representacao = max_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como há palavras pouco relevantes (como stopwords) podemos remove-las e, também podemos utilizar apenas as palavras de um vocabulario controlado. Abaixo veja a representação. Como esta representação é vetorial, a mesma não é uma representação simples de ser entendida por humanos, porém, pode-se obter bons resultados. Você pode adicionar o vocabulario controlado ou as stopwords por meio dos parametros correpondentes. O parametro `aggregate_method` define se será feito um maximo ou média entre os embeddings colocando os valores `max` ou `avg`, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>-0.059204</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.372070</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>-0.102600</td>\n",
       "      <td>-0.024734</td>\n",
       "      <td>-0.340088</td>\n",
       "      <td>0.083252</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258545</td>\n",
       "      <td>0.219971</td>\n",
       "      <td>-0.457031</td>\n",
       "      <td>-0.332764</td>\n",
       "      <td>-0.326172</td>\n",
       "      <td>-0.164795</td>\n",
       "      <td>-0.160400</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.268555</td>\n",
       "      <td>0.395020</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>-0.191528</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.433350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>-0.274170</td>\n",
       "      <td>-0.524414</td>\n",
       "      <td>-0.061218</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.521973</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.541992</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>-0.030762</td>\n",
       "      <td>0.119934</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>-0.437012</td>\n",
       "      <td>-0.739258</td>\n",
       "      <td>-0.153442</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>-0.385498</td>\n",
       "      <td>-0.687988</td>\n",
       "      <td>-0.416260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440430</td>\n",
       "      <td>0.083313</td>\n",
       "      <td>0.200317</td>\n",
       "      <td>-0.754883</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.528809</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>1.065430</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>0.441895</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-0.411377</td>\n",
       "      <td>-0.300293</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283691</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>-0.050598</td>\n",
       "      <td>-0.695801</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.408447</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.449951</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>-0.087097</td>\n",
       "      <td>-0.048950</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>-0.627441</td>\n",
       "      <td>-0.139526</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>-0.392822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.448730</td>\n",
       "      <td>-0.454346</td>\n",
       "      <td>-0.360840</td>\n",
       "      <td>-0.517578</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.040253</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.346191</td>\n",
       "      <td>-0.253418</td>\n",
       "      <td>0.219482</td>\n",
       "      <td>-0.346924</td>\n",
       "      <td>-0.459229</td>\n",
       "      <td>-0.234009</td>\n",
       "      <td>-0.016251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>-0.037048</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.152466</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>-0.471924</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.158936</td>\n",
       "      <td>-0.016418</td>\n",
       "      <td>0.793945</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>-0.129272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.206665</td>\n",
       "      <td>-0.333496</td>\n",
       "      <td>0.056488</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.162231</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.299561</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.139404</td>\n",
       "      <td>0.153687</td>\n",
       "      <td>-0.460449</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>-0.473389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045044</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.549805</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.111450</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>-0.154175</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>0.332520</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>-0.582031</td>\n",
       "      <td>0.210815</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.190674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284912</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.336914</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>-0.390137</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.071411</td>\n",
       "      <td>0.483154</td>\n",
       "      <td>-0.466309</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>-0.036652</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>-0.241211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>0.099060</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>-0.311035</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.349121</td>\n",
       "      <td>0.174072</td>\n",
       "      <td>0.460205</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "208138 -0.059204  0.191895  0.372070  0.126587 -0.102600 -0.024734 -0.340088   \n",
       "157010 -0.064270  0.268555  0.395020 -0.094360 -0.176514  0.011276 -0.220093   \n",
       "101657 -0.030762  0.119934  0.539062 -0.437012 -0.739258 -0.153442  0.081116   \n",
       "49225  -0.127930  0.394287  0.441895 -0.243408 -0.411377 -0.300293  0.028076   \n",
       "158265  0.449951  0.435547  0.288086 -0.087097 -0.048950  0.714844 -0.627441   \n",
       "204215  0.040314  0.040253  0.532227 -0.346191 -0.253418  0.219482 -0.346924   \n",
       "274316  0.158936 -0.016418  0.793945 -0.266113 -0.669434 -0.004871 -0.398193   \n",
       "57708   0.162231  0.439941  0.299561 -0.111328 -0.139404  0.153687 -0.460449   \n",
       "200048 -0.154175  0.127686  0.332520 -0.328125 -0.582031  0.210815  0.188110   \n",
       "60933  -0.006786  0.071411  0.483154 -0.466309 -0.287598  0.137451 -0.036652   \n",
       "\n",
       "               7         8         9  ...        91        92        93  \\\n",
       "208138  0.083252 -0.264160 -0.224731  ... -0.258545  0.219971 -0.457031   \n",
       "157010 -0.191528 -0.177979 -0.433350  ... -0.135132  0.084656 -0.274170   \n",
       "101657 -0.385498 -0.687988 -0.416260  ... -0.440430  0.083313  0.200317   \n",
       "49225  -0.054688 -0.478516 -0.224731  ... -0.283691  0.092102 -0.050598   \n",
       "158265 -0.139526  0.037781 -0.392822  ... -0.330322  0.201050  0.036438   \n",
       "204215 -0.459229 -0.234009 -0.016251  ... -0.179688  0.093750 -0.037048   \n",
       "274316 -0.331055  0.040588 -0.129272  ... -0.127441 -0.073242 -0.206665   \n",
       "57708  -0.149048  0.320801 -0.473389  ... -0.045044  0.411377 -0.177979   \n",
       "200048  0.594238  0.397949  0.190674  ... -0.284912 -0.298096 -0.150879   \n",
       "60933   0.012230 -0.068115 -0.241211  ...  0.056396  0.058685  0.099060   \n",
       "\n",
       "              94        95        96        97        98        99     class  \n",
       "208138 -0.332764 -0.326172 -0.164795 -0.160400  0.378906  0.456543  positive  \n",
       "157010 -0.524414 -0.061218 -0.264160 -0.521973  0.185547  0.541992  positive  \n",
       "101657 -0.754883  0.169189 -0.265625 -0.528809  0.175781  1.065430  positive  \n",
       "49225  -0.695801  0.098633  0.142578 -0.408447  0.257324  0.741699  positive  \n",
       "158265 -0.448730 -0.454346 -0.360840 -0.517578  0.019989  0.399902  positive  \n",
       "204215 -0.417480 -0.152466  0.052826 -0.471924  0.008827  0.480225  negative  \n",
       "274316 -0.333496  0.056488  0.071777  0.239014  0.317383 -0.004375  negative  \n",
       "57708  -0.549805 -0.000109 -0.155273 -0.111450  0.056763  0.045990  negative  \n",
       "200048  0.051758  0.336914  0.061096  0.478516 -0.390137  0.254883  negative  \n",
       "60933  -0.135742 -0.311035 -0.453125 -0.349121  0.174072  0.460205  negative  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets\\\\stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, aggregate_method=\"avg\", \n",
    "                                            words_to_filter=stop_words, words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "emb_keywords_exp.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n",
      "query_embedding = [-0.3066   -0.06323   0.03488  -0.1909   -0.7354    0.00856   0.1096\n",
      "  0.3506   -0.01727  -0.1555    0.166     0.548     0.0995   -0.793\n",
      "  0.005028 -0.1854   -0.4702    0.1768    0.3542   -0.4429    0.05655\n",
      " -0.0721   -0.0948   -0.089     0.1793    0.00867   0.3093    0.5933\n",
      "  0.2896   -0.09656   0.10016  -0.4019   -0.1934    0.3723   -0.02042\n",
      "  0.3005   -0.2382   -0.07135   0.05432  -0.5586    0.3428    0.439\n",
      " -0.1652    0.397    -0.2627   -0.4888    0.1287    0.799    -0.0392\n",
      "  0.1985   -0.3123   -0.2142    0.4116   -0.2343   -0.1995    0.3694\n",
      "  0.4587    0.202    -0.844    -0.5967    0.1337   -0.1814   -0.456\n",
      " -0.2452    0.02249  -0.2664    0.4875    0.437     0.0174   -0.23\n",
      " -0.05627   0.3435    0.2397    0.09033  -0.4534    0.5547    0.406\n",
      "  0.5806    0.0834   -0.4453   -0.0788    0.08044   0.04617   0.0667\n",
      " -0.002344 -0.646    -0.0887   -0.3157   -0.8823    0.1499   -0.4146\n",
      " -0.1073    0.2627    0.1058    0.7324    0.02605  -0.01859   0.1885\n",
      " -0.7344   -0.2445  ]\n",
      "query_embedding = [ 0.2377    0.2079    0.3872    0.0712   -0.7334    0.964    -0.07526\n",
      "  0.3093    0.5947   -0.9067   -0.1605   -0.1726    0.2856   -1.091\n",
      "  0.3662    0.2805   -0.1228    0.02213   0.258     0.3584    0.2925\n",
      "  0.128    -0.1042   -0.7314    0.6816   -0.08765   0.07605   0.708\n",
      "  0.1417   -0.2362   -0.3408    0.2556    0.1283   -0.2378   -0.015495\n",
      " -0.2563   -0.4512    0.325     0.7446   -0.4543   -0.4348   -0.1998\n",
      "  0.175    -0.3618   -0.2043   -0.271     0.3057    0.8564    0.3196\n",
      "  0.05908   0.8423   -0.6694    0.188     0.2742   -0.52     -0.9\n",
      "  0.2367    0.08154   0.4487   -0.513    -0.1122    1.236     0.1609\n",
      "  0.05685   1.025    -0.454     0.523     0.2299    0.2362   -0.3125\n",
      "  0.06915   0.1062   -0.2013   -0.2737   -0.3508    0.601     0.545\n",
      " -0.381    -0.02414  -0.5034    0.1669    0.0806   -0.2454   -0.1865\n",
      " -0.9424   -0.3726   -0.2554   -0.1289   -0.7017   -0.8867    0.0907\n",
      "  0.167     0.1323   -0.764    -0.4797   -0.03555  -0.979    -0.3376\n",
      " -0.272     0.6636  ]\n",
      "query_embedding = [ 0.0298    0.1576    0.3062   -0.2925   -1.315     0.7144   -0.2451\n",
      "  0.4583    0.6074   -0.269     0.1086    0.482     0.6235   -1.28\n",
      " -0.02727  -0.0657   -0.6816    0.678     0.2472    0.09033   0.191\n",
      "  0.599     0.02216  -0.2203    0.702    -0.3801    0.722     0.513\n",
      "  0.7466   -0.557     0.1237   -0.2605    0.02812   0.256    -0.366\n",
      "  0.3389   -0.4329   -0.1901    0.5693   -0.4534   -0.2408    0.3694\n",
      " -0.09467   0.08154  -0.3816   -0.0674    0.5405    0.9243   -0.4446\n",
      " -0.1311   -0.2313   -0.2803    0.0815    0.074    -0.733     0.2522\n",
      "  0.08405   0.1792   -0.801     0.0867    0.09076   0.2294   -0.32\n",
      "  0.2351   -0.3513   -0.5747    0.4397    0.666    -0.01423   0.1979\n",
      "  0.4092    0.0965    0.4775   -0.3086   -0.3696    0.2622    0.303\n",
      "  0.2524   -0.2559   -0.786    -0.01088   0.1914   -0.0911    0.007675\n",
      " -0.2444   -0.7695    0.3884   -0.01665  -1.155    -0.5737    0.0945\n",
      " -0.1453   -0.0899    0.1547    0.3455    0.10925   0.2305   -0.1787\n",
      " -0.8164    0.4756  ]\n",
      "query_embedding = [ 0.0373    0.3972   -0.09247  -0.006264 -0.645     0.02327  -0.2141\n",
      " -0.09955  -0.2208    0.1218   -0.323     0.1179   -0.495    -0.3328\n",
      "  0.5186   -0.06287  -0.463     0.04565   0.4844   -1.023     0.2306\n",
      " -0.6       0.1713    0.12164   0.0695    0.413    -0.226    -0.2617\n",
      " -0.11115  -0.791     0.1592   -0.541    -1.453     0.3213    0.02643\n",
      " -0.2332    0.3013    1.231    -0.1393   -0.4783   -0.01817  -0.3853\n",
      " -0.45     -0.3064    0.6626    0.2988    0.03415  -0.1355   -0.1794\n",
      " -0.06696   0.271     0.8184   -0.1826    0.912     0.581    -0.5815\n",
      "  1.037    -0.85      0.007275 -0.4702    0.1198   -0.3452    0.266\n",
      "  1.586    -0.11127   0.3364   -0.189    -0.851     0.422    -0.4255\n",
      "  0.3413   -0.4917    0.3557    0.569     0.3818   -0.12396  -0.2068\n",
      " -0.3699    0.04617  -0.2097   -0.3464   -0.03433  -0.238     0.1306\n",
      "  0.1516    0.1482    0.6196    0.3071   -0.3582   -0.1658    0.1664\n",
      " -0.2277    0.549    -0.614     1.491     0.3445    0.05353  -0.324\n",
      " -0.1804    0.748   ]\n",
      "query_embedding = [-0.2056    0.1475   -0.94     -0.7183   -0.8164    1.722     0.1299\n",
      "  0.938     0.4807    1.08     -0.4116   -0.3281   -0.1145    0.5005\n",
      " -0.4731   -0.1383   -0.4924    0.1738    0.0501   -0.7607    0.7915\n",
      " -0.8022   -0.711     0.5327    0.8936    0.10864  -0.298    -0.495\n",
      "  0.886     0.1638    0.1058    0.1694   -0.4456    0.2229   -0.2678\n",
      " -1.508     0.215    -0.6743    0.1595    0.706    -0.4102   -0.4297\n",
      " -0.425    -0.4084    0.0053   -0.03052  -0.5513    0.4675   -0.7026\n",
      " -0.7944    0.006214  0.5176    0.0792    0.5864   -0.1786   -1.246\n",
      " -0.2377   -0.1128    0.9585    0.7637    0.2634    1.291    -0.633\n",
      "  1.056    -0.1731    0.1462    1.011    -1.12     -0.3193   -0.685\n",
      "  0.4082    0.252     0.2686   -0.6387   -0.2542   -0.1116    0.428\n",
      "  0.4827   -0.1985    0.953     1.144    -0.3381   -0.3982   -0.03848\n",
      " -0.274    -0.1152    0.3794    0.951    -0.3843    0.61      0.443\n",
      " -0.963    -0.747    -0.3347   -0.1763    0.2278    0.0969    0.6396\n",
      "  0.05585   0.519   ]\n",
      "query_embedding = [-0.551    0.2935   0.4      0.2786  -1.348    0.5044  -0.247    0.6416\n",
      "  0.5254  -0.5356   0.2725   0.3296   0.4822  -1.2705  -0.4424  -0.212\n",
      " -0.8726   0.6147  -0.3271   0.2893  -0.04367  0.529   -0.09204 -0.7646\n",
      "  0.563    0.1267   0.2487   0.292    0.5234  -0.291   -0.438    0.2742\n",
      " -0.1283   0.6245   0.03662 -0.03717 -0.531   -0.02058  0.4287  -0.6885\n",
      " -0.2664   0.2957  -0.2112  -0.357   -0.2119   0.0638   0.6143   0.264\n",
      "  0.314   -0.736   -0.02965 -0.6064   0.4587   0.2103  -0.5137  -0.453\n",
      "  0.12274  0.3564  -0.2073  -0.11475  0.2676   0.624   -0.418    0.0666\n",
      " -0.2573  -0.2976   0.5522   0.7188   0.3948   0.1929   0.662   -0.01741\n",
      "  0.1098  -0.128    0.02715  0.295    0.4995  -0.10583  0.03833 -0.538\n",
      "  0.1631   0.00762 -0.536   -0.0503  -0.924   -0.6323   0.05133 -0.2996\n",
      " -1.569   -0.4487   0.1558  -0.2473   0.0775  -0.1737  -0.2644  -0.3037\n",
      " -0.6387  -0.4126  -0.587    0.701  ]\n",
      "query_embedding = [ 5.0000e-01 -1.8661e-02 -7.4646e-02  9.6387e-01  6.0547e-01 -1.3232e-01\n",
      " -3.7329e-01 -7.4805e-01  5.5762e-01  2.7905e-01 -1.0293e+00 -4.4702e-01\n",
      "  7.4561e-01 -7.6782e-02 -1.7285e-01  2.2095e-01  6.2354e-01  9.9365e-02\n",
      " -1.0864e-01  8.5254e-01 -8.4375e-01  2.5940e-02 -1.4478e-01 -3.8239e-02\n",
      "  6.7529e-01  2.3254e-01  3.5083e-01 -2.7954e-01  7.4365e-01 -9.3604e-01\n",
      "  4.9658e-01  4.5459e-01 -7.8369e-02  4.8364e-01  5.8545e-01 -6.6455e-01\n",
      "  3.2861e-01  4.7656e-01 -1.3457e+00 -2.4487e-01 -3.0884e-01 -1.0747e-04\n",
      "  4.4678e-01 -1.5186e-01 -2.3376e-01 -4.5117e-01  5.8057e-01  1.0876e-01\n",
      " -2.7295e-01 -7.2461e-01  5.7281e-02  2.5415e-01  1.6998e-02  9.3555e-01\n",
      "  2.7393e-01 -1.4473e+00  4.5776e-01 -7.9004e-01  1.5469e+00 -2.6074e-01\n",
      "  1.4746e-01  5.0537e-01 -1.6895e-01  3.3081e-01  2.4072e-01 -3.1836e-01\n",
      " -3.5815e-01 -5.1384e-03  3.8184e-01  9.2041e-02 -3.8910e-02  2.0630e-01\n",
      "  3.1799e-02 -3.4009e-01  5.4785e-01  3.2227e-01 -2.7295e-01 -1.2732e-01\n",
      " -8.4912e-01 -3.4595e-01  2.9712e-01 -4.2480e-01  4.4165e-01 -3.0420e-01\n",
      " -1.0195e+00  5.9863e-01  2.0679e-01 -6.4062e-01  3.7537e-02 -5.5713e-01\n",
      " -1.3513e-01 -6.0120e-03 -4.9316e-01  9.4434e-01 -1.4514e-01  1.3025e-01\n",
      " -2.4146e-01  3.4363e-02  3.0518e-01  5.2588e-01]\n",
      "query_embedding = [-0.0652    0.59      0.6104   -0.2388    0.1571    1.192    -0.36\n",
      " -0.9883   -0.1667    0.0213   -1.013    -0.1208   -0.1971   -0.762\n",
      "  0.7095   -0.0401   -0.4724    0.4207   -0.5264    0.06537  -0.3086\n",
      "  0.2803   -0.5703   -0.1932    0.3582    0.1315    0.0881    0.497\n",
      "  0.4368   -0.887     0.295    -0.1597   -0.3801    0.415    -0.227\n",
      "  1.065     0.3743   -0.03644  -0.647     0.0379   -0.349    -0.2034\n",
      " -0.3298    0.4893    0.11743  -0.249     0.7974    0.2954   -0.1577\n",
      " -0.0986   -0.03302  -0.4277    0.0697    0.3613   -0.2646    0.526\n",
      " -0.1311   -0.3384   -0.3538   -0.7773    0.5073    0.4531    0.127\n",
      "  0.6426    0.02135  -0.6333    0.947    -0.6826    0.1757   -0.1094\n",
      " -0.589     0.003244  0.2079   -0.5635   -0.76      0.6577   -0.1614\n",
      " -0.534    -0.8804    0.1461    0.0832   -0.1625    0.1514   -0.3547\n",
      " -0.5615   -0.05023  -0.2485    0.2776   -0.3877   -0.5327    0.11523\n",
      "  0.05597  -0.2898    0.719     0.7734    0.1914   -0.12115   0.798\n",
      "  0.2239    0.3503  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.5938   0.4973   0.2566   0.1631  -0.791    0.01602 -0.666   -1.013\n",
      "  0.06494  0.1925  -0.7656   0.397   -0.05392 -0.4465   0.6973   0.0676\n",
      " -0.5454   0.09735  0.3853  -0.4355   0.05667  0.3079  -0.79     0.177\n",
      "  0.2496   0.5884   0.3777  -0.5503  -0.2356  -0.27    -0.0996   0.1444\n",
      " -0.771    0.2632   0.03314  0.2417   0.4216   0.1617  -0.09485 -0.1993\n",
      " -0.1697  -0.09064 -0.193   -0.3665   0.2198  -0.08655  0.3098   0.303\n",
      " -0.0893  -1.122   -0.05106 -0.2195  -0.1149   0.4548   0.3938  -0.5625\n",
      "  0.8003  -0.774   -0.2264  -0.2224   0.4915  -0.1545   0.641    0.273\n",
      " -0.1631   0.11743  0.0939  -1.293    0.38    -0.2783  -0.674   -0.3157\n",
      "  0.2372  -0.1757  -0.5396  -0.1718  -0.2817  -0.0697  -0.6426  -0.03543\n",
      "  0.3098  -0.3223  -0.04126  0.2476  -0.1079  -0.3538  -0.591    0.2693\n",
      " -0.6016  -0.9404  -0.712   -0.0996   0.05084  0.2219   0.562   -0.0454\n",
      "  0.2625  -0.5815  -0.0688   0.631  ]\n",
      "query_embedding = [ 0.6987   -0.2341    1.359    -0.534    -0.8813   -0.1376    0.072\n",
      " -0.02542   0.2595   -0.9883   -0.4353    0.2366    0.1193    0.01544\n",
      "  0.3542    1.076     0.1334   -0.0357   -0.5107    0.3706    0.606\n",
      "  0.752     0.003366 -1.027     0.5967    0.85      0.497     0.2341\n",
      "  0.178     1.058     0.06192   0.7397    0.3591   -0.2245   -0.2444\n",
      "  0.06052  -0.5786   -0.2382   -0.1406   -0.1554   -0.468     0.5283\n",
      " -0.757    -0.876     0.3684    0.2754   -0.0851    0.01588   0.02878\n",
      " -0.6104    0.195     0.02744   0.0197    0.4255   -0.697    -1.495\n",
      " -0.2646    1.203     0.1525   -0.36      1.11      1.057    -0.1755\n",
      " -0.1311    0.906     0.6196   -0.2015   -0.1691    0.4749    0.1245\n",
      "  0.05536  -0.11383  -0.875     0.6606   -0.307     0.1621    0.312\n",
      " -0.075    -0.7275    0.1058    0.4814   -0.177    -0.2996    0.3481\n",
      " -1.267    -0.1227   -0.1869    0.2637   -0.7197    0.1746   -0.3645\n",
      "  0.04395  -0.1674    0.1771   -0.09064  -0.6943   -0.1594   -0.494\n",
      " -0.07904  -0.606   ]\n",
      "query_embedding = [ 0.306    0.5776   0.714   -0.874   -0.4973   1.26    -1.343   -0.4404\n",
      "  0.39    -0.184   -0.02434  0.544   -0.11584  0.2595  -0.1625   0.07043\n",
      "  0.2507   0.5825  -0.9805   0.1732   0.02791  0.5146  -0.906   -0.54\n",
      "  0.2448   0.739    0.1812   0.3662   1.047   -0.4355   0.6177   0.06198\n",
      " -0.08997  0.2688  -0.532    0.2144   0.5317   0.0689  -0.2507  -0.1913\n",
      " -0.1519   0.2522   0.1135  -0.5034  -0.08    -0.08514  0.987    0.6377\n",
      " -0.688   -0.758   -0.1285  -0.3306  -0.1604   0.7085  -0.43    -1.2\n",
      "  0.0405   0.4167   0.2844  -0.0719   0.06946  0.2025  -0.6094  -0.197\n",
      "  0.3745   0.4482   0.796   -0.1348  -0.03032 -0.2123   0.2832   0.2354\n",
      " -0.2151   0.2374   0.3562   1.199    0.2351  -0.1649  -0.4822   0.6504\n",
      " -0.2069  -0.0685   0.1165   0.3506  -1.055   -0.2686  -0.6875  -0.01877\n",
      " -0.2507  -0.3623  -0.2186  -0.333    0.1488   0.5015   0.557   -0.2751\n",
      "  0.2335   0.11383 -0.0409   0.4875 ]\n",
      "query_embedding = [-0.1735   -0.7227    0.2502    0.3013   -0.4631    0.02011   0.6826\n",
      " -0.1582   -0.2068   -0.2817    0.2065   -0.3018    0.1942    0.5796\n",
      " -0.2256   -0.7573    0.3179    0.5244   -0.292     0.379    -0.1326\n",
      " -0.1127    0.335     0.10583   0.01973   1.033     0.693    -0.563\n",
      "  0.01645  -0.548    -0.04755  -0.2568   -0.02687   0.298    -0.3538\n",
      "  0.3206   -0.2727    0.4685    0.5938   -0.2394    0.3403   -0.11804\n",
      " -0.4316   -0.1688   -0.4727    0.08453  -0.04407  -0.006203  0.2057\n",
      "  0.19     -0.07007  -0.48      0.1345    0.001672 -0.01724  -0.6284\n",
      "  0.2698   -0.1772    0.236     0.508     0.72     -0.1982   -0.2532\n",
      " -0.7524    0.1731   -0.1871    0.5337   -0.7036   -0.2031    0.3208\n",
      " -0.3015   -0.09607  -0.11584  -0.1879   -0.0859    0.6714    0.8076\n",
      " -0.3525   -0.2896    0.374    -0.1772    0.459     0.2013    0.4207\n",
      "  0.193     0.575     0.2861   -0.4407   -0.1215   -0.5728   -0.7476\n",
      " -0.1323   -0.06665   0.5024   -0.1882   -0.428    -0.1148   -0.651\n",
      " -0.4836    0.2435  ]\n",
      "query_embedding = [ 1.6882e-01 -2.2400e-01 -2.4597e-01 -2.5955e-02 -1.9852e-02  1.3506e+00\n",
      "  2.9932e-01 -5.8838e-01 -1.4343e-01  4.4409e-01 -2.6587e-01  4.0869e-01\n",
      "  6.6895e-02 -3.4570e-01  2.2986e-01  4.1504e-01 -1.0712e-01 -1.8234e-02\n",
      " -1.1298e-01  6.4941e-02 -1.1462e-01  2.9199e-01 -4.7070e-01  5.4626e-02\n",
      " -1.2939e-02  4.1382e-01  3.9124e-02  1.6516e-01 -6.0693e-01 -1.1123e+00\n",
      "  2.5879e-01 -3.8965e-01  3.0884e-01  1.8555e-01 -4.2267e-02 -1.6833e-01\n",
      "  1.1908e-01 -2.2839e-01 -2.9492e-01  1.1070e-02  1.6382e-01 -1.7432e-01\n",
      " -8.7646e-02  5.8984e-01 -1.5747e-01 -3.4888e-01  4.4409e-01  1.7914e-02\n",
      " -1.5222e-01 -6.3672e-01 -9.4666e-02  2.8662e-01 -2.5903e-01 -1.1481e-01\n",
      "  1.4458e-02  7.2412e-01  5.2393e-01  8.8978e-04 -3.7598e-01 -7.4756e-01\n",
      " -1.9058e-02 -3.3252e-01  5.9668e-01  2.5903e-01  3.4961e-01  1.2695e-01\n",
      "  9.6283e-03 -5.1025e-01  3.9551e-01 -4.2285e-01 -1.0266e-01 -1.4661e-01\n",
      " -1.6565e-01  2.9150e-01 -2.9443e-01  6.1475e-01 -3.4204e-01  2.6562e-01\n",
      " -4.1895e-01  5.1709e-01 -1.5625e-01  3.0493e-01  4.2407e-01  1.0186e+00\n",
      " -4.1553e-01 -8.2474e-03 -8.1104e-01  7.7637e-02  6.3135e-01 -2.5806e-01\n",
      "  1.8768e-02  1.0437e-01  4.5483e-01  2.3523e-01  8.8428e-01 -3.4692e-01\n",
      " -1.4795e-01 -7.4768e-02 -5.0928e-01  6.4502e-01]\n",
      "query_embedding = [ 0.615   -0.4116   0.5786  -0.0366   0.263    1.32    -0.702   -0.03705\n",
      " -0.3718   0.04218 -0.794    0.6953  -0.2058  -0.4272  -0.5127  -0.03824\n",
      "  0.024    0.3584  -0.7583   0.2139  -0.1013  -0.4624  -0.2295  -0.663\n",
      "  0.989    0.462    0.4653  -0.4255   0.361   -0.3982   1.366   -0.08344\n",
      " -0.9854  -0.2695  -0.08026  0.9497   0.4448  -0.68    -0.2402   0.832\n",
      " -1.014   -0.2023   0.02216 -0.4312   0.03888 -0.2502   0.593   -0.4219\n",
      " -0.2458  -0.5444   1.081   -0.705    0.5107   1.09    -0.4287   0.2404\n",
      "  0.6235  -0.404    0.873   -0.628   -0.11017  0.1366  -0.498    0.0171\n",
      "  0.3833  -0.2      1.128    0.2074  -0.372    0.4849  -0.2502   0.4414\n",
      " -0.2483  -0.321   -0.1938   1.115   -0.2693  -0.446   -0.821    0.0643\n",
      " -0.1332   0.1915   0.0851   0.6206  -0.3928   0.1259  -0.1388   0.2129\n",
      "  0.4758  -0.3057  -0.2094  -0.5596  -0.2866   0.3474   1.094    0.5435\n",
      " -0.0362  -0.4775   0.4805   0.1992 ]\n",
      "query_embedding = [ 4.0967e-01  5.8929e-02  8.1982e-01 -1.4673e-01 -4.5898e-01  3.2129e-01\n",
      "  3.6297e-03 -1.5466e-01 -1.4392e-01 -4.9731e-01 -2.9858e-01  5.2783e-01\n",
      " -1.5662e-01 -5.9473e-01  9.0479e-01  3.7872e-02 -5.2002e-01  2.3132e-01\n",
      "  5.1660e-01 -8.4180e-01  1.5967e-01  9.2173e-04 -3.4790e-01 -1.5771e-01\n",
      " -3.9600e-01  4.9731e-01  5.5566e-01  4.7266e-01  3.5864e-01 -9.2676e-01\n",
      " -4.8370e-02  2.8931e-02 -8.5498e-01  5.6396e-01 -1.8921e-01 -2.5415e-01\n",
      "  4.6899e-01  2.6016e-02 -3.4888e-01 -5.9912e-01 -2.9678e-02 -3.7915e-01\n",
      " -2.3636e-02 -3.2935e-01  3.5400e-01 -4.1309e-01  1.0938e+00  1.2427e-01\n",
      "  4.7217e-01 -8.2422e-01 -4.5923e-01 -1.2732e-01  4.0771e-01  1.1182e-01\n",
      "  5.4346e-01 -1.0469e+00  9.5337e-02  5.9521e-01 -7.7820e-02 -1.6003e-01\n",
      "  1.7114e-01  5.8301e-01  2.1228e-01  1.2335e-01 -1.2006e-01  5.4346e-01\n",
      "  3.3276e-01 -1.2432e+00  3.0322e-01 -6.6553e-01  8.3740e-02  5.5511e-02\n",
      " -7.0361e-01 -2.1411e-01 -3.3447e-01  1.0703e+00  5.3864e-02  6.4697e-03\n",
      " -7.1729e-01 -2.8516e-01  7.2571e-02 -3.3691e-01 -1.5186e-01  6.1426e-01\n",
      " -1.6785e-01 -6.9092e-02 -2.4170e-01 -2.8418e-01 -4.8682e-01 -6.3086e-01\n",
      "  1.1911e-03  1.1884e-01 -8.3838e-01  7.1094e-01  1.2244e-01 -1.7725e-01\n",
      "  7.1777e-01  1.5173e-01 -3.6255e-01  4.3652e-01]\n",
      "query_embedding = [ 0.3184   0.11646  0.05933 -0.8193  -0.946    1.549   -0.7085  -0.02975\n",
      "  0.6587  -0.3984  -0.4753  -0.1791   0.08954 -0.4722  -0.1965   0.116\n",
      " -1.161   -0.4036  -0.2037   0.3064   1.252   -0.07104  0.1809  -0.09894\n",
      "  0.633   -0.2072  -0.3606   0.85     0.855    0.505    0.337    0.6206\n",
      "  0.3818  -0.0724  -0.2898  -0.422    0.1428  -0.1533   0.551    0.2075\n",
      " -0.917   -0.5      0.3176  -0.403    0.3328  -0.379   -0.11584  0.893\n",
      "  0.4927  -0.2173   0.4883  -0.552    0.4192   0.512   -0.3323  -1.22\n",
      "  0.307    0.1565   0.726   -0.2122   0.4866   0.5205  -0.2686  -0.2898\n",
      "  0.6816  -0.3574   0.864    0.3542   0.1918  -0.3252  -0.0383  -0.2129\n",
      " -0.62    -0.7847  -0.2578   1.111    0.7397   0.10876 -0.8726  -0.0843\n",
      "  1.127   -0.4248  -0.5645  -0.08466 -1.342   -0.6216  -0.492    0.2339\n",
      " -0.604   -0.694    0.581   -0.2927  -0.5635   0.1719  -0.3188   0.1174\n",
      " -0.7173   0.9683  -0.06555  0.2827 ]\n",
      "query_embedding = [ 0.056    -0.3254   -0.5005   -0.3992   -0.688     0.9194   -0.3267\n",
      "  0.2328    0.2627   -0.3293   -0.17     -0.05515  -0.3591   -0.55\n",
      " -0.393    -0.012245 -1.238    -0.08624  -0.1409    0.3708    0.742\n",
      "  0.296     0.0493   -0.1359    0.571     0.2462   -0.4587    0.9985\n",
      "  0.632    -0.04904  -0.2844    0.317     0.03647  -0.08307  -0.0596\n",
      " -0.0568   -0.005924 -0.3252    0.48      0.342    -0.68     -0.1097\n",
      "  0.2368   -0.277     0.4995   -0.344     0.1581    0.4998    1.061\n",
      " -0.2449    0.5728   -0.3423    0.2133    0.451     0.1797   -1.127\n",
      "  0.04764   0.08795  -0.03656  -0.06525   0.2407    0.0663   -0.1458\n",
      " -0.01458   0.4585   -0.2317    0.2008    0.605    -0.003723 -0.1543\n",
      "  0.0941    0.0471   -0.5415   -0.1989   -0.732     0.6675   -0.02498\n",
      " -0.1787   -0.879    -0.1925    0.833    -0.1271   -0.3372    0.2769\n",
      " -0.59     -0.499    -0.5415    0.3572   -0.3455   -0.79      0.2365\n",
      "  0.2847   -0.786     0.3242    0.1748   -0.2593   -0.2688    0.4082\n",
      "  0.088     0.1074  ]\n",
      "query_embedding = [ 0.03488   0.4607    0.0684   -0.02898   0.165    -0.386    -0.511\n",
      " -0.11774   0.1462   -0.8013   -0.531     0.4165    0.0884   -0.3315\n",
      " -0.2747   -0.631     0.07794   0.3079   -0.643     0.429    -0.1758\n",
      "  0.0793   -0.3752   -0.4148   -0.05374   0.5254    0.02846  -0.4158\n",
      "  0.5654   -0.535    -0.4446    0.3228   -0.02986   0.3372   -0.01744\n",
      " -0.2185    0.063     0.1567    0.1141   -0.2786   -0.8486   -0.3013\n",
      "  0.3083   -0.6636   -0.1371   -0.446     0.262    -0.05334   0.02173\n",
      " -0.9517   -0.11115  -0.329     0.0724    0.9883    0.4082   -2.635\n",
      " -0.01332  -0.2432    0.8613   -0.0733   -0.4346    0.696    -0.4995\n",
      " -0.2063    0.1611    0.5024   -0.01152   0.897    -0.4536   -0.871\n",
      "  0.805    -0.12354  -0.544    -0.9136    0.4692    0.6885    0.199\n",
      " -0.1765   -0.9766   -0.188     0.318     0.403    -0.357     0.1891\n",
      " -1.253     0.2217   -0.0659    0.2957   -0.1396    0.008125 -0.0811\n",
      " -0.06586  -0.05566   0.1996   -0.857    -0.0279    0.1218   -0.279\n",
      "  0.1727    0.5107  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.508     0.178    -0.02151  -0.4321    0.0788    0.792    -0.1808\n",
      " -0.054     0.0792   -0.999    -0.2312    0.0859   -0.371    -0.2717\n",
      "  0.1724   -0.883    -0.3003    0.6255   -0.781     0.5186   -0.1307\n",
      "  0.1287    0.07745  -0.4006    0.3608    1.306     0.4973    0.1886\n",
      "  0.873    -0.3328   -0.3462   -0.194    -0.3938    0.1996   -0.3242\n",
      " -0.3562    0.4436    1.045     0.2351   -0.4797    0.06287   0.10504\n",
      " -0.1311   -0.0883    0.0879    0.2825   -0.1222   -0.237     0.4512\n",
      " -0.3428    0.01926  -0.2273    0.3572    0.5283    0.3467   -1.851\n",
      " -0.1721    0.4875    0.1294   -0.5977    0.823     0.5605   -0.2205\n",
      " -0.1986   -0.3872    0.1447    0.882    -0.499    -0.1342   -0.5093\n",
      "  0.38      0.3154   -0.3394   -0.402     0.851     1.84      0.11566\n",
      " -0.466    -0.5625   -0.03665  -0.2576    0.06903   0.599     0.3638\n",
      " -0.731    -0.1741   -0.08624  -0.5967   -0.345    -0.3572    0.0684\n",
      " -0.3806   -0.0866    0.9614   -0.3308   -0.326     0.001855 -0.4653\n",
      "  0.0346    0.693   ]\n",
      "query_embedding = [ 0.1251    0.241     0.388    -0.4353   -0.7065    1.341    -0.733\n",
      " -0.4995    0.239     0.0792   -0.591     0.3896   -0.038    -0.2031\n",
      "  0.4875    0.4175   -0.4524    0.698    -0.3213   -0.1268   -0.277\n",
      "  0.672    -0.3518   -0.5615    0.4155    0.3882    0.765     0.5225\n",
      "  0.9546   -0.512     0.4263    0.387    -0.438     0.2484   -0.4888\n",
      "  0.588     0.5044   -0.1252   -0.242    -0.1411   -0.3403   -0.2917\n",
      " -0.001363 -0.1598   -0.3738   -0.02968   0.6445    0.3374   -0.3555\n",
      " -0.668    -0.01588  -0.11786   0.2357    0.9146   -0.2256   -0.557\n",
      "  0.4504   -0.01485  -0.0267   -0.1492    0.303     0.251     0.5127\n",
      "  0.1932   -0.44      0.1742    0.728    -0.7437   -0.01945   0.2795\n",
      " -0.1671    0.39     -0.355    -0.5303   -0.337     1.106    -0.10376\n",
      "  0.10516  -0.437     0.4966   -0.1924    0.3142    0.2583    0.1893\n",
      " -0.2922   -0.823    -0.3457    0.3276   -0.572    -0.54     -0.148\n",
      "  0.01108  -0.655     1.109     0.7686    0.176    -0.0359   -0.02278\n",
      "  0.342     0.2333  ]\n",
      "query_embedding = [ 0.3694    0.2659    0.5596   -0.02826  -0.832     0.5107    0.1764\n",
      " -0.2664   -0.1128    0.004486 -0.2542    0.5215    0.001541 -0.7993\n",
      "  0.3286   -0.801    -0.8       0.4033    0.2847   -0.2642   -0.11426\n",
      "  0.2468   -0.198    -0.5576    0.2134    0.6924    0.3271    0.5635\n",
      "  0.4185   -1.132    -0.1484   -0.427    -0.601     0.3508   -0.5845\n",
      "  0.3208    0.565     0.3386    0.01001  -0.4863   -0.1021   -0.1025\n",
      "  0.08124  -0.2208    0.5273   -0.06665   0.5703    0.1523    0.5215\n",
      " -0.1505   -0.2081    0.1771    0.5845    0.4023    0.2004   -0.2314\n",
      "  0.33      0.0334   -0.2356    0.1769    0.1537    0.08356   0.394\n",
      "  0.2637   -0.768     0.2455    0.3977   -0.5356   -0.2844   -0.4216\n",
      " -0.401     0.01582  -0.2954   -0.4263    0.2537    1.254     0.05978\n",
      " -0.3735   -0.3218   -0.05856  -0.5757   -0.02206   0.211     0.683\n",
      " -0.3171   -0.1127    0.27     -0.3086   -0.9136   -0.8228   -0.1516\n",
      "  0.2167   -0.4998    0.562     0.4944   -0.02405   0.1932   -0.0849\n",
      " -0.1699    0.5596  ]\n",
      "query_embedding = [ 4.5166e-01  4.7412e-01  3.2153e-01 -1.2360e-01 -1.4136e-01  1.4763e-02\n",
      " -2.0679e-01 -6.5918e-01 -2.6782e-01 -6.2347e-02 -1.0127e+00  1.7212e-01\n",
      " -1.9666e-01 -1.2656e+00 -3.0298e-01 -3.4326e-01 -6.1084e-01  8.6426e-01\n",
      "  2.3718e-01 -4.7699e-02 -2.9736e-01 -3.0350e-02 -6.0791e-01 -8.5596e-01\n",
      " -1.1304e-01  3.6011e-01  3.0713e-01  1.9824e-01  1.0553e-01 -5.5518e-01\n",
      " -2.5610e-01  4.9683e-01 -3.2153e-01  2.0105e-01 -3.1177e-01 -4.5044e-02\n",
      "  2.5806e-01  1.2598e-01 -2.4048e-01 -1.0834e-01  1.8494e-02 -6.6162e-01\n",
      "  2.7985e-02 -1.1473e-03  6.8848e-02 -6.8750e-01  5.5713e-01 -4.0649e-01\n",
      "  8.1934e-01 -1.3936e+00 -1.3489e-01 -1.5900e-02  8.9417e-02  4.7046e-01\n",
      "  3.4644e-01 -1.4365e+00  2.1423e-01 -1.0791e-01  1.1780e-01 -4.8169e-01\n",
      "  4.3188e-01  6.0059e-01 -7.3486e-02  2.1582e-01  4.6631e-02  3.0347e-01\n",
      "  6.1865e-01 -6.9727e-01  8.1787e-01 -2.1802e-01  7.2852e-01 -5.3955e-02\n",
      " -2.6294e-01 -7.6318e-01 -1.1322e-01  1.0547e+00  7.6904e-02 -2.1973e-01\n",
      " -7.8027e-01 -2.1948e-01  2.2046e-01  3.0200e-01  1.9791e-02 -3.6426e-01\n",
      " -7.6904e-01 -6.0352e-01 -2.6392e-01  3.5352e-01 -4.5264e-01 -4.8926e-01\n",
      " -1.7236e-01 -1.4282e-01 -2.6831e-01  6.5576e-01 -5.3467e-01 -2.3950e-01\n",
      " -3.1470e-01 -4.1553e-01  3.1311e-02  8.1299e-01]\n",
      "query_embedding = [ 0.1492   0.3652   0.431    0.297   -0.1501   0.03973  0.2047   0.4993\n",
      " -0.2256   0.0961   0.3403   0.1542  -0.6387  -1.084    0.1207   0.596\n",
      " -0.4226   0.4397   0.1282   0.6636   0.535    0.0815  -0.2932  -0.3684\n",
      "  0.685    0.0987   0.2432   0.397   -0.1439  -1.096   -0.2128  -0.315\n",
      " -0.503   -0.3044  -0.2878  -0.1232  -0.0818   0.1328  -0.3333  -0.0635\n",
      "  0.168    0.7524  -0.1682  -0.12085  0.11707  0.1365   1.127   -0.27\n",
      " -0.01317 -0.319    0.1831  -0.1971   0.602    0.4424   0.01872 -1.182\n",
      "  0.234    0.1295   0.01894  0.02744 -0.293    0.295   -0.4072  -0.2195\n",
      "  0.1428  -0.3315   0.3762  -0.1813   0.3103  -0.01883  0.2391   0.3523\n",
      " -0.0644   0.1508   0.4077   0.1718  -0.3826  -0.391   -0.1285  -0.0545\n",
      "  0.462    0.545    0.4287  -0.1002  -0.5454  -0.52     0.6816  -0.2123\n",
      " -0.575   -0.0713   0.2435   0.4016  -0.0808   0.11926 -0.4072  -0.7134\n",
      " -0.7227  -1.08    -0.02348  0.5225 ]\n",
      "query_embedding = [-0.0353    0.3635    0.1048    0.2194   -0.635     0.4978   -0.652\n",
      " -0.7764    0.3713   -0.408    -0.623    -0.0935    0.10767  -0.5947\n",
      "  0.5615   -0.006134  0.1879    0.1201    0.06647   0.7354   -0.03833\n",
      "  0.6035   -0.267    -0.336    -0.1526    0.02847   0.532    -0.1495\n",
      "  0.7446   -0.6147    0.4648    0.824     0.0743   -0.1924   -0.1598\n",
      "  0.709     0.641     0.09924   0.3792    0.0913   -0.1785    0.0213\n",
      "  0.3438   -0.3215   -0.439    -0.1836    0.3303    0.023     0.2754\n",
      " -0.818     0.4883    0.2184    0.09454   0.4658    0.1692   -0.416\n",
      "  1.009    -0.3633   -0.346    -0.6704    0.1082    0.1461    0.4937\n",
      "  0.206     0.3394    0.007286  0.4526   -0.1799   -0.10077  -0.0703\n",
      " -0.4114   -0.2834    0.02641  -0.2115   -1.109     0.5903   -0.1556\n",
      " -0.4517   -0.257     0.0486   -0.08527   0.007576 -0.3162    0.3132\n",
      " -0.723    -0.09174  -0.6143    0.5454   -0.615    -0.6084   -0.0849\n",
      " -0.1029   -0.3386    0.075    -0.01776  -0.2783   -0.8276   -0.255\n",
      "  0.1335    0.4631  ]\n",
      "query_embedding = [-1.962e-01  3.347e-01  4.839e-01 -3.767e-01 -1.544e+00  1.088e+00\n",
      " -7.905e-01  4.519e-01  7.383e-01  1.865e-01  2.150e-01  9.116e-01\n",
      "  2.360e-01 -5.966e-02 -3.123e-01 -1.436e-01 -7.574e-02  4.282e-01\n",
      " -2.637e-01  6.523e-01 -6.061e-02  7.568e-01 -2.274e-01 -4.214e-01\n",
      " -4.548e-01  2.366e-01  7.660e-02  8.282e-02  1.334e+00  3.408e-01\n",
      "  4.109e-01  3.252e-01  1.680e-01 -4.807e-02 -8.301e-02  1.847e-01\n",
      " -2.908e-01  2.512e-01  7.251e-02 -4.436e-01 -4.949e-01  1.333e-01\n",
      "  1.057e-01 -3.193e-01 -7.393e-01  2.659e-01  1.186e+00  5.303e-01\n",
      " -2.338e-01 -1.153e+00 -2.757e-02 -1.714e-01  3.655e-01  2.389e-01\n",
      " -9.604e-01  2.664e-02  1.921e-01  8.600e-02 -8.978e-02 -3.662e-01\n",
      "  1.925e-01 -3.311e-02 -5.112e-01 -3.921e-01  5.441e-02 -4.565e-01\n",
      "  1.028e+00  8.911e-01 -3.021e-02 -3.599e-01  2.313e-01 -4.456e-01\n",
      " -1.635e-01  1.020e-01  1.219e-02  4.719e-01  2.581e-01  5.425e-01\n",
      " -2.472e-01  1.350e-03 -9.971e-01 -9.418e-02 -2.017e-01  9.663e-01\n",
      " -9.165e-01 -4.307e-01 -3.718e-01  3.687e-01 -8.340e-01 -5.303e-01\n",
      "  5.823e-02 -4.695e-01  3.560e-01  8.507e-03  9.564e-02 -1.497e-01\n",
      " -2.020e-01 -2.246e-01 -4.226e-01  5.164e-02]\n",
      "query_embedding = [-3.3862e-01  2.5146e-01  8.2324e-01  2.3254e-01 -2.6978e-01  5.8887e-01\n",
      " -1.5393e-01  4.9658e-01 -3.2104e-01  1.0853e-03  1.2537e-01 -3.6719e-01\n",
      " -1.8945e-01 -2.0142e-01  4.0234e-01 -6.6357e-01 -6.5088e-01  1.5894e-01\n",
      "  5.8301e-01  1.9302e-02  3.4790e-01  1.0967e+00  3.9551e-01 -7.9639e-01\n",
      " -2.0154e-01 -5.0684e-01 -1.8457e-01 -8.0908e-01  1.0433e-03 -3.4180e-01\n",
      " -3.1519e-01  3.0176e-01 -1.9421e-01 -4.5630e-01  1.1641e+00  7.1436e-01\n",
      " -4.6240e-01  5.9912e-01  1.1017e-01 -1.3989e-01  3.3521e-01 -4.2236e-01\n",
      "  8.6963e-01 -3.4814e-01 -3.3862e-01 -4.8169e-01  4.6265e-01  4.3188e-01\n",
      "  4.1162e-01 -1.0791e+00 -4.3060e-02 -6.4062e-01  8.4839e-03  7.0312e-01\n",
      "  2.5171e-01 -2.2988e+00  8.4521e-01  3.8379e-01  7.9785e-01 -1.5710e-01\n",
      " -4.7314e-01 -1.1072e-01 -4.6973e-01  1.9849e-01  2.4194e-01  2.8564e-01\n",
      "  1.0010e+00  1.9373e-01 -5.4248e-01 -1.5002e-01  8.2764e-02 -3.4271e-02\n",
      "  2.4878e-01 -6.2256e-01  1.4319e-01  3.3691e-01 -6.4941e-01 -5.2734e-01\n",
      "  3.6450e-01 -4.0283e-01 -2.3352e-01  3.3765e-01 -5.5420e-01  2.9688e-01\n",
      " -5.3467e-01 -2.2803e-01  1.0370e-01  1.3257e-01 -1.3721e+00 -3.2568e-01\n",
      "  4.4556e-01 -4.6313e-01  1.1621e+00 -5.2588e-01 -4.6753e-01  1.9055e-01\n",
      " -4.8877e-01  1.3657e-02  4.5044e-01  2.6221e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.572   -0.1411   0.301    0.1075   0.3154   0.371   -0.761   -0.547\n",
      "  0.1487   0.1587  -0.06085 -0.2974   0.825    0.05948  0.3481   0.02576\n",
      "  0.2439   0.4224   0.05432  0.231   -0.1438  -0.2219  -0.543   -0.121\n",
      "  0.3145   0.449    0.01657  0.3564   0.0517  -0.689   -0.0588  -0.6553\n",
      "  0.23    -0.1812  -0.1914  -0.10004  0.2074   0.04977 -0.4104   0.3616\n",
      "  0.246    0.5537  -0.00846 -0.3528  -0.03122 -0.2883   0.2788   0.2898\n",
      "  0.3875   0.03946  0.6826   0.1558  -0.0829   0.02533 -0.09045  0.1617\n",
      "  0.8887  -0.4546  -0.093   -0.597   -0.1771  -0.09534  0.1236  -0.3225\n",
      " -0.5405   0.0525   0.4639  -0.596    0.1431  -1.019    0.2152  -0.733\n",
      "  0.233    0.1761   0.245    0.568    0.2747  -0.572    0.2307  -0.3066\n",
      " -0.4285   0.2108   0.04138  0.6055   0.4531  -0.198    0.3564  -0.3923\n",
      "  0.745   -0.5176  -0.5493   0.3682  -0.1343   0.564    0.262   -0.8726\n",
      " -0.02629 -0.4187  -0.3586   0.1227 ]\n",
      "query_embedding = [ 0.462    0.3176   0.4058  -0.1084  -0.668    1.189   -0.1711   0.49\n",
      " -0.00401 -0.273    0.1588  -0.2766   0.1155  -1.144   -0.00368 -0.2595\n",
      " -0.4258   0.2725   0.489    0.2942   0.4548   0.56    -0.2477  -0.2742\n",
      "  0.4734   0.559    0.3171   0.877    0.822   -0.2913  -0.583   -0.2373\n",
      " -0.1349   0.635   -0.152   -0.3455   0.075    0.7114  -0.0649  -0.372\n",
      " -0.1004   0.2036   0.06006 -0.2612  -0.1068  -0.444    0.1267   0.2167\n",
      "  0.416   -0.2211   0.4     -0.5137   0.7046  -0.1538  -0.256   -0.1823\n",
      "  0.4836   0.2272  -0.0656  -0.038    0.5405   0.1663  -0.4966  -0.08527\n",
      " -0.3398  -0.3687   0.55     0.3184  -0.1959  -0.647    0.1901  -0.10315\n",
      "  0.10944 -0.7886  -0.411    0.7583   0.1603  -0.508   -0.1578  -0.7954\n",
      "  0.04654  0.1233  -0.07574  0.4548  -0.3586  -0.3645   0.0821  -0.186\n",
      " -0.921   -0.6343   0.2     -0.3252  -0.3105   0.4448  -0.08997 -0.4216\n",
      " -0.606    0.3228  -0.2351   0.9277 ]\n",
      "query_embedding = [ 0.2783    0.4102    0.6494    0.2058    0.2935    0.1288   -0.1394\n",
      " -0.3645    0.1532   -0.1605   -0.853     0.788     0.5376   -0.1582\n",
      " -0.181    -0.1451   -0.11743   0.0497    0.1685    0.3928   -0.3242\n",
      " -0.1569   -0.128    -0.6274    0.3877    0.36      0.0738   -0.3933\n",
      " -0.2668   -1.122     0.0633   -0.299    -0.4338    0.02557   0.2375\n",
      "  0.5503    0.3042    0.336    -0.05353  -0.5444    0.1321   -0.1451\n",
      " -0.0678   -0.4138   -0.348    -0.3945    0.487     0.2167   -0.008514\n",
      " -0.02057  -0.1855   -0.3806    0.1666    0.602     0.167     0.1718\n",
      "  0.5405   -0.3406   -0.6504   -0.1427   -0.01825   0.1372   -0.5186\n",
      " -0.7607   -0.09064   0.578     0.843    -0.7905    0.421    -0.1365\n",
      "  0.1652    0.01994   0.2742   -1.062     0.3994    1.107     0.25\n",
      " -0.456     0.0938   -0.3062   -0.6465    1.046     0.3447    0.49\n",
      "  0.2568    0.2219    0.5293   -0.108    -0.7734   -0.235    -0.4949\n",
      " -0.0855   -0.1326    0.362     0.3337    0.2534    0.2338   -0.266\n",
      "  0.03198  -0.1023  ]\n",
      "query_embedding = [ 0.3154   0.6733   0.4294  -0.3232  -1.085    0.612   -0.1209   0.1702\n",
      "  0.3271  -0.9434  -0.2417   0.5747   0.707   -0.0826   0.4705  -0.524\n",
      " -0.3174  -0.4055  -0.02411  0.6357   0.2446   0.7686   0.01755 -0.1543\n",
      " -0.1431   0.3833  -0.501   -0.3027   0.3262   0.976    0.2515   0.4739\n",
      "  0.464   -0.305   -0.5146   0.4604  -0.729    0.2163   0.0992  -0.1561\n",
      " -0.348    0.2183  -0.0541  -0.8496   0.3801   0.5566  -0.521    0.4832\n",
      " -0.1058  -0.607    0.3064  -0.693   -0.1022   0.3645  -0.5483  -0.882\n",
      "  0.344   -0.2181   1.049    0.1343   0.7563   0.974   -0.2507  -0.668\n",
      "  0.02473  0.623    0.3193  -0.1665   0.1891  -0.5557  -0.272   -0.6074\n",
      " -0.1378   0.5356   0.756    0.8647  -0.2676   0.3508  -0.408   -0.1362\n",
      "  0.3086   0.3503  -0.2996   0.5747  -2.139   -0.295   -0.0335   0.51\n",
      " -1.187    0.02914  0.0738  -0.502    1.056   -0.05762 -0.725   -0.03516\n",
      "  0.1534   0.1885  -0.2362  -0.699  ]\n",
      "query_embedding = [ 2.6733e-01 -3.1348e-01  5.1123e-01  1.1261e-01 -9.1064e-01  1.3782e-01\n",
      " -5.5566e-01 -1.1481e-01  8.6377e-01 -4.7852e-01 -1.7395e-01  5.9033e-01\n",
      "  3.8477e-01 -1.0713e+00  5.1221e-01 -6.4014e-01 -3.8794e-01  3.3112e-02\n",
      "  7.8369e-01 -5.8441e-02  2.8076e-01  7.1436e-01  1.8311e-02 -1.5173e-01\n",
      " -1.9507e-01  2.5342e-01  7.9004e-01  2.9834e-01  3.1494e-01  4.9341e-01\n",
      " -8.5059e-01 -2.8564e-01 -4.4159e-02 -3.4888e-01 -3.4351e-03  4.2920e-01\n",
      " -2.2793e-03  3.3350e-01  4.1089e-01 -3.8501e-01 -6.0889e-01  2.7954e-01\n",
      "  2.8564e-01 -1.0262e-03 -8.6365e-02 -4.4849e-01  9.4434e-01  3.7231e-01\n",
      "  5.5273e-01 -4.0381e-01  4.7607e-02 -6.2207e-01  3.4595e-01  1.9104e-01\n",
      " -3.8745e-01  1.2915e-01  6.3281e-01 -2.8711e-01 -7.2510e-01 -1.0712e-01\n",
      "  1.0175e-01  7.4902e-01 -4.7583e-01  7.5537e-01 -2.3840e-01 -5.6396e-01\n",
      "  3.8452e-01  4.5117e-01  2.2278e-01 -8.1299e-02  3.8037e-01 -1.0176e+00\n",
      " -4.6448e-02 -3.2764e-01 -6.0400e-01  7.2852e-01 -2.8247e-01 -1.2093e-02\n",
      " -2.9077e-01 -5.5371e-01 -8.6487e-02  6.7978e-03 -4.4214e-01  6.3867e-01\n",
      " -6.5967e-01 -1.7834e-01 -2.9648e-02  8.4656e-02 -5.2539e-01 -6.0840e-01\n",
      "  1.5112e-01 -1.3123e-01 -1.2805e-01 -2.5366e-01 -3.5083e-01  4.3640e-02\n",
      " -7.4768e-02  2.6642e-02 -5.5322e-01  3.4570e-01]\n",
      "query_embedding = [-6.7627e-01  6.5576e-01 -2.2888e-01  1.3257e-01 -7.6367e-01  3.5205e-01\n",
      " -1.1631e+00 -4.1479e-01 -2.2247e-02  7.9773e-02 -3.5034e-02 -9.3079e-03\n",
      "  1.5759e-01 -3.2788e-01  1.9153e-01  7.4951e-01 -3.2812e-01  6.3135e-01\n",
      " -2.3694e-01  4.1919e-01  1.0864e-01  3.2910e-01  8.5205e-02 -4.0991e-01\n",
      " -3.8110e-01 -6.7139e-02  3.0933e-01 -1.7236e-01  3.7646e-01 -9.9023e-01\n",
      "  2.3267e-01  4.2212e-01 -2.8735e-01  1.7432e-01  3.6530e-02  1.2164e-01\n",
      " -2.1216e-01  3.6987e-02  3.5767e-01 -6.4795e-01 -5.7007e-02  7.1045e-01\n",
      "  2.1863e-01 -5.0684e-01 -7.6025e-01  7.6172e-02  8.0078e-01  2.9932e-01\n",
      " -3.1177e-01 -2.0178e-01  3.3472e-01  8.2275e-01 -2.5146e-01 -2.2290e-01\n",
      " -2.8418e-01  6.5527e-01  1.0859e+00 -7.5867e-02 -6.9678e-01  7.4280e-02\n",
      " -5.1416e-01 -5.1483e-02 -3.0835e-01 -3.3203e-01 -7.2632e-02  2.3328e-01\n",
      "  9.4531e-01  2.0886e-01 -2.3010e-01 -6.9580e-01  1.6675e-01 -7.7979e-01\n",
      "  7.2815e-02  3.4131e-01 -8.2324e-01 -6.2256e-01 -1.6992e-01 -1.5845e-01\n",
      " -6.2842e-01 -2.6196e-01  1.6370e-01 -2.1216e-01 -6.2500e-02  7.8760e-01\n",
      "  2.8101e-01 -2.4329e-01 -4.3297e-03 -6.8970e-02 -6.7383e-01  3.3855e-05\n",
      " -1.6772e-01 -3.2837e-01 -1.0590e-01 -1.8591e-01 -3.4448e-01 -9.2102e-02\n",
      " -5.4901e-02 -5.2917e-02  3.3020e-02  2.9614e-01]\n",
      "query_embedding = [-0.4812    0.983     0.4949   -0.2217    0.1339    1.057    -0.8477\n",
      " -0.683     0.2952    0.2292   -0.85     -0.3533   -0.2341   -0.4673\n",
      " -0.0954   -0.05124  -0.02985   0.1492   -0.724     0.4011   -0.1631\n",
      "  0.376    -0.71     -0.251     0.3228    0.5103    0.2363    0.5933\n",
      "  0.4353   -1.068     0.8374   -0.0769   -0.7573    0.534    -0.377\n",
      "  1.143     0.1396   -0.09515  -0.9653    0.2778   -0.6655    0.08575\n",
      " -0.2915    0.203     0.1143    0.00543   0.0565    0.513    -0.012924\n",
      " -0.367     0.1808   -0.1053    0.0894    0.801    -0.4932    0.06415\n",
      "  0.2942   -0.3523   -0.387    -0.6064    0.6006    0.08966   0.04648\n",
      "  0.4763    0.1074   -0.3408    0.588    -0.24     -0.242    -0.1357\n",
      " -0.499     0.1109    0.1542   -0.2234   -0.5215    0.611    -0.1293\n",
      " -0.605    -0.873     0.00306   0.1971    0.3564    0.03867  -0.08777\n",
      " -0.1271   -0.1876   -0.4807    0.484    -0.296    -0.728     0.02917\n",
      "  0.01888  -0.508     0.6763    0.605    -0.08636  -0.2966    0.4097\n",
      "  0.427     0.7407  ]\n",
      "query_embedding = [-0.2625   -0.1592    0.666    -0.1243   -0.638    -0.341    -0.1027\n",
      "  0.02988  -0.3943   -0.02727   0.2345    0.1566    0.2683   -0.867\n",
      " -0.527    -0.3943   -0.4246    0.3381    0.1796    0.0926    0.1698\n",
      "  0.3313    0.01019  -0.4412   -0.6045    0.232     0.2646   -0.1864\n",
      " -0.09033   0.1771   -0.6733    0.3186   -0.06055  -0.3672    0.8564\n",
      " -0.12067  -0.2301    0.08167   0.03778  -0.2097   -0.54     -0.3909\n",
      "  1.09     -0.3164   -0.1039   -0.2216    0.688    -0.0953    0.237\n",
      " -1.338     0.1383   -0.5327    0.397     0.5806    0.1165   -1.822\n",
      "  0.1077    0.1398    0.6943   -0.088     0.0658   -0.05106  -1.112\n",
      " -0.06146  -0.004208  0.2394    0.2369    1.018    -0.531    -0.403\n",
      "  0.9277   -0.02171  -0.385    -0.9224    0.664    -0.01207  -0.4507\n",
      " -0.1292   -0.2064   -0.7935    0.5405   -0.02573  -0.541     0.2323\n",
      " -0.9316   -0.705    -0.05936   0.6406   -1.078    -0.5503    0.1759\n",
      " -0.1732   -0.0436   -0.004112 -0.2666    0.6113   -0.5674   -0.1771\n",
      " -0.1516    0.4058  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.599    0.03084  0.472    0.3154  -0.4683  -0.8823  -0.2249  -0.3242\n",
      " -0.03683 -0.4548  -0.0893   0.338    0.4604  -1.022   -0.05197 -0.1895\n",
      " -0.6016  -0.2074   0.9893  -0.02028  0.585    0.1743  -0.2058   0.105\n",
      " -0.9404  -0.02734  0.05313  0.02542 -0.371    0.253   -0.2646  -0.0265\n",
      " -0.3733  -0.08026  0.1779   0.3918  -0.1871   0.3818   0.2207  -0.5366\n",
      " -0.7217  -0.1447   0.676    0.10376 -0.2002  -0.925    0.603   -0.1658\n",
      " -0.1558  -1.179    0.6313  -0.6987   0.3835   0.3777  -0.1201  -1.682\n",
      "  0.425    0.1328   0.3835  -0.2391   0.03705  0.2742  -1.157   -0.4539\n",
      " -0.416   -0.2002   0.03687  0.9785  -0.373   -0.6416   0.736    0.0739\n",
      " -0.531   -1.096    0.266   -0.0171  -0.02007  0.08954 -0.557   -1.333\n",
      "  0.1521   0.3418  -0.911    0.3354  -0.828   -0.455   -0.1181   0.1604\n",
      " -0.8354  -0.5386   0.2817   0.1527  -0.4333  -0.471   -0.7344   0.6333\n",
      " -0.575    0.098   -0.1342   0.1674 ]\n",
      "query_embedding = [ 1.7960e-02 -1.1316e-01  6.1084e-01 -3.6646e-01 -3.2837e-01 -2.1008e-01\n",
      "  1.5637e-01  3.6743e-02 -6.6711e-02 -4.2212e-01 -3.5303e-01  3.9429e-02\n",
      " -5.4492e-01 -9.1675e-02  1.9080e-01  7.2656e-01 -7.5488e-01  3.0688e-01\n",
      " -2.0801e-01  6.0449e-01  5.0537e-01 -5.3009e-02 -9.5654e-01  1.7126e-01\n",
      " -2.2375e-01  2.8613e-01  3.8727e-02  6.3721e-01  7.9736e-01 -9.9365e-01\n",
      " -5.2490e-01 -1.5625e-01 -1.8274e-01 -1.6125e-01  3.8379e-01  1.8738e-01\n",
      " -1.3428e-01  7.1228e-02 -2.2083e-01 -4.1235e-01 -2.5781e-01 -3.2043e-02\n",
      "  5.5469e-01 -1.8848e-01  7.6074e-01  4.3335e-02  5.1025e-01  1.0901e-01\n",
      "  4.7363e-02 -9.6484e-01  3.9307e-01 -4.0283e-01  4.9927e-01  5.5176e-01\n",
      " -2.3407e-02 -2.4219e+00 -6.9702e-02 -5.9906e-02  8.9453e-01 -1.5869e-01\n",
      " -5.0391e-01  3.5376e-01 -4.9652e-02  8.8745e-02  4.9414e-01 -5.0293e-01\n",
      "  5.4230e-02  2.5171e-01 -1.7456e-01  7.0679e-02 -1.2671e-01  9.7290e-02\n",
      " -4.7461e-01 -5.3369e-01 -2.1252e-01  8.6963e-01  1.1786e-01 -3.0225e-01\n",
      " -4.4312e-01  1.7065e-01  4.6997e-01 -3.1519e-01 -2.3376e-01 -1.4830e-03\n",
      " -1.1064e+00 -1.2861e+00  3.5474e-01  3.8623e-01 -1.4893e-01 -8.9990e-01\n",
      "  5.5115e-02  1.1310e-01 -1.3940e-01  3.6621e-01 -6.1035e-01 -7.2327e-02\n",
      "  2.0032e-01  7.5195e-02  6.1279e-02  3.2959e-01]\n",
      "query_embedding = [-0.519   -0.01668  0.618    0.10913 -0.62     0.9077  -0.833   -0.11633\n",
      "  0.2788   0.1144  -0.1494   0.4495  -0.4998  -0.305    0.712   -0.278\n",
      " -0.6265   0.2183   0.1344  -0.322   -0.03854  0.04526  0.03223 -0.327\n",
      "  0.5825   0.5674   0.03525 -0.3953   0.2576  -0.9194  -0.4563   0.00424\n",
      " -0.146   -0.1744  -0.4355   0.3022   0.1901   0.0748   0.205   -0.02599\n",
      " -0.02261  0.2393  -0.7314  -0.05414  0.2081  -0.2717   0.3794   0.1615\n",
      "  0.0914   0.1677  -0.3113  -0.4587  -0.3462   0.578    0.606   -0.1416\n",
      "  0.3894  -0.1292  -0.2421   0.4534   0.1865   0.1598   0.2534  -0.5273\n",
      " -0.064    0.09155  0.896   -1.119   -0.1131  -0.10187 -0.364   -0.1803\n",
      " -0.4062   0.1566  -0.1164   1.141    0.0876  -0.3787  -0.2202   0.3845\n",
      " -0.4705  -0.04034 -0.4712   0.6123  -1.109    0.12103 -0.2415   0.2625\n",
      " -0.806    0.03268 -0.4612   0.1401   0.416    0.4312   0.2256  -0.3594\n",
      "  0.574   -0.3594   0.09155  0.2144 ]\n",
      "query_embedding = [ 0.3113    0.1504    0.2937    0.4075   -0.7124    0.608    -0.182\n",
      " -0.6353   -0.0778    0.623    -0.10565  -0.2686    0.187    -0.62\n",
      "  0.3992   -0.074    -0.717    -0.4265   -0.1714    0.3433    0.914\n",
      " -0.1191    0.2104   -0.07336  -0.2043    0.05853  -0.1527    0.911\n",
      "  0.05463  -0.02426   0.4563    0.739     0.05313  -0.9653    0.93\n",
      "  0.005802  0.3657   -0.962    -0.1981    0.3313   -1.064     0.139\n",
      " -0.3894    0.2102    0.3354    0.09283  -0.2686    1.085    -0.1727\n",
      " -0.631     0.4956    0.4746    0.8877    0.368     0.1406   -1.319\n",
      "  0.8657   -0.58      1.282    -0.547    -0.01816  -0.0355   -0.222\n",
      "  0.004448  0.01362  -0.2686    0.554     0.3508   -0.084    -0.1891\n",
      " -0.2147   -1.175    -0.4854   -0.313    -0.315    -0.5806    0.011475\n",
      " -0.2827   -1.082     0.6084    0.5615    0.04843  -0.0676    1.157\n",
      " -0.9185   -0.3462    0.04147   0.4707    0.353    -0.871    -0.02408\n",
      " -0.1691   -0.2086    0.2263    0.00947   0.9087   -1.135     0.4856\n",
      "  0.3044    0.292   ]\n",
      "query_embedding = [ 1.3702e-02  4.0503e-01  4.1064e-01 -7.0801e-01 -3.8428e-01  1.1641e+00\n",
      " -1.1514e+00 -7.4609e-01  3.7598e-01  3.1860e-01 -1.0332e+00  3.4448e-01\n",
      "  9.3262e-02 -3.0200e-01  3.4261e-04 -2.6318e-01 -1.3125e+00  1.2585e-01\n",
      " -8.3154e-01 -2.2229e-01  2.4646e-01 -1.8640e-01 -3.5449e-01 -3.0298e-01\n",
      "  4.8859e-02  3.7402e-01  4.2407e-01  5.1904e-01  7.9150e-01 -8.2947e-02\n",
      "  3.2501e-02  5.7812e-01 -1.8848e-01  3.4766e-01 -9.0674e-01  1.7725e-01\n",
      "  1.0684e+00  9.9792e-02 -1.2524e-01  7.3425e-02 -6.6162e-01 -4.5898e-01\n",
      " -1.9104e-01 -3.6182e-01  4.0918e-01 -3.9453e-01  6.7041e-01  3.5254e-01\n",
      "  4.3555e-01 -8.0713e-01 -1.5027e-01 -3.4106e-01 -2.0599e-02  1.2920e+00\n",
      "  2.1118e-01 -1.7139e+00  3.8208e-01 -9.3994e-02  5.1465e-01 -3.0298e-01\n",
      "  7.9346e-01  4.3921e-01  7.6953e-01 -2.4414e-01  3.7329e-01  3.8867e-01\n",
      "  7.5293e-01 -6.3525e-01  1.3977e-01 -2.2205e-01 -1.9932e-03  1.0040e-01\n",
      " -7.5732e-01 -3.8306e-01 -1.6357e-01  1.4189e+00  4.4067e-01 -6.0059e-02\n",
      " -1.0791e+00  7.1045e-01  5.7666e-01  1.3391e-01 -4.8248e-02  4.8279e-02\n",
      " -9.4873e-01 -6.3623e-01 -3.7354e-01  2.4341e-01 -2.5830e-01 -7.3096e-01\n",
      " -4.9878e-01  1.0896e-04 -9.2578e-01  6.8457e-01  4.3628e-01  3.3374e-01\n",
      "  2.5708e-01 -1.1932e-01  3.8770e-01  2.9688e-01]\n",
      "query_embedding = [-0.002932  0.6304    0.5146   -0.4326   -0.904    -0.132    -1.177\n",
      " -0.65      0.61     -0.473    -0.4688    0.3901    0.0587   -0.2222\n",
      " -0.3772   -0.2747   -0.623     0.4426    0.0344    0.2107   -0.1442\n",
      "  0.6523   -0.3665   -0.2986   -0.353     0.2856   -0.1688   -0.4194\n",
      "  0.3513   -0.1219   -0.1079    0.1501    0.1152   -0.1509   -0.3354\n",
      "  0.1256   -0.01484   0.3003    0.613    -0.2347   -0.5625   -0.1309\n",
      "  0.04034  -0.653     0.0771   -0.4097    0.709     0.03098  -0.3376\n",
      " -1.268     0.547    -0.01569  -0.689     0.503     0.2864   -1.489\n",
      "  0.347    -0.2229    0.594     0.2155   -0.0726    0.6772   -0.1102\n",
      " -0.1436    0.5776    0.7324    0.5303    0.0627    0.00829  -0.181\n",
      "  0.0822   -0.1611   -0.3967   -0.3645   -0.3306    0.402     0.1827\n",
      "  0.0538   -0.505     0.2296    0.537    -0.5054   -0.623    -0.1736\n",
      " -1.388    -0.4417   -0.3748    0.3289   -0.7163   -0.3213   -0.04895\n",
      "  0.0921   -0.09814  -0.641    -0.6836    0.1475    0.3245   -0.1547\n",
      "  0.2452    0.2935  ]\n",
      "query_embedding = [ 0.2544   0.5186   0.3633   0.044   -0.3433   0.392    0.1018  -0.6455\n",
      " -0.0168   0.5225  -0.3096   0.1163  -0.2094  -0.4912  -0.04474 -0.1084\n",
      " -0.337    0.3218   0.4739   0.01974 -0.2761  -0.2644  -0.2751   0.1798\n",
      "  0.1603   0.4114  -0.0666  -0.05612 -0.352   -0.67     0.3982   0.01884\n",
      " -0.512    0.2695  -0.2335   0.473    0.387   -0.196    0.0399  -0.289\n",
      "  0.4517   0.02545  0.0593  -0.4033   0.2466  -0.3508   0.5425  -0.414\n",
      "  0.73    -0.1741  -0.06396  0.3093  -0.225    0.4385   0.1112   0.2078\n",
      "  0.4675  -0.3125  -0.687   -0.2401   0.172    0.1138   0.272    0.04538\n",
      " -0.3188   0.00583  0.659   -0.7363   0.2485  -0.1165   0.12305 -0.5557\n",
      "  0.138   -0.498   -0.4785   0.5864  -0.215   -0.1571   0.2294  -0.182\n",
      " -0.4175   0.3486   0.3176   0.4094   0.02016  0.02773  0.2349   0.482\n",
      " -0.2964  -0.7783  -0.11395  0.259   -0.2347   0.3557   0.3953  -0.3997\n",
      " -0.2803  -0.4778  -0.224    0.736  ]\n",
      "query_embedding = [-4.1016e-01  2.9004e-01  4.0210e-01  1.4636e-01 -2.9199e-01  2.4646e-01\n",
      "  1.7651e-01  9.6729e-01 -1.7212e-01 -2.7148e-01  2.2290e-01 -7.0679e-02\n",
      " -8.3447e-01 -1.0000e+00  6.2402e-01  3.3472e-01 -1.4246e-01  3.0716e-02\n",
      "  1.0052e-01  5.8838e-01  9.9609e-01  1.0779e-01  6.7017e-02 -4.3530e-01\n",
      "  7.8223e-01  2.9858e-01 -1.5625e-01  7.6562e-01  8.6365e-02 -7.8760e-01\n",
      " -5.2734e-01 -2.8540e-01 -6.8457e-01 -3.5474e-01 -1.8042e-01 -3.5596e-01\n",
      " -2.3840e-01  2.0679e-01  7.6843e-02 -4.4727e-01  9.7046e-02  6.4600e-01\n",
      " -7.9834e-02 -1.0376e-01 -1.4075e-01  7.8809e-01  1.3174e+00 -3.9209e-01\n",
      "  3.7451e-01 -1.4258e-01 -7.9163e-02 -4.3262e-01  5.7520e-01  1.9897e-01\n",
      "  2.4438e-01 -5.0488e-01  5.8740e-01  4.3921e-01 -5.9113e-02 -2.4780e-01\n",
      " -3.5620e-01  4.0210e-01 -5.8594e-03 -2.5342e-01 -5.6549e-02 -7.7197e-01\n",
      "  7.9932e-01 -6.3525e-01 -1.5686e-01 -6.0303e-01  2.7612e-01 -2.7563e-01\n",
      "  3.9722e-01 -4.0436e-03 -1.0848e-05  7.3303e-02 -5.0830e-01 -5.7617e-01\n",
      "  1.4124e-01 -6.4111e-01 -5.0977e-01  3.6426e-01 -3.6285e-02  7.8369e-01\n",
      " -4.3481e-01  1.6541e-01  1.7871e-01  8.2458e-02 -8.9990e-01 -6.4551e-01\n",
      " -3.1494e-02  4.3188e-01 -1.9434e-01 -1.1652e-01  1.6504e-01 -7.5562e-02\n",
      " -1.8616e-01 -2.7344e-01  8.2169e-03  1.0146e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.1846    0.1555    0.4805   -0.532    -0.7373    0.857     0.010086\n",
      "  0.15      0.6216   -0.1936   -0.2053   -0.01187   0.2349   -1.2\n",
      "  0.0735    0.1062   -0.537     0.032     0.7725   -0.161     0.013016\n",
      "  0.532    -0.1251    0.1892    0.6665   -0.3135    0.1973    0.04358\n",
      "  0.1886    0.12006  -0.1737   -0.519    -0.423    -0.2123   -0.3118\n",
      "  0.09015  -0.33      0.371     0.364    -0.154    -0.1495   -0.1877\n",
      "  0.5938   -0.11676  -0.12     -0.1295    0.4172    1.167     0.12146\n",
      "  0.1583    0.1743   -0.4888    0.2751   -0.1835   -0.08777  -0.06647\n",
      " -0.04468   0.1418   -0.5703    0.122    -0.1106   -0.1445   -0.3818\n",
      "  0.3179   -0.4536   -0.0564    0.8467    0.677    -0.10614  -0.1517\n",
      "  0.2189   -0.3276   -0.0528   -0.8594    0.02893   0.589    -0.01241\n",
      "  0.1576    0.601    -0.3135   -0.05426   0.02713  -0.10406   0.2267\n",
      " -0.0854   -0.908    -0.11206   0.3196   -0.9937   -0.1027    0.2471\n",
      " -0.2725    0.1824    0.4036    0.01743   0.0779    0.003368  0.9478\n",
      " -0.773     0.3596  ]\n",
      "query_embedding = [-0.1953    0.2023    0.612    -0.29     -0.8657    0.756    -0.6465\n",
      "  0.815     0.8486   -0.449     0.1123   -0.1542    0.003717 -0.3325\n",
      " -0.2842    0.412     0.02948  -0.3782    0.4824    1.28      0.6807\n",
      "  0.7227   -0.4895    0.2333    0.09906   0.2036    0.08984   0.645\n",
      "  0.744     0.2805   -0.1528   -0.2023    0.4304   -0.6978   -0.822\n",
      " -0.377     0.2495    0.654     0.6143   -0.785     0.2487    0.1873\n",
      "  0.4248   -0.7324   -0.2191    0.242     1.149     0.721     0.00767\n",
      " -0.701     0.4382   -0.714     0.545     0.4382   -0.3853   -1.345\n",
      "  0.5063    0.647     0.4612    0.0931    0.1488    1.039    -1.124\n",
      " -1.06      0.614    -0.557     0.8457    0.3442   -0.6704   -0.333\n",
      "  0.3406   -0.4053    0.1561   -0.1154    0.7607    0.585     0.4692\n",
      "  0.1109    0.01868  -0.1814   -0.3433    0.3208    0.0948    0.263\n",
      " -1.747    -0.08026  -0.707    -0.384    -0.2401   -0.1685    0.746\n",
      "  0.4714   -0.511     0.572    -0.4465    0.3396   -0.224    -0.1465\n",
      " -0.7607    0.591   ]\n",
      "query_embedding = [ 0.3904    0.005074  0.6797    0.4858   -0.1644    0.1088   -0.3462\n",
      " -0.2725    0.4062   -0.06604  -0.4526    0.08105   0.3037    0.03027\n",
      "  0.4885    0.5396    0.2136   -0.04843   0.0797    0.0897   -0.5054\n",
      "  0.1942   -0.5513   -0.1483    0.274     0.0633    0.637     0.6387\n",
      "  0.003239 -0.872     0.079    -0.2004    0.3096    0.565    -0.2313\n",
      "  0.1807   -0.05423   0.3499   -0.5884    0.04425   0.02916   0.8096\n",
      "  0.465     0.471    -0.2219   -0.1914    0.8833   -0.1699   -0.1261\n",
      " -0.2952    0.4497    0.06216   0.581     0.08386  -0.01614  -0.1234\n",
      "  0.3296   -0.7173    0.3464   -0.1809    0.2888   -0.1262   -0.1936\n",
      "  0.595     0.2217   -0.7183   -0.002949 -0.18      0.1698   -0.546\n",
      " -0.1973   -0.1907    0.1973   -0.81      0.1881    0.839    -0.2401\n",
      " -0.2979   -0.781    -0.1898    0.0632   -0.2563    0.403     0.2224\n",
      " -0.5195   -0.0617    0.1444    0.1664   -0.07     -0.6245   -0.15\n",
      " -0.0042   -0.631     0.5073    0.3892   -0.3308    0.2123   -0.4336\n",
      " -0.3691    0.5225  ]\n",
      "query_embedding = [-0.307     0.2644    0.717     0.01052  -0.2272    0.7485   -0.3337\n",
      " -0.346     0.5005    0.1766   -0.2242    0.6855    0.1611   -0.961\n",
      "  0.5234    0.523    -0.721     0.4058    0.0645   -0.415    -0.3018\n",
      "  0.2076    0.051    -0.3662   -0.0745   -0.1327   -0.2512   -0.07904\n",
      " -0.5435    0.002161 -0.692    -0.3765   -0.378     0.00851  -0.548\n",
      " -0.1067    0.04425  -0.3882    0.3972   -0.12134   0.1344   -0.2332\n",
      " -0.2499    0.3904   -0.1985    0.2722    0.9307    0.2942   -0.1946\n",
      " -0.28     -0.1179   -0.2347    0.4705   -0.1653   -0.10455   0.6665\n",
      "  0.2925   -0.476    -0.4668   -0.637    -0.0226   -0.3157   -0.1874\n",
      "  0.3337   -0.8623   -0.586     0.0972    0.4058    0.5176   -0.1389\n",
      " -0.1149   -0.5415    0.1707    0.03384  -0.4111    0.3071   -0.2076\n",
      "  0.001322  0.2876   -0.893    -0.04562  -0.605     0.0852   -0.1694\n",
      " -0.004486 -0.1561    0.419    -0.09863  -0.648     0.06744   0.1404\n",
      " -0.497    -0.1646    0.0976    0.03455   0.0817   -0.2603   -0.2145\n",
      " -0.9653    0.522   ]\n",
      "query_embedding = [ 0.2358    0.3962    0.2786   -0.1858   -0.6035    1.015    -0.9385\n",
      " -0.557     0.0641   -0.3337   -0.3281    0.523    -0.02011  -0.374\n",
      "  0.05402  -0.556    -0.736     0.2458   -0.3408    0.446    -0.1763\n",
      "  0.04944  -0.4185    0.2336    0.3477    0.3955    0.4507    0.2305\n",
      "  0.59     -0.517     0.335    -0.2822   -0.4895    0.307    -0.9185\n",
      " -0.03897   0.494     0.634     0.264     0.04373  -0.06854  -0.014854\n",
      "  0.03168  -0.4036   -0.2231    0.4324    0.8926    0.5186    0.279\n",
      " -0.2842   -0.1779   -0.4458    0.6       0.9873    0.556    -0.4048\n",
      "  0.3428   -0.6016   -0.3982    0.382     0.7085    0.7275    0.375\n",
      " -0.624    -0.05072   0.11584   0.6006   -0.665    -0.09375  -0.504\n",
      " -0.09735   0.4087    0.08673  -0.3054    0.5874    1.537     0.1791\n",
      " -0.048    -0.677     0.4924   -0.3105    0.5054    0.4563    0.621\n",
      " -1.023    -0.1318   -0.557    -0.0649   -0.7256   -0.4946    0.0382\n",
      "  0.1709   -0.1135    0.2634    0.511     0.2491    0.716    -0.3704\n",
      "  0.1932   -0.007317]\n",
      "query_embedding = [-0.4253   -0.5503    0.2942    0.519    -0.33      0.8755    0.3909\n",
      "  0.452    -0.2546   -0.429    -0.9004   -0.644    -0.00995  -0.2355\n",
      "  1.04     -0.1642   -1.053    -0.9844    0.4644   -0.895     0.1049\n",
      " -0.1744   -1.068     1.408    -0.1427    1.226     0.587    -0.2537\n",
      "  0.4143   -1.034     0.1483    0.088    -0.713    -0.6304   -0.7915\n",
      " -0.02429   0.89      0.0889    0.5405    0.4055   -0.04385   0.5405\n",
      " -0.424    -1.633     0.1549    0.4038    0.2191   -0.0285    0.10266\n",
      "  0.0208   -0.005207 -0.1974   -1.505     0.2257    0.725    -0.2194\n",
      "  0.4448    0.4119   -0.7534   -0.1428    0.1384    1.088     0.2805\n",
      "  0.4512   -0.644    -0.01401   1.206    -1.534     0.843    -0.4539\n",
      " -0.4814   -0.3958    0.4495    0.3347    0.04782   0.8525   -0.6787\n",
      "  0.5103   -0.6733    0.3433    0.07166  -0.9067   -0.564     0.62\n",
      " -1.931    -0.517     0.2966    0.56     -1.051     0.115     0.005287\n",
      " -0.584     0.184     0.296     0.6846    0.563     0.4958   -0.4707\n",
      "  0.5015   -0.705   ]\n",
      "query_embedding = [-0.07745  -0.2456    0.5176   -0.848    -0.1692    1.742    -0.1448\n",
      " -0.311     0.4       0.202    -0.2896   -0.1813    0.2135   -0.5664\n",
      "  0.138     0.03114  -0.5073    0.6753   -0.565     0.10675   0.75\n",
      " -0.2351    0.02118   0.1481    0.3535    0.689     0.7227    0.1566\n",
      "  0.6367    0.02756  -0.1948    0.2175   -0.1688    0.1971   -0.7085\n",
      "  0.1616    0.3716   -0.1677    0.7397   -0.0388   -0.047     0.07104\n",
      " -0.4807   -0.6333    0.508    -0.2427    0.417     0.4382    0.4458\n",
      " -0.643    -0.378     0.0371   -0.005066  1.13      0.656    -1.192\n",
      "  0.2678    0.2917    0.01912  -0.0771    0.682     0.609     0.2803\n",
      " -0.5195    0.1155    0.5903    0.7524   -0.7427   -0.0924    0.03543\n",
      "  0.04623  -0.4866   -0.4314   -0.0183   -0.1991    0.5186    0.541\n",
      " -0.2815   -0.4238    0.162     0.2527    0.2795    0.1605    0.405\n",
      " -0.3115   -0.2554   -0.1921   -0.4426   -0.428    -0.04758  -0.303\n",
      "  0.2377   -0.641     0.7603    0.4426    0.1316   -0.301    -0.1796\n",
      "  0.72      0.1487  ]\n",
      "query_embedding = [-0.5703   -0.787     0.1293   -0.3064   -0.568     0.6074   -0.516\n",
      " -0.2617    0.04922   0.3337   -0.1354    0.1592   -0.01067  -0.05984\n",
      "  0.2043    0.01657  -0.083    -0.1406   -0.2925   -0.5435   -0.136\n",
      " -0.2773    0.09753  -0.4075    0.09894   0.594     0.2969    0.802\n",
      " -0.662    -0.1367    0.3018   -0.4656    0.1492    1.258     0.2477\n",
      "  0.002031 -0.2917   -0.6714    0.653    -0.1948   -0.1672    0.2037\n",
      "  0.06824  -0.5107    1.335     0.3206    0.3584    0.475     0.3108\n",
      " -0.967     0.7817   -0.06064  -0.357     0.8726   -0.345    -1.404\n",
      "  0.4895   -0.1356    1.88      0.332     0.1826    0.671    -0.02658\n",
      " -0.456     0.2686    0.4944   -1.281    -0.0666    1.082     0.3564\n",
      "  0.337    -1.062    -0.374    -0.4866    0.6255    1.203     0.7603\n",
      "  0.2556   -0.2947   -0.702     0.909    -0.9727   -1.094     0.1938\n",
      " -0.6675    0.08746   0.3657    0.584    -0.3071    0.1287   -0.2805\n",
      "  0.597    -0.1995   -0.2783   -0.2964    1.155     0.01823  -0.3186\n",
      " -0.7886   -0.3398  ]\n",
      "query_embedding = [ 0.00663   0.1827    0.679     0.1063   -0.981     0.5186   -0.806\n",
      " -0.8843    0.147     0.0146   -0.862     0.04276  -0.5166   -0.377\n",
      "  0.821     0.01197  -0.7617    0.04282  -0.2496   -0.579    -0.11957\n",
      " -0.1257   -0.6714    0.02599  -0.542    -0.0336    0.688    -0.2888\n",
      "  0.08655  -0.1257   -0.0941    0.107    -0.4182    0.2106   -0.6763\n",
      "  0.2328    0.9365    0.3652    0.2169    0.1415   -0.321    -0.2087\n",
      " -0.2507   -0.6904    0.328    -0.1785    0.61      0.338     0.0895\n",
      " -0.8247   -0.1394   -0.3875   -0.3198    0.6694    0.553    -1.486\n",
      "  0.7104   -0.1721   -0.002619  0.218     0.736     1.145     0.6665\n",
      " -0.3916   -0.10016   0.7295    0.6997   -1.138     0.81     -0.2957\n",
      " -0.2974   -0.2046   -0.718     0.2366    0.09625   1.155     0.1271\n",
      " -0.2231   -0.965     0.01315   0.1653   -0.04498  -0.4246    0.243\n",
      " -1.353    -0.1553   -0.2352    0.02849  -0.871    -0.506    -0.7485\n",
      "  0.331    -0.0751    0.2324    0.3484    0.778     1.129    -0.662\n",
      "  0.2034    0.1807  ]\n",
      "query_embedding = [ 0.8      -0.0982    1.108     0.501    -0.8037    0.985    -0.5317\n",
      " -0.337    -0.2798    0.1724   -0.8203    0.443     0.624    -0.523\n",
      "  0.422     0.573    -0.2471    0.4504   -0.3953   -0.007763 -0.05008\n",
      "  0.4314   -0.02367  -0.411     0.371     0.396     0.505    -0.014015\n",
      "  0.3628   -1.066     0.2499    0.311    -0.4622    0.3425   -0.5527\n",
      "  0.908     0.698    -0.02644  -0.1458    0.388    -0.4714    0.1025\n",
      " -0.4202    0.4294   -0.1404   -0.3813    0.6426    0.4211   -0.6187\n",
      " -0.2212    0.4358   -0.2964   -0.1029    1.154    -0.01701  -0.1183\n",
      "  0.548    -0.1267    0.0662   -0.672     0.1305   -0.5977   -0.1676\n",
      " -0.258     0.1141   -0.3235    0.3489   -0.84      0.2417    0.10895\n",
      " -0.874    -0.3916   -0.2365   -0.259     0.432     0.4243    0.3933\n",
      "  0.5205    0.06415   0.05637   0.1892    1.105    -0.01264   0.834\n",
      " -0.8867   -0.568    -0.5303   -0.04468  -0.2878   -0.524    -0.2339\n",
      " -0.142    -0.9546    0.6655    0.387    -0.293    -0.1503   -0.7207\n",
      " -0.0193    0.0834  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-9.1797e-01 -5.0842e-02 -2.9663e-02 -1.3843e-01 -7.3633e-01  2.9321e-01\n",
      " -2.2253e-01  9.0527e-01  5.5908e-01 -2.2314e-01  2.9053e-01  2.9077e-01\n",
      " -9.9121e-02 -4.7485e-01  3.9276e-02  5.0964e-03  5.2704e-02  5.7220e-02\n",
      "  1.0654e+00  3.5864e-01  7.6660e-02  2.5024e-01  2.6074e-01 -2.5635e-01\n",
      "  5.3027e-01  2.8397e-02 -5.2295e-01  3.2153e-01  9.8991e-04 -2.7734e-01\n",
      " -8.3984e-01 -6.9482e-01 -2.1606e-01 -1.3574e+00 -3.3813e-01  2.9004e-01\n",
      " -5.9521e-01 -7.7588e-01  5.8154e-01  2.5122e-01 -6.3667e-03  2.5171e-01\n",
      " -1.6064e-01  1.1816e-01 -8.5010e-01  2.5574e-02  4.9121e-01 -2.2247e-02\n",
      "  5.2295e-01 -1.1597e-02  6.1963e-01 -8.6670e-01 -6.0699e-02  3.7207e-01\n",
      " -4.9255e-02 -7.8796e-02  1.3245e-01  5.5298e-02  2.2253e-01 -3.3716e-01\n",
      " -5.0146e-01  4.6875e-01  9.8877e-02 -5.7227e-01  3.3032e-01 -5.3955e-01\n",
      "  6.1475e-01 -2.0117e-01 -2.8229e-02 -3.6353e-01 -2.9590e-01 -2.2717e-01\n",
      "  5.8154e-01  4.5972e-01 -6.7932e-02 -9.3323e-02 -1.1318e+00 -2.3792e-01\n",
      " -4.6539e-02 -4.7046e-01  2.0199e-03  4.1821e-01 -2.2913e-01  3.2568e-01\n",
      " -1.7920e+00  6.8311e-01  2.3694e-01  1.5479e-01 -5.9326e-01 -4.6191e-01\n",
      "  2.6245e-01  4.8926e-01 -2.4280e-01  1.9739e-01  7.7637e-01 -4.2798e-01\n",
      "  1.1896e-01 -1.0590e-02 -1.4915e-02  7.3242e-01]\n",
      "query_embedding = [-0.2522    0.2908    0.2393    0.3533   -0.04895   0.259    -0.8877\n",
      "  0.7715    0.08606  -1.018    -0.532    -0.1211    0.2113   -0.2203\n",
      " -0.2532   -0.7534    0.228     0.2296   -0.4211    0.6855   -0.06366\n",
      "  0.6846    0.24     -0.6704   -0.11285   0.03308  -0.342    -0.6816\n",
      "  0.285    -0.665    -0.5425    0.6924   -0.04407  -0.3784    1.123\n",
      "  0.2355    0.2291    0.2454   -0.4026   -0.7046    0.08344   0.068\n",
      "  0.4927   -0.3562   -0.006603 -0.139     0.3677   -0.129     0.553\n",
      " -1.477    -0.07     -0.367    -0.3035    0.6577    0.214    -1.421\n",
      "  0.943     0.004696  1.184    -0.321    -0.53      0.7964   -0.777\n",
      " -0.7505    0.259     0.725     0.6636   -0.597    -0.0959   -0.2957\n",
      "  0.1678   -0.1628    0.5073   -0.1967   -0.55      0.4822   -0.5396\n",
      " -0.5996   -0.02371  -0.1035    0.11304  -0.1052    0.01593   0.2328\n",
      " -0.499     0.1094   -0.3828    0.03983  -0.1743   -0.1417    0.3574\n",
      " -0.4321    0.552    -1.107    -0.786    -0.2925   -0.2036   -0.448\n",
      "  0.631     0.523   ]\n",
      "query_embedding = [ 0.4521    0.1543   -0.06714   0.0654    0.8564   -0.5347   -0.1417\n",
      " -0.1434   -0.5034    0.0998    0.04425  -0.7437    0.4836   -1.065\n",
      " -0.2954    0.913    -0.2465    0.2025    0.9326    0.505    -0.2788\n",
      "  0.02559   0.6772   -0.2686    0.3362    0.1962    0.551     0.7026\n",
      " -0.413    -0.1909   -0.402     0.8525   -1.166    -0.09735  -0.1454\n",
      " -0.1887   -0.3267   -0.0387   -0.04794  -1.201     0.377     0.5244\n",
      " -1.3545    0.7573   -0.2443    0.1285   -0.4849   -0.246     0.679\n",
      "  0.1545   -0.8755   -0.827    -0.0759    0.631     0.3245   -1.338\n",
      " -0.5664    0.1683    0.667    -0.1296    0.1272    1.543    -0.2842\n",
      "  0.1962    0.586    -0.1935    0.5522    0.3398   -0.4072    0.758\n",
      "  0.2708    0.771    -0.1251    0.2646    0.008545  0.4932   -0.2974\n",
      "  0.709    -0.3408    0.2329   -0.767    -0.4119   -0.04153   0.2327\n",
      " -1.697     1.04     -0.1925   -0.1754    0.6484   -0.3342    0.4563\n",
      " -0.3337    0.2451   -0.04385   0.5      -0.7715   -0.2307   -0.498\n",
      "  0.763     0.0307  ]\n",
      "query_embedding = [-0.562    0.304    0.7197  -0.7104  -0.3936  -0.2151  -0.8564  -1.163\n",
      " -0.03592 -0.2556  -0.3875   0.3108   0.2366  -0.4666   0.741   -0.1721\n",
      "  0.03348  0.3826   0.2101  -0.562   -0.2028   0.01657 -0.0607  -0.265\n",
      "  0.2183  -0.0998   0.5137   0.2368  -0.8564  -0.653   -0.2483  -1.395\n",
      " -0.7905   0.2808  -1.322    0.4353   0.351   -0.3398   0.1185  -0.4683\n",
      " -0.838   -0.7886  -0.2598   0.192    0.3784  -0.5103   0.8438   0.582\n",
      "  0.8325  -1.04    -0.703    0.0729  -0.1742   0.714   -0.1722  -1.346\n",
      "  0.3303   0.3384  -0.575    0.4875   0.2124   0.2922  -0.585    0.03598\n",
      " -0.627    0.3928  -0.0652  -0.3582  -0.06903 -0.9062   0.02802 -0.0939\n",
      " -0.7705  -0.2742  -0.1448   0.6636   0.4065  -0.4548  -0.336   -0.02571\n",
      "  0.2717   0.1566  -0.4746   0.8237  -0.3386  -0.6797  -0.4365  -0.163\n",
      "  0.4148  -0.0846   0.8125   0.7056  -0.2861   0.0665   0.4998   0.0746\n",
      "  0.5566  -0.011   -0.202    0.3528 ]\n",
      "query_embedding = [-0.005566  0.1321    0.608    -1.029    -0.149     0.925    -0.3281\n",
      " -0.338    -0.02237  -0.02914  -0.523    -0.1656    0.1388   -0.3433\n",
      " -0.155     0.317    -0.5728    0.4846   -0.404     0.1761    0.657\n",
      " -0.2942   -0.4238   -0.4485    0.5176    0.967     0.1631    0.6914\n",
      "  0.8584    0.0803    0.095     0.1432   -0.4849    0.23     -0.47\n",
      " -0.07245   0.2299   -0.4377    0.2142    0.1284   -0.5425    0.04324\n",
      " -0.2379    0.1862    0.75     -0.11115   0.3562    0.2137    1.013\n",
      "  0.07996  -0.3855    0.11523   0.1891    0.882     0.0662   -1.222\n",
      "  0.704     0.07184  -0.5312    0.142     0.9224    0.7046   -0.2083\n",
      "  0.005276  0.2625   -0.0347   -0.4062   -0.2961    0.02415  -0.0339\n",
      "  0.1144   -0.4744   -0.4941   -0.0707   -0.03317   0.8955    0.748\n",
      " -0.1741   -0.9604    0.6836    0.586     0.02336   0.2084    0.2297\n",
      "  0.243    -0.1493   -0.0863    0.04047   0.2115   -0.3203   -0.3123\n",
      "  0.06216  -0.8623    0.83      0.3562    0.1912   -0.2211    0.01491\n",
      "  0.4915   -0.4683  ]\n",
      "query_embedding = [-0.09045  0.1964   0.2947  -0.477   -0.804    0.3079  -0.5522   0.5845\n",
      " -0.1705  -0.8486   0.1953   0.2367   0.4683  -0.59    -0.12164 -0.247\n",
      " -0.07294  0.1726  -0.0485   0.9526   0.5063   0.585   -0.1937  -0.4546\n",
      " -0.0311   0.516   -0.2405  -0.1007   0.536    0.02423 -0.5015   0.737\n",
      "  0.4946  -0.3474   0.8936   0.05743 -0.1913   0.3933   0.2118  -0.8984\n",
      "  0.0787  -0.1635   0.4526  -0.411   -0.195   -0.1349  -0.01631 -0.02185\n",
      "  0.1714  -1.241    0.0795  -0.9116   0.357    0.3628  -0.2494  -2.12\n",
      "  0.1454   0.53     0.9014   0.0336   0.02281  0.706   -1.036   -0.598\n",
      "  0.706   -0.0728   0.6704   0.528   -0.478   -0.674    0.3662  -0.3828\n",
      " -0.1035  -0.64     0.181    0.8257   0.0664  -0.408   -0.0838  -0.365\n",
      "  0.04535 -0.07355 -0.2012   0.3745  -1.402   -0.256   -0.4707  -0.1615\n",
      " -0.8794  -0.3633  -0.1736  -0.078    0.4326   0.00893 -1.031   -0.1159\n",
      " -0.3452   0.1151  -0.4082   0.202  ]\n",
      "query_embedding = [-0.5264    0.4827    0.2927   -0.2766   -1.205     0.594    -1.33\n",
      " -0.1131    1.064     0.11365  -0.457    -0.0744    0.5464   -0.5186\n",
      " -0.0902   -0.189    -0.25     -0.477    -0.094     0.56      0.002058\n",
      "  0.3757    0.2622    0.02283  -0.2155   -0.193     0.02556  -0.3262\n",
      "  1.212    -0.6978    0.1196    0.3267   -0.04794  -0.377    -0.2389\n",
      "  0.11365   0.19      0.4558    0.3599   -0.2598   -0.697     0.05548\n",
      "  0.093     0.185    -0.1047   -0.03958   0.8203    0.149     0.2218\n",
      " -0.6045   -0.0894    0.362     0.234     0.25     -0.10187   0.2421\n",
      "  0.7896   -0.836    -0.1788   -0.3962   -0.2291   -0.09863   0.1969\n",
      "  0.1442   -0.1313   -0.05115   0.4614    0.12085  -0.3103    0.11084\n",
      "  0.0394   -0.5146   -0.4377   -0.545    -1.04     -0.04333  -0.1517\n",
      " -0.2217   -0.3438    0.1355   -0.1781   -0.4978   -0.6377    0.3643\n",
      " -0.7266   -0.11206  -0.6694   -0.268    -0.931    -0.6035    0.1837\n",
      " -0.3904   -0.371    -0.6597   -0.2705   -0.5107   -0.762    -0.2266\n",
      " -0.0978    0.5825  ]\n",
      "query_embedding = [ 0.43     -0.3142    1.065    -0.2812   -1.058     0.3716   -0.6616\n",
      "  0.0546    0.5894   -0.9067   -0.424    -0.2231    0.1769   -0.3828\n",
      "  0.4548    0.4119   -1.22     -0.2466    1.131    -0.2585    0.2233\n",
      "  0.1787   -0.07477  -0.4946   -0.07776   0.2001    0.3118    0.0418\n",
      "  0.8384    0.707    -0.05215   0.432     0.05518  -0.3079    0.402\n",
      " -0.348    -0.1509    0.303     0.529     0.223    -0.1516   -0.5195\n",
      " -0.01372  -0.5117   -0.01627  -0.1013    0.8594    0.537     0.06903\n",
      " -1.054    -0.2253   -0.593     0.35     -0.0794    0.3164   -1.171\n",
      "  0.11456  -0.315     0.4148    0.758     0.2129    1.423    -0.4595\n",
      "  0.3342    0.02562   0.1415    0.4092    0.235     0.694    -0.347\n",
      " -0.1737   -0.3997   -0.1558   -0.5093   -0.0555    0.363     0.006466\n",
      "  0.1266   -0.1769   -0.5903    0.8193   -0.00743  -1.048     0.668\n",
      " -1.887    -0.10077  -0.3157    0.4463   -1.354    -0.519     0.292\n",
      " -0.02063   0.27      0.256    -0.275     0.67      0.6016   -0.3442\n",
      " -0.207     0.2937  ]\n",
      "query_embedding = [-0.256    0.31    -0.639    0.09344 -0.8237   0.2229  -0.8545  -0.3303\n",
      "  1.077    0.2815   0.1318   0.4197  -0.297   -0.6636  -0.1888  -0.5435\n",
      " -0.36     0.7085   0.321    0.11346  0.515    0.05008 -0.1649   0.906\n",
      "  0.1572   0.2118   0.05414 -0.8926   0.619   -0.02072 -0.2146  -0.2842\n",
      " -0.516   -0.01511 -0.1702   0.5454   0.6724  -0.526   -0.7305  -0.92\n",
      "  0.636   -0.3643  -0.737    0.1556  -0.10565  0.1592   0.8687  -0.4675\n",
      "  0.3403  -1.589   -0.1126   0.577   -0.2054   0.3875  -0.0226   0.8354\n",
      "  0.3738  -0.6685   0.08594 -0.283   -0.1343   0.7046  -0.204    0.8525\n",
      " -0.118    0.406   -0.575    0.132    0.089   -0.1514   0.03778 -0.2644\n",
      "  0.699   -0.00941 -0.2118  -0.6313  -0.323    0.7803   0.4846   0.652\n",
      "  0.3203  -0.1993   0.03119 -0.6284   0.799    0.1101   0.0673  -0.1224\n",
      "  0.6484  -0.4329  -0.4297   0.587   -0.1125  -0.565    0.1741  -1.07\n",
      "  0.772   -0.1897  -0.12354  0.543  ]\n",
      "query_embedding = [-0.1554   0.3953   0.1969  -0.629   -0.761    0.3538  -0.6465   0.12103\n",
      " -0.299   -0.0392   0.01974 -0.277    0.5195  -0.7     -0.956   -0.704\n",
      "  0.63    -0.0088  -0.3      0.8286   0.191    0.81    -0.5347  -0.6416\n",
      "  0.3945   0.1395  -0.3396   0.02588  0.932   -0.3135  -0.3247   0.7217\n",
      "  0.911    0.1252   0.6577  -0.358   -0.4976   0.3193   0.3599  -0.4282\n",
      "  0.07404  0.10065  0.72    -0.1511  -0.05612 -0.0632   1.139    0.5786\n",
      " -0.1738  -1.108   -0.2373  -0.5723   0.66     0.406   -0.2815  -1.051\n",
      "  0.1686   0.285    0.651   -0.431    0.2097   0.3599  -0.689   -0.4226\n",
      "  0.4119  -0.2133   0.546    1.153   -0.1192  -0.1582   1.008    0.0953\n",
      " -0.1561  -0.2411   0.648    0.3777  -0.09576 -0.2126  -0.3423  -0.342\n",
      " -0.399   -0.05154 -0.02853  0.5093  -1.015   -0.385   -0.6787  -0.2081\n",
      " -0.7964  -0.621   -0.124   -0.2627   0.628    0.1398  -1.047   -0.1302\n",
      " -0.9976  -0.58    -0.3635   0.7383 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.2137   -0.3794    0.8965   -0.2147   -0.8403    0.4558   -0.331\n",
      "  0.1632    0.5317   -0.1818    0.4185    0.10724  -0.5566    0.0707\n",
      "  0.00971   1.014    -0.4006   -0.341     0.3975    0.5527    0.322\n",
      " -0.02164  -0.2854   -0.05722  -0.0437   -0.4756   -0.3586    0.881\n",
      "  0.449    -0.4036    0.2869   -0.1599   -0.11096  -0.4668   -0.654\n",
      " -0.08374  -0.6675   -0.3545   -0.626     0.0792   -0.11975   0.35\n",
      "  0.115    -0.563    -0.5254    0.521     1.273     0.702    -0.1138\n",
      " -0.6016    0.6797   -0.34      0.1493    0.3723   -0.2253   -1.529\n",
      "  0.8286    0.0807    0.5703   -0.006477 -1.197     0.2122   -0.01921\n",
      " -0.2769    0.6025   -0.8896    1.048     0.1436   -0.1698   -0.2695\n",
      " -0.1716    0.1909   -0.4138    0.1492   -0.03705   0.5996   -0.476\n",
      "  0.02107  -0.62      0.784     0.2917    0.2445    0.5103    0.241\n",
      " -1.399    -0.4463   -0.02902   0.985    -0.459    -0.806     0.01452\n",
      "  0.631    -0.08014  -0.03522   0.2952    0.01071   0.0398    0.178\n",
      " -0.0867   -0.1555  ]\n",
      "query_embedding = [-0.289   -0.4426   0.119   -1.423   -0.2834   0.66    -0.67    -0.6147\n",
      " -0.1088   0.2515  -0.5264  -0.2893   0.06134 -0.6     -0.1051  -0.537\n",
      " -0.3015   0.2666  -0.785    1.094    0.02994 -0.09283 -0.5654   0.18\n",
      "  0.669    1.3     -0.8413   0.1921   0.534   -0.987    0.5244  -0.3936\n",
      " -0.1246  -0.0979  -0.3806   0.02538  0.605   -0.4607  -0.2136   0.0981\n",
      " -0.5156   0.3103   0.2238   0.08075  0.776   -0.817    0.5654   0.4468\n",
      "  0.3965   0.02415 -0.2274   0.08325  0.3948   0.6353  -0.474   -0.4595\n",
      "  0.7983  -0.472    0.1431  -0.2654   0.9487   1.072   -0.2566  -0.2233\n",
      "  0.2617  -0.1785  -0.627   -0.536    0.0938  -0.04385  0.2637   0.03192\n",
      " -0.273   -0.404   -0.7725   0.4077  -0.03314  0.11505 -1.071    0.61\n",
      "  0.519    0.0956  -0.5503   0.415   -0.3286   0.02388 -0.1367   0.07227\n",
      "  0.3274  -0.6475  -0.4597   0.5317  -0.05533  0.6655   0.3884  -0.1658\n",
      "  0.05295  0.4563   0.0983  -0.08496]\n",
      "query_embedding = [ 6.4648e-01  4.8877e-01  6.9727e-01  8.6975e-03 -8.4961e-01  9.3359e-01\n",
      " -5.7764e-01  1.0876e-01  4.5996e-01  1.4441e-01 -2.5122e-01  1.0762e+00\n",
      " -4.1943e-01  4.6997e-01  3.6743e-01 -3.9600e-01  1.0040e-02  1.2097e-01\n",
      " -7.7441e-01  6.1816e-01 -1.9263e-01 -9.6191e-02 -9.1406e-01 -6.5674e-01\n",
      " -3.1592e-01  6.1035e-01  7.5391e-01 -6.1890e-02  1.5820e-01 -1.3647e-01\n",
      "  5.6201e-01 -5.1422e-02 -5.7666e-01 -1.2854e-01 -6.1035e-01  2.4948e-02\n",
      "  9.4141e-01  4.0991e-01  2.1927e-02 -1.0535e-01 -3.2349e-01  4.2725e-01\n",
      "  6.3086e-01 -6.2988e-01 -2.7374e-02 -8.5205e-01  1.0371e+00 -1.8201e-01\n",
      "  4.1901e-02 -6.3525e-01 -2.0215e-01 -3.0786e-01  1.1957e-01  1.0137e+00\n",
      "  6.5796e-02 -6.5576e-01  5.4932e-01 -3.3716e-01  7.5586e-01  2.6782e-01\n",
      "  7.0166e-01  3.3252e-01  3.0566e-01 -3.6108e-01  4.0625e-01 -4.3433e-01\n",
      "  7.1973e-01  1.7920e-01 -1.2861e+00 -8.8745e-02 -3.0688e-01  2.1924e-01\n",
      "  1.1234e-03  1.7371e-01  1.5442e-01  9.3945e-01  1.1768e-01 -9.7607e-01\n",
      " -8.8818e-01  5.7324e-01 -4.0356e-01  9.7363e-01  6.9824e-02  3.1055e-01\n",
      " -9.4043e-01 -2.2754e-01 -9.8291e-01  3.5840e-01 -8.6230e-01 -1.8848e-01\n",
      " -1.2744e+00 -2.9370e-01  4.4678e-01  4.3945e-01  1.2559e+00 -6.2439e-02\n",
      "  4.5190e-01 -7.1143e-01 -2.4902e-02 -7.6721e-02]\n",
      "query_embedding = [ 0.646     0.3984    0.2798    0.02727  -0.2844    0.734    -0.68\n",
      " -0.1738   -0.3853    0.00487  -0.3218    0.4958   -0.1399    0.1081\n",
      "  0.4746    0.2764    0.1807    0.502    -0.2454    0.1566    0.12366\n",
      " -0.0858   -1.143    -0.515     0.1031    0.555     0.1637    0.6357\n",
      "  0.2096   -0.798     0.5566    0.2108   -0.8887   -0.03363  -0.1597\n",
      "  0.8145    0.5483   -0.5166   -0.387     0.0529    0.1012    0.3943\n",
      "  0.4941    0.4502   -0.913    -0.3909    1.188    -0.708     0.1263\n",
      " -0.864    -0.564    -0.1951    0.4678    0.287    -0.3289    0.003073\n",
      " -0.0934   -0.4124   -0.06305  -0.1184    0.1276   -0.125     0.11566\n",
      " -0.1382   -0.586    -0.296     0.2369   -0.335    -0.01192  -0.5986\n",
      " -0.3489   -0.1296    0.2306   -0.532     0.7256    0.856     0.3557\n",
      " -0.00476  -0.4163   -0.1852   -0.7407   -0.1797    0.7607    0.3743\n",
      " -0.329     0.04428   0.2267    0.8496   -0.4724   -0.4941   -0.1392\n",
      "  0.08777  -0.2056    0.549     0.865    -0.05313   0.1757   -0.6885\n",
      " -0.005608  0.354   ]\n",
      "query_embedding = [ 0.04892   0.04016   0.9053   -0.4426   -0.3713    0.637    -0.1176\n",
      " -0.4546    0.2478    0.6704   -0.6846    0.1754    0.04028  -0.582\n",
      "  0.4414   -0.2188   -0.3843    0.04852   0.02913  -0.1823   -0.373\n",
      " -0.0582   -0.0946   -0.396     0.6875    0.1624    0.06726  -0.202\n",
      " -0.2047   -0.7505   -0.1305   -0.4001   -0.2424   -0.2766   -0.2773\n",
      "  0.2125   -0.1526   -0.2047   -0.0755    0.1854    0.4329    0.0754\n",
      " -0.539    -0.1209    0.03256  -0.2893    0.5845   -0.04822   0.1292\n",
      "  0.3489   -0.3364   -0.00898  -0.1395    0.03574   0.2927    0.2812\n",
      "  0.5386    0.207    -0.4436    0.2668    0.3113    0.1447    0.3127\n",
      " -0.1863   -0.1107    0.0529    0.3965   -0.8394   -0.2363   -0.5356\n",
      " -0.3474   -0.1407   -0.264    -0.2162   -0.2338    0.3054    0.10425\n",
      " -0.133    -0.2482   -0.3909   -0.406     0.0386   -0.03485   0.8555\n",
      "  0.00972  -0.2001   -0.1959   -0.121    -0.5073   -0.471    -0.1726\n",
      "  0.3435   -0.0554   -0.010956  0.307     0.2162    0.522    -0.5176\n",
      " -0.2017    0.0733  ]\n",
      "query_embedding = [ 0.2123   -0.5874    0.2766   -0.158    -1.157     0.3303   -0.502\n",
      " -0.6455   -0.2869    0.4282   -0.8833    0.096    -0.3225   -0.01225\n",
      " -0.4023    0.566    -0.881     0.005257  0.0677   -0.4375    0.7485\n",
      " -0.4297   -0.2595    0.3792    0.2191    0.2769   -0.887     0.3167\n",
      "  0.03226  -0.02034   0.1343    0.6094   -0.2472    0.1881   -0.6665\n",
      " -0.1997    0.06726  -0.1048    0.334    -0.02768   0.05463  -0.2896\n",
      "  0.1359   -1.327    -0.2426   -0.1048    0.8438   -0.4614    0.5386\n",
      " -1.265     0.1523    0.5884    0.3464    0.7656    0.9795   -0.9883\n",
      "  0.3813    0.09235  -0.5093    0.0786    0.5215    0.4792   -0.2944\n",
      "  0.1433    0.31      0.00824   1.274    -0.9976    0.8833   -0.6626\n",
      "  0.1294    0.2878    0.1902   -0.0722   -0.6494    0.205    -0.2369\n",
      " -0.4956    0.2262    0.1311    0.5977    0.008835 -0.192    -0.3652\n",
      " -0.3423   -0.7095   -0.03403   0.753     0.05783  -0.677    -0.308\n",
      "  0.1833   -0.00939   0.1829    0.0686    0.05447  -0.2083    0.00762\n",
      " -0.2041    0.2266  ]\n",
      "query_embedding = [-0.2717  -0.10077  0.828   -0.2913  -0.4084   0.493   -0.6353  -0.1858\n",
      "  1.029    0.048   -0.1025  -0.142    0.3025  -0.704    0.2869   0.2896\n",
      " -0.478   -0.587    0.2394   0.28     0.2988  -0.03293 -0.13     0.02788\n",
      "  0.6865  -0.1538  -0.3691   0.622    0.2522  -0.3943  -0.112    0.0173\n",
      "  0.1708  -0.3604  -0.6255   0.2262   0.155   -0.1168  -0.3652  -0.2944\n",
      " -0.2725   0.3027   0.1284  -0.2256  -0.302    0.1284   1.352    0.02151\n",
      " -0.1306   0.4888   0.1658  -0.2913   0.1632   0.1729  -0.5854   0.07587\n",
      "  0.572   -0.251   -0.3967  -0.261   -0.5635  -0.0865   0.651    0.2522\n",
      " -0.05222 -0.4739   0.615    0.2578   0.461   -0.574   -0.398   -0.2905\n",
      " -0.2334   0.1214  -0.3284   0.2367   0.1552   0.1735   0.2347  -0.1472\n",
      "  0.05432  0.1803   0.03069  0.1798  -0.5166   0.3718  -0.1622   0.501\n",
      " -0.458   -0.416    0.2336  -0.02525 -0.07446 -0.3196  -0.11694  0.0696\n",
      " -0.1561   0.2146  -0.2166   0.5776 ]\n",
      "query_embedding = [-0.599    -0.1183    0.5356   -0.1494   -0.629    -0.0756   -0.9106\n",
      " -0.5654    0.3801   -0.2485   -0.226     0.483     0.4365    0.10065\n",
      " -0.2524   -0.1599   -0.6997   -0.3044   -0.4707    0.3997    0.01842\n",
      " -0.0566    0.00661  -0.52     -0.679    -0.1211   -0.5303    0.1829\n",
      "  0.5093   -0.4915    0.754     0.4026   -0.2181    0.322    -0.842\n",
      " -0.177     0.2423    0.05304   0.1594    0.1017   -0.6562    0.485\n",
      "  0.1538   -0.1829   -0.3062   -0.1148    0.1252    0.2595   -1.134\n",
      " -0.8203    0.2617    0.216    -0.469     1.113    -0.2544   -1.483\n",
      "  0.2167    0.0383    1.153    -0.10065   0.1288   -0.09454   0.01471\n",
      " -0.1164    0.6323    0.02237   0.3967   -0.01255  -0.6772    0.2338\n",
      " -0.841    -0.01808  -0.5723    0.03464   0.010345 -0.173     0.1259\n",
      "  0.096    -0.6226    0.654     0.4612    0.223    -0.918    -0.1952\n",
      " -1.043    -0.733    -1.012     0.549    -1.021    -0.264    -0.3992\n",
      " -0.2546   -0.648    -0.4883    0.11676  -0.3677   -0.2424    0.2593\n",
      "  0.4172   -0.011345]\n",
      "query_embedding = [ 0.1404    0.174     0.577     0.2196   -0.9014    0.881    -0.1627\n",
      "  0.7617   -0.11914   0.02338   0.2474   -0.3171   -0.2286    0.1617\n",
      "  0.4385    1.118    -0.4087   -0.7876    0.08655   0.4094    0.835\n",
      " -0.1759   -0.11316   0.622    -0.2439   -0.404    -0.391     0.427\n",
      "  1.155    -0.638     0.5327   -0.403    -0.5234   -0.514    -0.2385\n",
      " -0.5225   -0.9907   -0.5337   -1.156    -0.659    -0.3076    0.341\n",
      "  0.41      0.1123   -0.3237    0.9424    0.708     1.005    -0.2498\n",
      " -0.0816    0.2673   -0.2876    0.04092   0.857    -0.004597 -0.8423\n",
      "  1.138    -0.3127    0.2605   -0.2852   -0.9644   -0.3608   -0.3235\n",
      " -0.4124    0.2428   -0.04675   0.763     0.011665 -0.793    -0.3396\n",
      " -0.592    -0.081    -0.3865    0.4194   -0.1844    0.3738   -1.146\n",
      "  0.5913   -0.3735    0.3503    0.616     0.233     0.477     1.265\n",
      " -1.105    -0.7324    0.3044    0.5796   -0.0406   -0.717     0.2367\n",
      " -0.06604   0.4539    0.6353    0.2683   -0.0877   -0.09235   0.3225\n",
      " -0.2002    0.186   ]\n",
      "query_embedding = [ 5.8057e-01  5.7520e-01 -9.9976e-02 -2.9980e-01 -1.2317e-01  7.0166e-01\n",
      " -4.0601e-01 -7.9102e-01 -9.8877e-02 -1.4929e-01 -3.1323e-01  2.6953e-01\n",
      "  7.9004e-01  3.9600e-01 -4.4775e-01 -7.5586e-01  2.1362e-01  8.3887e-01\n",
      " -1.3086e+00  8.9844e-01 -1.8982e-01 -2.2546e-01 -4.2773e-01 -1.6016e-01\n",
      " -8.7280e-02  4.1919e-01  2.2461e-01 -2.6904e-01 -1.3025e-01  4.1284e-01\n",
      "  3.7354e-01  2.3206e-01 -5.3369e-01  7.4097e-02 -4.8145e-01 -6.4758e-02\n",
      " -5.2002e-02  3.4302e-01  1.1237e-01 -2.3254e-01 -9.8779e-01  2.6587e-01\n",
      "  1.0098e+00  4.2603e-01 -8.8770e-01 -1.2734e+00  2.7246e-01  2.1652e-02\n",
      "  4.3018e-01 -9.0283e-01 -1.0147e-03  5.9717e-01 -3.8605e-02  5.8740e-01\n",
      " -1.0150e-01 -2.2656e+00  4.3945e-01 -3.1982e-01  8.9258e-01 -2.1643e-01\n",
      "  8.2129e-01  5.6183e-02 -4.6783e-02 -7.1826e-01  1.2115e-02  1.5662e-01\n",
      "  2.4390e-01  8.5144e-03  1.6162e-01 -7.2021e-01  2.6660e-01  4.2139e-01\n",
      "  1.4001e-01 -6.1035e-01  4.2456e-01  6.3416e-02 -3.1934e-01 -4.7266e-01\n",
      " -7.4072e-01 -4.3604e-01 -5.6836e-01  6.7480e-01  5.3375e-02  5.5518e-01\n",
      " -6.3818e-01  1.9189e-01 -5.6982e-01  1.5808e-01  1.1986e-02 -2.4414e-01\n",
      " -6.7529e-01 -4.9951e-01 -2.3010e-01  6.4941e-01 -3.9526e-01 -5.2832e-01\n",
      "  9.2590e-02 -7.9443e-01  2.2290e-01  9.0479e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [ 0.348    0.517    0.6885   0.4668   0.2328   0.2494  -0.3057  -0.668\n",
      "  0.3386  -0.4138  -0.265    0.8174   0.1647  -0.02042 -0.3025  -0.357\n",
      " -0.0358   0.6055  -0.657    0.526   -0.0967  -0.1857  -0.6846  -0.317\n",
      "  0.352    0.8696   0.362   -0.3489   0.2732  -0.794   -0.4177   0.26\n",
      " -0.2231   0.3313   0.2393   0.1909   0.4387   0.949   -0.3413  -0.7993\n",
      " -0.1486  -0.0835  -0.1919  -0.277   -0.2213  -0.361    0.1327   0.2052\n",
      " -0.1995  -0.5176  -0.0343  -0.4949   0.1761   0.9106   0.2544  -1.191\n",
      "  0.724    0.481    0.2334  -0.4001   0.0889   0.4497  -0.572   -0.913\n",
      "  0.2241   0.619    0.747   -0.9795   0.63    -0.5454   0.1431   0.1841\n",
      "  0.2583  -0.368    0.5176   1.3955   0.1273  -0.9478  -0.7397  -0.04538\n",
      " -0.696    1.169    0.42     0.1014  -0.7905   0.1605   0.2366  -0.6465\n",
      " -0.346   -0.2021  -0.22    -0.1176   0.4683   0.286   -0.1705  -0.4016\n",
      "  0.0684  -0.4302  -0.10144  0.2311 ]\n",
      "query_embedding = [ 3.6035e-01  9.3457e-01  1.6028e-01 -8.4961e-01 -7.3828e-01  9.7754e-01\n",
      " -5.1855e-01 -4.7266e-01 -2.4902e-01 -6.4453e-01 -3.4448e-01  2.5244e-01\n",
      " -2.9639e-01 -4.8438e-01  3.0493e-01  1.2341e-01 -5.9509e-02  5.0391e-01\n",
      " -2.4451e-01  1.9321e-03  2.0483e-01 -2.1680e-01 -2.1411e-01 -3.1616e-01\n",
      " -1.1206e-01  1.2634e-01 -7.7441e-01  4.2065e-01  9.5312e-01 -5.9668e-01\n",
      " -1.9861e-01 -1.1792e-01  2.5073e-01  6.3379e-01 -4.1748e-02 -4.7231e-04\n",
      " -1.3931e-02  5.0049e-01  1.0205e-01 -9.1309e-01  2.1448e-01 -1.2195e-01\n",
      " -4.1699e-01 -5.4834e-01  4.0234e-01 -4.8462e-02  5.8740e-01  1.9348e-01\n",
      "  3.9380e-01 -1.2119e+00 -5.1465e-01 -1.5686e-01  4.5728e-01  3.0090e-02\n",
      "  2.2720e-02 -1.2129e+00  6.4026e-02  2.4817e-01  3.2495e-01 -4.4946e-01\n",
      " -2.0850e-01  6.0059e-01 -1.3599e-01  2.3035e-01 -6.0645e-01  7.1228e-02\n",
      " -1.7426e-02 -2.7466e-01  2.2681e-01 -2.5317e-01  9.9609e-02 -6.9141e-01\n",
      " -5.8447e-01 -7.9248e-01  3.7811e-02  1.3164e+00  1.5881e-01 -3.3765e-01\n",
      "  2.7539e-01 -4.3066e-01  3.9331e-01 -4.2285e-01 -5.7770e-02 -6.6406e-01\n",
      " -5.5225e-01 -7.1045e-01 -6.3086e-01  4.1113e-01  5.4817e-03  7.6477e-02\n",
      "  5.1074e-01 -1.6858e-01 -6.8408e-01 -1.1481e-01 -2.6465e-01 -5.0293e-01\n",
      " -1.6870e-01  8.0518e-01  2.6172e-01  1.0693e+00]\n",
      "query_embedding = [ 0.788     0.4233    0.3672    0.1035   -0.4678    0.6714   -0.974\n",
      " -0.1006    0.2046   -0.0326   -0.2642    0.821    -0.1945    0.1626\n",
      "  0.516     0.4485   -0.4473    0.365    -0.7686    0.529     0.159\n",
      "  0.2678   -1.609    -0.6787    0.0543    0.8965    0.657     0.4387\n",
      "  0.833    -0.1489    1.018     0.3535   -0.803     0.4753   -0.4314\n",
      "  0.207     1.081    -0.0897   -0.4424   -0.2896    0.0071    0.3962\n",
      "  0.917     0.1567   -0.3035    0.188     1.439    -0.552     0.48\n",
      " -1.162     0.04605  -0.1533    0.3398    0.823    -0.1366    0.13\n",
      "  0.3606   -0.545     0.2113    0.01573   0.3147    0.6367   -0.0979\n",
      "  0.006805 -0.006905 -0.1024    0.598     0.2783   -0.7495    0.2148\n",
      " -0.264    -0.2593    0.0918   -0.3186    0.6177    2.035    -0.0881\n",
      "  0.1981   -0.857     0.2067   -0.583     0.3813    0.59      0.736\n",
      " -0.4727   -0.2388   -0.467     0.4675   -0.712    -0.4849   -0.573\n",
      " -0.413    -0.0856    0.4329    0.5254    0.3345    0.1344   -1.048\n",
      "  0.01974   0.1658  ]\n",
      "query_embedding = [-0.04214  -0.1488    0.635     0.886    -0.6357   -0.2676   -0.2808\n",
      " -0.1622    0.01478   0.1167   -0.6104    0.1222    0.3452   -0.2194\n",
      " -0.1473   -0.0753   -0.3936    0.3342   -0.1309    0.5386   -0.4739\n",
      "  0.2325   -0.9395   -0.242    -0.1617   -0.10724   0.51      0.3604\n",
      "  0.2783   -0.7603    0.1062    0.2452   -0.2435    0.3635    0.4158\n",
      "  0.399     0.5776    0.77     -0.5747   -0.3762    0.1935   -0.1251\n",
      "  0.3784   -0.196    -0.3242   -0.1401    0.03647  -0.213     0.0043\n",
      " -0.9727    0.3345   -0.624    -0.1831    1.072     0.2014   -0.88\n",
      "  0.5303   -0.462     0.6406   -0.0244    0.0725   -0.489    -0.5977\n",
      " -0.5957   -0.2842    0.418     0.3574   -0.2267    0.5366    0.1779\n",
      " -0.00655   0.4548    0.2898   -0.8486    0.5605    1.454    -0.2083\n",
      " -0.7666   -0.649    -0.508     0.03084   0.7856    0.2146   -0.1039\n",
      " -0.997    -0.3625    0.04663  -0.3965   -0.857    -0.565    -0.42\n",
      " -0.1561   -0.6475    0.4343   -0.012825  0.623     0.0692   -0.5806\n",
      "  0.127     0.4565  ]\n",
      "query_embedding = [ 0.4614    0.1848    0.0897    0.405     0.1625   -0.605    -1.018\n",
      " -0.8086   -0.507    -0.5273   -0.3164    0.4404   -0.352    -0.1782\n",
      "  0.07776  -0.348    -0.6045    0.4287    0.37      0.481    -0.058\n",
      "  0.1225   -0.3054    0.1434    0.1026   -0.5483   -0.04843  -0.76\n",
      " -0.8145    0.003096 -0.2494    0.4717   -0.643     0.05743  -0.433\n",
      "  0.2484    0.5415    0.007133 -0.2751   -0.4822   -0.2081   -0.5273\n",
      "  0.03983  -0.004944  0.05453  -0.405     0.3757   -0.493     0.2009\n",
      " -1.664     0.2305   -0.01299   0.08795   0.7783    0.10114  -2.342\n",
      "  0.04025  -1.211     1.89      0.432     0.3435    0.5527   -0.2046\n",
      " -0.05676   0.947     0.2695    0.2683    0.2993    1.205    -0.0846\n",
      "  0.2847   -0.1781    0.1514   -0.3745   -0.5015    0.1737    0.1898\n",
      " -0.2131   -1.103    -0.3074    0.316     0.4592   -0.0178   -0.626\n",
      " -0.7236   -0.6147   -0.559     0.3074    0.2214   -0.8926   -1.095\n",
      "  0.013275  0.1493   -0.357    -0.7983    0.09424   0.2502   -0.589\n",
      "  0.7246    0.01014 ]\n",
      "query_embedding = [ 0.142   -0.2379   0.796   -0.04666 -0.6436  -0.1108  -0.653   -0.1318\n",
      "  0.3325  -0.218   -0.3286   0.2852  -0.3523  -0.738    0.1947  -0.25\n",
      " -0.07666  0.1475  -0.03009  0.1638  -0.5317   0.3826  -0.4856  -0.1381\n",
      "  0.3242  -0.1807  -0.3733  -0.03293  1.128   -0.6997   0.2402  -0.278\n",
      " -0.2795  -0.01973 -0.6294   0.4102   0.5146   0.1132   0.526    0.04977\n",
      "  0.0993   0.0791  -0.2311  -0.2491  -0.04907 -0.00551  0.3767   0.25\n",
      "  0.3823   0.2133  -0.02243 -0.2375   0.1648  -0.1407  -0.1837  -0.06213\n",
      " -0.04468 -0.1711  -0.6826  -0.6064   0.2754   0.0939   0.556    0.5024\n",
      " -0.4043  -0.3264   0.3464  -0.4734   0.3328  -0.1976  -0.194    0.3242\n",
      " -0.1243  -0.2983  -0.02782  0.528    0.317   -0.3906  -0.5405   0.4001\n",
      " -0.3228   0.0647   0.395   -0.2063  -0.5977  -0.2292  -0.2384   0.1583\n",
      "  0.06433 -0.3887  -0.03928  0.2051  -0.2332   0.615   -0.645   -0.1526\n",
      " -0.55    -0.07733  0.2725   0.4048 ]\n",
      "query_embedding = [ 2.1265e-01 -1.8079e-01  1.0518e+00 -1.9678e-01 -6.4502e-01  3.4741e-01\n",
      " -9.6729e-01 -4.3457e-01  5.2197e-01 -6.7725e-01 -5.5957e-01  1.9455e-02\n",
      "  1.3184e-01 -7.1143e-01 -5.6671e-02  1.7273e-01 -7.8223e-01 -4.9243e-01\n",
      "  8.5352e-01 -5.8008e-01  3.3203e-01  9.3384e-02  2.7588e-01 -1.4514e-01\n",
      " -1.7975e-02 -2.5830e-01  2.4146e-01  7.4463e-01 -4.4830e-02  6.0107e-01\n",
      "  1.1826e-02 -5.3174e-01 -3.3350e-01  5.7422e-01 -1.8689e-01  1.1462e-01\n",
      "  1.8066e-01  6.8848e-02  7.7588e-01 -3.4814e-01 -2.2681e-01 -5.7959e-01\n",
      "  3.6865e-01  1.4075e-01  5.6580e-02 -1.3379e-01  5.7861e-01  2.6196e-01\n",
      "  9.3115e-01 -7.5684e-01 -9.5801e-01 -2.1326e-01 -2.3270e-04  6.7627e-02\n",
      " -2.2864e-01 -8.7939e-01  1.4795e-01 -6.8408e-01  1.0266e-01 -2.0801e-01\n",
      "  1.5442e-01  7.0996e-01 -3.2446e-01  4.1284e-01 -4.9146e-01  6.1096e-02\n",
      "  2.6758e-01  1.6248e-01  6.7432e-01 -6.5088e-01 -3.7012e-01 -4.0991e-01\n",
      " -4.7339e-01 -1.2384e-01 -9.8206e-02  2.5146e-01 -8.1177e-02  2.6562e-01\n",
      " -1.9434e-01 -5.4443e-01  1.0420e+00 -1.1749e-02 -6.2402e-01  7.7295e-01\n",
      " -2.9907e-01 -6.4404e-01 -2.0300e-01  8.1299e-01 -1.2646e+00 -6.4648e-01\n",
      " -7.8308e-02  7.5049e-01 -2.4292e-01 -5.9180e-01 -4.6875e-01  5.7422e-01\n",
      "  5.4785e-01  4.0356e-01 -8.8037e-01  8.3594e-01]\n",
      "query_embedding = [ 0.2411    0.3696    0.6465    0.0761   -0.3486    0.7163   -0.6494\n",
      " -0.7544   -0.07263   0.0964   -0.9653    0.1844    0.1831   -0.5137\n",
      "  0.362     0.2625    0.003628  0.2717   -0.2817   -0.0801   -0.2006\n",
      "  0.4175   -0.628    -0.7314    0.255     0.2036    0.3235   -0.1993\n",
      "  0.4438   -0.905     0.6094    0.5986   -0.3408    0.1652   -0.1672\n",
      "  0.9756    0.4172   -0.01118  -0.2866    0.3735   -0.1587   -0.1439\n",
      " -0.1248    0.368     0.0661   -0.379     0.5874    0.2522   -0.2256\n",
      " -0.3962    0.2133    0.07404   0.2993    0.508    -0.03616  -0.2974\n",
      "  0.5933   -0.321    -0.188    -0.636     0.2544    0.1089    0.1562\n",
      "  0.2927   -0.1959   -0.0677    0.9155   -0.4558   -0.06537   0.1675\n",
      " -0.5483    0.1138    0.004032 -0.4983   -0.4321    0.4397   -0.2737\n",
      " -0.1625   -0.3052   -0.04205   0.02718   0.3093    0.2788    0.7456\n",
      " -0.6763   -0.06247  -0.339     0.371    -0.3987   -0.4333   -0.0792\n",
      " -0.10376  -0.3237    0.4695    0.1451   -0.1543   -0.5474   -0.291\n",
      "  0.11316   0.7837  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding = [-0.1389    0.4202    0.834    -0.008484 -0.5635    1.038    -0.979\n",
      " -0.3008    0.1846   -0.3318   -0.536     0.561    -0.01805  -0.2388\n",
      "  0.3232   -0.385    -0.973     0.3972   -0.5303    0.477    -0.405\n",
      "  0.3718   -0.8726   -0.2367    0.2045    0.8994    1.063     0.998\n",
      "  1.255    -0.597     0.04843  -0.07336  -0.776     0.2316   -1.091\n",
      "  0.2104    0.8735    0.7173    0.1267   -0.2156   -0.09393  -0.2363\n",
      "  0.1239   -0.2542   -0.04425   0.5527    0.771     0.5767    0.0903\n",
      " -0.8335   -0.122    -0.5337    0.8247    0.782     0.1852   -1.019\n",
      "  0.3186    0.286    -0.06287   0.01894   0.575     0.564     0.2223\n",
      " -0.695    -0.2345   -0.06354   0.65     -0.7764   -0.1366   -0.5786\n",
      "  0.0355    0.1444   -0.2695   -0.2605    0.5825    1.708     0.336\n",
      " -0.09155  -0.539     0.6367   -0.4675    0.10297   0.41      0.673\n",
      " -0.8047   -0.57     -0.1622   -0.3225   -0.719    -0.1917   -0.2042\n",
      "  0.1924   -0.5215    0.831     0.002758  0.2461    0.1218   -0.8145\n",
      " -0.4485    0.3582  ]\n",
      "query_embedding = [-0.2219   1.158   -0.346   -0.4084   0.0871  -0.2131  -0.4421   0.04092\n",
      " -0.2815  -0.5605  -0.255   -0.4639   0.1085  -0.0869   0.01455 -0.371\n",
      " -0.1567  -0.04465 -0.2834  -0.0874  -0.1177   0.4744  -0.3523  -0.01671\n",
      " -0.2205   0.7407  -0.3455   0.4763  -0.4265  -0.6807  -0.0493  -0.62\n",
      " -0.2893   0.5283  -0.7554  -0.4385  -0.2947   0.7207  -0.06885  0.1556\n",
      " -0.2522   0.1356   0.412   -0.2283   1.16    -0.3867  -0.4287  -0.508\n",
      " -0.02756 -0.3901  -0.3196   0.6177   0.3997   0.69    -0.10693 -1.573\n",
      "  0.0945  -0.803    2.15     0.9272  -0.2203   0.2019  -0.05716  0.1329\n",
      "  0.3743  -0.2413  -0.453    0.1294   0.3464  -0.6587   0.1844  -0.6445\n",
      " -0.9937  -0.397    1.725    1.158   -0.433    0.9263  -0.5083  -0.1173\n",
      "  0.1722   0.6997  -0.2942  -0.10095 -1.332    0.745    0.2107  -0.2252\n",
      " -0.6035  -0.644    0.526   -0.7837  -0.1759  -0.3936   0.2644   0.2037\n",
      "  0.3188   0.6895   0.3645  -0.1575 ]\n",
      "query_embedding = [-0.2896   -0.0405    0.6855    0.05887  -0.876     0.781     0.2307\n",
      " -0.6387    0.9185   -0.2391    0.3328    0.4424    0.04498  -0.858\n",
      " -0.3374   -0.2954   -0.2961    0.11346   0.7305    0.03702   0.6606\n",
      "  0.3647    0.4546   -0.00636   0.0143    0.5547    0.2091   -0.08496\n",
      " -0.098    -0.4993    0.1067   -0.1565   -0.01624  -0.4377   -0.57\n",
      "  0.1274   -0.5225    0.10504   0.8525   -0.4446   -0.1458    0.9033\n",
      " -0.09796  -0.753     0.1821   -0.00412   0.5435    0.7554   -0.6084\n",
      "  0.005924  0.3208   -0.0683    0.0265    0.3975    0.371    -0.6973\n",
      "  0.927    -0.05984  -0.2427    0.03442  -0.1276    0.03738  -1.004\n",
      " -0.4856    0.1942   -0.1708    0.727     0.251    -0.518    -0.7363\n",
      "  0.2612   -0.84     -0.4185    0.1351    0.3933    0.274    -0.3577\n",
      "  0.2418    0.0375   -0.3257   -0.285     1.157     0.04294   0.814\n",
      " -1.019    -0.1833   -0.1224    0.03497  -0.753    -0.3896   -0.1464\n",
      "  0.431     0.11896   0.5767   -0.07227   0.252    -0.311     0.3064\n",
      " -0.5967   -0.1648  ]\n",
      "query_embedding = [-0.304     0.812    -0.9683   -0.3936    0.613     0.8735   -0.5903\n",
      " -0.3247    0.428    -0.1088   -0.324     0.002815  0.359     1.089\n",
      " -0.6313   -0.719     0.1926    0.394    -0.1954   -0.6743   -0.04633\n",
      " -0.567    -0.568     1.044     0.385     0.92     -0.3591    0.9946\n",
      " -0.646     0.1514   -0.665    -0.11365  -0.5195    0.7046   -0.8706\n",
      "  0.413    -0.0669   -0.2137    0.7354   -0.5913   -0.1948   -0.7754\n",
      "  0.1501   -0.499     0.11176  -0.3867    0.00969   0.7573   -0.2206\n",
      " -0.4307    0.0735    0.04044  -0.3738    0.2969   -0.5215   -0.0672\n",
      " -0.388     0.11194   0.309    -0.672     0.6694    0.7437    0.3733\n",
      "  0.3833   -1.264    -0.6494    1.289    -0.9194   -0.5376   -1.62\n",
      "  0.03903  -0.672     1.282     0.0923   -0.3472    0.646     0.3567\n",
      " -0.4272   -0.446    -0.314     0.1962   -0.4016   -0.0955    0.0297\n",
      "  0.3481   -0.165    -0.07745  -0.0646    0.4775    0.2605   -0.01753\n",
      " -0.729    -0.1921    0.03134  -1.027    -1.257    -0.748    -0.02707\n",
      "  0.5415    0.3762  ]\n",
      "query_embedding = [ 4.1772e-01  1.1926e-01  5.7422e-01  1.0632e-01 -1.3855e-01  9.2139e-01\n",
      " -3.1396e-01 -3.3374e-01 -1.2549e-01 -3.4546e-01 -5.2979e-01 -8.9417e-02\n",
      " -3.1128e-01 -7.4805e-01 -5.8777e-02 -8.8196e-02 -3.5376e-01  4.3652e-01\n",
      " -2.5049e-01  3.5797e-02  4.7882e-02 -3.1067e-02 -3.5254e-01 -6.8848e-01\n",
      "  2.9688e-01  8.8965e-01  1.7993e-01  6.5137e-01  6.1572e-01 -1.0127e+00\n",
      " -2.4573e-01 -6.7810e-02 -2.3328e-01  2.5586e-01  5.4703e-03  1.9458e-01\n",
      "  9.3750e-02  5.7812e-01 -9.8724e-03 -6.3965e-01  2.8491e-01 -9.2773e-02\n",
      " -3.1494e-01 -2.6123e-01  5.1611e-01  1.3135e-01  2.5073e-01  3.4515e-02\n",
      "  2.3926e-01 -1.2048e-01  2.4658e-02 -3.8501e-01  2.3669e-01  7.1240e-01\n",
      " -3.7567e-02 -8.8330e-01 -4.2297e-02 -3.7567e-02  1.7807e-02 -4.8975e-01\n",
      "  5.7373e-01  7.5439e-01 -9.5367e-03 -2.8564e-02 -1.1981e-01 -1.3171e-01\n",
      "  4.7388e-01 -3.1030e-01 -6.3095e-03 -3.9819e-01 -2.4707e-01  2.2827e-01\n",
      " -2.1271e-02 -7.7002e-01 -2.5464e-01  1.3262e+00  8.2703e-02 -4.1431e-01\n",
      "  1.0529e-01  1.4694e-02 -1.5771e-01 -1.9238e-01 -1.2042e-01 -1.4746e-01\n",
      " -6.3623e-01 -8.2471e-01 -2.3267e-01  1.1841e-01 -4.4604e-01 -2.2583e-01\n",
      " -1.2016e-03  1.2817e-01  1.1377e-01  2.4255e-01 -3.4473e-01 -3.8281e-01\n",
      " -3.3398e-01 -1.7603e-01  3.1104e-01  4.7437e-01]\n",
      "query_embedding = [ 0.3958    0.4727    0.1644   -0.887    -0.2026    0.6694   -0.7173\n",
      " -0.3845   -0.06146  -0.07495  -0.966     0.02066   0.1122   -0.713\n",
      " -0.919    -0.2075    0.5737    0.7817   -0.4146    0.3943    0.3428\n",
      " -0.4436   -0.6416   -0.2942    0.1401    0.4177   -0.1768    0.1333\n",
      "  0.9927   -0.6016   -0.5884    0.3596    0.03128   0.156    -0.05933\n",
      " -0.525     0.1494    1.003    -0.5264    0.03586   0.2057   -0.749\n",
      "  0.2133   -0.2416   -0.006485 -0.03973   0.6587    0.4263    0.10016\n",
      " -0.615    -0.2644   -0.00934  -0.278     0.7466   -0.05814  -1.694\n",
      "  0.2822    0.0164    0.4539   -0.1825    0.4172    0.7017   -0.585\n",
      "  0.02936  -0.1063    0.325     0.4634   -0.4155    0.1786    0.2008\n",
      "  0.2275   -0.03287  -0.2493    0.1098    0.2615    1.065     0.1148\n",
      " -0.09845  -1.105     0.1442   -0.6353   -0.273     0.608     0.9067\n",
      " -0.2642   -0.342    -0.958     0.0819   -0.01773   0.00866  -0.4993\n",
      " -0.776     0.12024   0.5576   -0.5923    0.023    -0.04758  -0.3003\n",
      "  0.4275    0.91    ]\n",
      "query_embedding = [ 0.2219   0.3254   0.5146   0.336   -0.441    0.577   -0.8623  -0.3276\n",
      " -0.4934  -0.0634  -0.771   -0.272    0.998   -0.4116  -0.513   -0.504\n",
      "  0.11914  0.308   -1.134    0.6743  -0.1458  -0.3274  -0.525   -0.785\n",
      "  0.4824   0.11273  0.1484  -0.5425   0.906   -0.594    0.2869   0.646\n",
      "  0.11584  0.441    0.408    0.1678   0.5864   0.513   -0.7505  -0.2969\n",
      " -0.1371  -0.1573   0.9136  -0.0813  -0.6694  -0.3406  -0.10046  0.11786\n",
      " -0.4946  -1.139    0.574    0.08203 -0.2708   1.06     0.3005  -1.173\n",
      "  0.7886  -0.4128   1.378   -0.3398  -0.06555 -0.1735  -0.927   -0.5156\n",
      "  0.7354   0.368    0.03455  0.4714   0.3264  -0.0998   0.3523   0.0339\n",
      "  0.5874  -0.5264   0.796    0.3914  -0.4783  -0.4595  -0.7256  -0.0434\n",
      "  0.5444  -0.00317 -0.03586  0.2343  -1.117    0.71    -0.4675  -0.1216\n",
      " -0.1857  -0.4832  -0.515   -0.5967  -0.3909   0.4897  -0.2151   0.2051\n",
      " -0.4524  -0.4724  -0.08746  1.063  ]\n",
      "query_embedding = [-0.1261   0.2817   0.9907   0.1128  -0.0631   1.166   -0.5063  -0.607\n",
      " -0.2179  -0.1423  -0.728    0.4846  -0.5503  -0.1062   0.407    0.392\n",
      " -0.06186 -0.1921   0.6953   0.424    0.476   -0.0742  -0.2053  -0.3057\n",
      "  0.359    0.531   -0.04694  0.4136   0.1353  -0.4312  -0.5513  -0.4563\n",
      " -0.4932  -0.346   -1.146    0.1864   0.1559   0.4197   0.3726  -0.633\n",
      "  0.6265  -0.32     0.02792  0.2186  -0.2467   0.3242   0.743    0.2327\n",
      "  0.501   -0.1531   0.1467  -0.3591   0.4192   0.3516  -0.07635 -0.7563\n",
      "  0.7163   0.2037  -0.2401   0.2742   0.9834   0.8564  -0.1886  -0.2761\n",
      "  0.041   -0.1483   0.795   -0.3918  -0.2467  -0.9224   0.08215 -0.0654\n",
      "  0.0795   0.2888   0.1362   0.5615   0.1527  -0.09357  0.372   -0.7354\n",
      " -0.7246   0.02582  0.298    0.693   -0.9414  -0.02812  0.04474 -0.1743\n",
      " -0.581    0.0592  -0.2328   0.08746 -0.1858   0.5664  -0.04886  0.384\n",
      " -0.7383  -0.7856  -0.9595   0.329  ]\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "\n",
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},#vs boredom\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},#vs sad\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},#\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\") \n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets\\\\stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Representações usadas**:Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words)\n",
    "\n",
    "arr_representations = [embedding, emb_nostop, emb_keywords_exp, bow_keywords, bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datasets\\\\amazon_reviews_mini.txt\",index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Representação: embbeding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 22:57:17,195]\u001B[0m A new study created in RDB with name: random_forest_embbeding_fold_0\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:57:32,810]\u001B[0m Trial 0 finished with value: 0.7026844411452852 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7026844411452852.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:57:48,410]\u001B[0m Trial 1 finished with value: 0.7076808594215102 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 30}. Best is trial 1 with value: 0.7076808594215102.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:58:05,113]\u001B[0m Trial 2 finished with value: 0.7257292199352072 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 35}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:58:21,193]\u001B[0m Trial 3 finished with value: 0.7100922628235254 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:58:37,920]\u001B[0m Trial 4 finished with value: 0.7093917088711424 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:58:53,377]\u001B[0m Trial 5 finished with value: 0.6963224688291163 and parameters: {'min_samples_split': 15, 'max_features': 80, 'num_arvores': 40}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:59:10,153]\u001B[0m Trial 6 finished with value: 0.7187088565569706 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 50}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:59:25,712]\u001B[0m Trial 7 finished with value: 0.683282053795201 and parameters: {'min_samples_split': 21, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:59:40,749]\u001B[0m Trial 8 finished with value: 0.6797143711243168 and parameters: {'min_samples_split': 19, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7257292199352072.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 22:59:57,444]\u001B[0m Trial 9 finished with value: 0.7340698717945805 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 50}. Best is trial 9 with value: 0.7340698717945805.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:00:04,832]\u001B[0m A new study created in RDB with name: random_forest_embbeding_fold_1\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365612911831398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:00:21,431]\u001B[0m Trial 0 finished with value: 0.7222799954681726 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:00:36,987]\u001B[0m Trial 1 finished with value: 0.7067929887823774 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:00:52,061]\u001B[0m Trial 2 finished with value: 0.690268529085199 and parameters: {'min_samples_split': 15, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:01:07,111]\u001B[0m Trial 3 finished with value: 0.6871506821302912 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:01:22,370]\u001B[0m Trial 4 finished with value: 0.7135292832673573 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:01:37,592]\u001B[0m Trial 5 finished with value: 0.7052235262492168 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:01:52,475]\u001B[0m Trial 6 finished with value: 0.7065851206386559 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:02:07,032]\u001B[0m Trial 7 finished with value: 0.6922181080599351 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:02:21,819]\u001B[0m Trial 8 finished with value: 0.6993618840537401 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7222799954681726.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:02:38,357]\u001B[0m Trial 9 finished with value: 0.7249207702806876 and parameters: {'min_samples_split': 3, 'max_features': 90, 'num_arvores': 45}. Best is trial 9 with value: 0.7249207702806876.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:02:45,584]\u001B[0m A new study created in RDB with name: random_forest_embbeding_fold_2\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7465286656068304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:03:01,721]\u001B[0m Trial 0 finished with value: 0.7201785797844312 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:03:16,801]\u001B[0m Trial 1 finished with value: 0.699776015116016 and parameters: {'min_samples_split': 9, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:03:31,777]\u001B[0m Trial 2 finished with value: 0.6856857677883778 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 50}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:03:46,524]\u001B[0m Trial 3 finished with value: 0.6890500880502324 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:04:02,452]\u001B[0m Trial 4 finished with value: 0.7089834273368224 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:04:18,601]\u001B[0m Trial 5 finished with value: 0.7035298194891331 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:04:33,538]\u001B[0m Trial 6 finished with value: 0.6756252600847198 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:04:49,150]\u001B[0m Trial 7 finished with value: 0.6874229318362918 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:05:05,609]\u001B[0m Trial 8 finished with value: 0.7106546331499247 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:05:21,232]\u001B[0m Trial 9 finished with value: 0.6737470061432735 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 45}. Best is trial 0 with value: 0.7201785797844312.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:05:31,500]\u001B[0m A new study created in RDB with name: random_forest_embbeding_fold_3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7179188307262616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:05:56,902]\u001B[0m Trial 0 finished with value: 0.7128944352036335 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:06:19,988]\u001B[0m Trial 1 finished with value: 0.6838556197407164 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 35}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:06:42,673]\u001B[0m Trial 2 finished with value: 0.6664411431054836 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 30}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:07:06,823]\u001B[0m Trial 3 finished with value: 0.6867754288955733 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 40}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:07:30,198]\u001B[0m Trial 4 finished with value: 0.6826937455495649 and parameters: {'min_samples_split': 19, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:07:53,140]\u001B[0m Trial 5 finished with value: 0.6913993256782258 and parameters: {'min_samples_split': 13, 'max_features': 70, 'num_arvores': 50}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:08:16,415]\u001B[0m Trial 6 finished with value: 0.6808881122016931 and parameters: {'min_samples_split': 15, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.7128944352036335.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:08:42,423]\u001B[0m Trial 7 finished with value: 0.7170388578278817 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 45}. Best is trial 7 with value: 0.7170388578278817.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:09:08,498]\u001B[0m Trial 8 finished with value: 0.720388942067586 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 45}. Best is trial 8 with value: 0.720388942067586.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:09:31,609]\u001B[0m Trial 9 finished with value: 0.6679346444551628 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 30}. Best is trial 8 with value: 0.720388942067586.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:09:42,110]\u001B[0m A new study created in RDB with name: random_forest_embbeding_fold_4\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365232181965737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:10:02,983]\u001B[0m Trial 0 finished with value: 0.7207572854636092 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.7207572854636092.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:10:26,133]\u001B[0m Trial 1 finished with value: 0.7169367127513508 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7207572854636092.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:10:45,845]\u001B[0m Trial 2 finished with value: 0.6978286957673175 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7207572854636092.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:11:08,083]\u001B[0m Trial 3 finished with value: 0.7102414464488311 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.7207572854636092.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:11:29,894]\u001B[0m Trial 4 finished with value: 0.6957899906211091 and parameters: {'min_samples_split': 13, 'max_features': 70, 'num_arvores': 50}. Best is trial 0 with value: 0.7207572854636092.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:11:54,115]\u001B[0m Trial 5 finished with value: 0.7231783051848016 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 5 with value: 0.7231783051848016.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:12:14,950]\u001B[0m Trial 6 finished with value: 0.6828795406777354 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 40}. Best is trial 5 with value: 0.7231783051848016.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:12:34,819]\u001B[0m Trial 7 finished with value: 0.7113360468698035 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 30}. Best is trial 5 with value: 0.7231783051848016.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:12:56,890]\u001B[0m Trial 8 finished with value: 0.7290150506880995 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 35}. Best is trial 8 with value: 0.7290150506880995.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:13:17,545]\u001B[0m Trial 9 finished with value: 0.6886663297616725 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 30}. Best is trial 8 with value: 0.7290150506880995.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:13:25,871]\u001B[0m A new study created in RDB with name: random_forest_emb_nostop_fold_0\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6758563074352548\n",
      "Representação: embbeding concluida\n",
      "===== Representação: emb_nostop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:13:46,817]\u001B[0m Trial 0 finished with value: 0.7035810115270137 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7035810115270137.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:14:08,877]\u001B[0m Trial 1 finished with value: 0.6923730392066213 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.7035810115270137.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:14:35,716]\u001B[0m Trial 2 finished with value: 0.699876086501197 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.7035810115270137.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:14:59,331]\u001B[0m Trial 3 finished with value: 0.6611263153513383 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 40}. Best is trial 0 with value: 0.7035810115270137.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:15:23,586]\u001B[0m Trial 4 finished with value: 0.6787875935943545 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7035810115270137.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:15:43,813]\u001B[0m Trial 5 finished with value: 0.7064933309223935 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 40}. Best is trial 5 with value: 0.7064933309223935.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:16:00,421]\u001B[0m Trial 6 finished with value: 0.6829459201480783 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:16:16,643]\u001B[0m Trial 7 finished with value: 0.6613812646429658 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:16:33,542]\u001B[0m Trial 8 finished with value: 0.681226771949658 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 45}. Best is trial 5 with value: 0.7064933309223935.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:16:50,544]\u001B[0m Trial 9 finished with value: 0.6949240986212261 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:16:58,135]\u001B[0m A new study created in RDB with name: random_forest_emb_nostop_fold_1\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72997299729973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:17:14,702]\u001B[0m Trial 0 finished with value: 0.6668600521873617 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.6668600521873617.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:17:32,976]\u001B[0m Trial 1 finished with value: 0.6895924356044895 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:17:50,472]\u001B[0m Trial 2 finished with value: 0.6830954361582454 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:18:07,287]\u001B[0m Trial 3 finished with value: 0.6438793581726391 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:18:23,980]\u001B[0m Trial 4 finished with value: 0.6619833593612074 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:18:40,477]\u001B[0m Trial 5 finished with value: 0.6450606710584972 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 35}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:18:57,550]\u001B[0m Trial 6 finished with value: 0.6682761245597723 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:19:14,492]\u001B[0m Trial 7 finished with value: 0.6636614246245495 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 45}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:19:31,544]\u001B[0m Trial 8 finished with value: 0.6741364407280238 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 45}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:19:48,513]\u001B[0m Trial 9 finished with value: 0.6682761245597723 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:19:56,582]\u001B[0m A new study created in RDB with name: random_forest_emb_nostop_fold_2\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.719922200611281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:20:13,502]\u001B[0m Trial 0 finished with value: 0.6870832321689521 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.6870832321689521.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:20:30,926]\u001B[0m Trial 1 finished with value: 0.6993184674584633 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 40}. Best is trial 1 with value: 0.6993184674584633.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:20:51,239]\u001B[0m Trial 2 finished with value: 0.7083920652447411 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.7083920652447411.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:21:10,000]\u001B[0m Trial 3 finished with value: 0.7098044405057493 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:21:29,121]\u001B[0m Trial 4 finished with value: 0.6934232419718857 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:21:46,334]\u001B[0m Trial 5 finished with value: 0.6609563361840829 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 50}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:22:03,015]\u001B[0m Trial 6 finished with value: 0.6588584581183431 and parameters: {'min_samples_split': 19, 'max_features': 100, 'num_arvores': 40}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:22:19,688]\u001B[0m Trial 7 finished with value: 0.6812735998906155 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:22:36,627]\u001B[0m Trial 8 finished with value: 0.6804426709520102 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 30}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:22:53,854]\u001B[0m Trial 9 finished with value: 0.6790666195591929 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 50}. Best is trial 3 with value: 0.7098044405057493.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:23:01,553]\u001B[0m A new study created in RDB with name: random_forest_emb_nostop_fold_3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7262986904616207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:23:19,832]\u001B[0m Trial 0 finished with value: 0.6915592329105215 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.6915592329105215.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:23:38,026]\u001B[0m Trial 1 finished with value: 0.698244094726185 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 45}. Best is trial 1 with value: 0.698244094726185.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:23:57,191]\u001B[0m Trial 2 finished with value: 0.6995291719150231 and parameters: {'min_samples_split': 1, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:24:13,706]\u001B[0m Trial 3 finished with value: 0.6732060612685578 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 50}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:24:31,132]\u001B[0m Trial 4 finished with value: 0.6885211505534286 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:24:48,707]\u001B[0m Trial 5 finished with value: 0.6723267931348701 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:25:06,569]\u001B[0m Trial 6 finished with value: 0.6936465262868516 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:25:23,435]\u001B[0m Trial 7 finished with value: 0.6707060162423684 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:25:40,110]\u001B[0m Trial 8 finished with value: 0.6682105582721224 and parameters: {'min_samples_split': 19, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:25:57,250]\u001B[0m Trial 9 finished with value: 0.6889900820997156 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 40}. Best is trial 2 with value: 0.6995291719150231.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:26:05,381]\u001B[0m A new study created in RDB with name: random_forest_emb_nostop_fold_4\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7307504703504982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:26:22,657]\u001B[0m Trial 0 finished with value: 0.696529841850048 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.696529841850048.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:26:40,516]\u001B[0m Trial 1 finished with value: 0.6916671380751621 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.696529841850048.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:26:58,062]\u001B[0m Trial 2 finished with value: 0.7025148587120373 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:27:15,528]\u001B[0m Trial 3 finished with value: 0.6741922141902327 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:27:32,778]\u001B[0m Trial 4 finished with value: 0.684719664155771 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:27:50,233]\u001B[0m Trial 5 finished with value: 0.6799551741624968 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:28:07,425]\u001B[0m Trial 6 finished with value: 0.6782508724848649 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 40}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:28:24,521]\u001B[0m Trial 7 finished with value: 0.6813979146999402 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:28:41,770]\u001B[0m Trial 8 finished with value: 0.7011614040154256 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:28:59,930]\u001B[0m Trial 9 finished with value: 0.6975802181210055 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7025148587120373.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:29:07,628]\u001B[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_0\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6773573392846695\n",
      "Representação: emb_nostop concluida\n",
      "===== Representação: emb_keywords_exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:29:22,559]\u001B[0m Trial 0 finished with value: 0.6652260033037036 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.6652260033037036.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:29:37,642]\u001B[0m Trial 1 finished with value: 0.6795096383503066 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 40}. Best is trial 1 with value: 0.6795096383503066.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:29:54,680]\u001B[0m Trial 2 finished with value: 0.6852179460116528 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 40}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:30:18,529]\u001B[0m Trial 3 finished with value: 0.6774484495962461 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:30:39,550]\u001B[0m Trial 4 finished with value: 0.6790786471412987 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:31:01,210]\u001B[0m Trial 5 finished with value: 0.6766355161565851 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:31:24,135]\u001B[0m Trial 6 finished with value: 0.6778445132719607 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 40}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:31:43,610]\u001B[0m Trial 7 finished with value: 0.6777913603915361 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:32:04,729]\u001B[0m Trial 8 finished with value: 0.6753842678302758 and parameters: {'min_samples_split': 11, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:32:26,230]\u001B[0m Trial 9 finished with value: 0.670404400935989 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 45}. Best is trial 2 with value: 0.6852179460116528.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:32:35,471]\u001B[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_1\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7049008138846669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:32:58,700]\u001B[0m Trial 0 finished with value: 0.6806867867245728 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.6806867867245728.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:33:19,590]\u001B[0m Trial 1 finished with value: 0.67964007303619 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.6806867867245728.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:33:42,369]\u001B[0m Trial 2 finished with value: 0.683882388988933 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:34:04,844]\u001B[0m Trial 3 finished with value: 0.6833677413958075 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:34:26,490]\u001B[0m Trial 4 finished with value: 0.6662400242056802 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 30}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:34:46,746]\u001B[0m Trial 5 finished with value: 0.6747668105862795 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:35:05,669]\u001B[0m Trial 6 finished with value: 0.6742115182064582 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:35:23,540]\u001B[0m Trial 7 finished with value: 0.674774880619422 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:35:45,479]\u001B[0m Trial 8 finished with value: 0.6733959556063578 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:36:08,259]\u001B[0m Trial 9 finished with value: 0.6790838009558461 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:36:18,497]\u001B[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_2\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6731008717310087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:36:41,120]\u001B[0m Trial 0 finished with value: 0.6855340933669751 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.6855340933669751.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:37:03,765]\u001B[0m Trial 1 finished with value: 0.6781884329389741 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.6855340933669751.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:37:25,689]\u001B[0m Trial 2 finished with value: 0.6661687032324266 and parameters: {'min_samples_split': 21, 'max_features': 85, 'num_arvores': 40}. Best is trial 0 with value: 0.6855340933669751.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:37:47,790]\u001B[0m Trial 3 finished with value: 0.6923533469433679 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 30}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:38:10,265]\u001B[0m Trial 4 finished with value: 0.6812503060618962 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:38:31,107]\u001B[0m Trial 5 finished with value: 0.6847465042684714 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:38:46,562]\u001B[0m Trial 6 finished with value: 0.6804165235869705 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 40}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:39:02,100]\u001B[0m Trial 7 finished with value: 0.6816255576110642 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 35}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:39:23,196]\u001B[0m Trial 8 finished with value: 0.6908443462124394 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 45}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:39:44,787]\u001B[0m Trial 9 finished with value: 0.6826564133252466 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 40}. Best is trial 3 with value: 0.6923533469433679.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:39:53,925]\u001B[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6848030018761726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:40:14,113]\u001B[0m Trial 0 finished with value: 0.6799844225125128 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:40:37,115]\u001B[0m Trial 1 finished with value: 0.679295171990479 and parameters: {'min_samples_split': 1, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:40:59,186]\u001B[0m Trial 2 finished with value: 0.6780320235279539 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 40}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:41:21,789]\u001B[0m Trial 3 finished with value: 0.6780604273521829 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:41:44,643]\u001B[0m Trial 4 finished with value: 0.6760858958764899 and parameters: {'min_samples_split': 1, 'max_features': 90, 'num_arvores': 40}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:42:05,454]\u001B[0m Trial 5 finished with value: 0.6696740843312413 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:42:27,933]\u001B[0m Trial 6 finished with value: 0.67094592092861 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:42:49,181]\u001B[0m Trial 7 finished with value: 0.6704456919698503 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 45}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:43:08,466]\u001B[0m Trial 8 finished with value: 0.6689392984710145 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:43:29,343]\u001B[0m Trial 9 finished with value: 0.680158653446646 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 9 with value: 0.680158653446646.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:43:38,288]\u001B[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_4\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6946259167480164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:43:59,385]\u001B[0m Trial 0 finished with value: 0.6777129323854263 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.6777129323854263.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:44:20,473]\u001B[0m Trial 1 finished with value: 0.6662154103288196 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 0 with value: 0.6777129323854263.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:44:42,153]\u001B[0m Trial 2 finished with value: 0.6792114708255209 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:45:02,968]\u001B[0m Trial 3 finished with value: 0.6749658523674725 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:45:23,638]\u001B[0m Trial 4 finished with value: 0.6785677452904952 and parameters: {'min_samples_split': 11, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:45:46,158]\u001B[0m Trial 5 finished with value: 0.6784739955830412 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:46:04,067]\u001B[0m Trial 6 finished with value: 0.6770525881016106 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 45}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:46:20,222]\u001B[0m Trial 7 finished with value: 0.6662154103288196 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:46:35,427]\u001B[0m Trial 8 finished with value: 0.6693242247013416 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 45}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:46:49,673]\u001B[0m Trial 9 finished with value: 0.6718047840167752 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:46:56,641]\u001B[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_0\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6729699666295884\n",
      "Representação: emb_keywords_exp concluida\n",
      "===== Representação: bow_keywords_exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:47:04,257]\u001B[0m Trial 0 finished with value: 0.6958641011771333 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.6958641011771333.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:09,431]\u001B[0m Trial 1 finished with value: 0.690873139424807 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 45}. Best is trial 0 with value: 0.6958641011771333.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:13,980]\u001B[0m Trial 2 finished with value: 0.6907100377504669 and parameters: {'min_samples_split': 21, 'max_features': 80, 'num_arvores': 35}. Best is trial 0 with value: 0.6958641011771333.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:20,761]\u001B[0m Trial 3 finished with value: 0.701648440842181 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 50}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:27,650]\u001B[0m Trial 4 finished with value: 0.6982857483795427 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 50}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:33,851]\u001B[0m Trial 5 finished with value: 0.6913282814447391 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 35}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:40,477]\u001B[0m Trial 6 finished with value: 0.700905666279779 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 50}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:46,579]\u001B[0m Trial 7 finished with value: 0.6958608178356291 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 40}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:51,586]\u001B[0m Trial 8 finished with value: 0.6939656443570231 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 45}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:47:56,454]\u001B[0m Trial 9 finished with value: 0.6987214796418645 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 40}. Best is trial 3 with value: 0.701648440842181.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:00,934]\u001B[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_1\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6763754045307443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:48:10,067]\u001B[0m Trial 0 finished with value: 0.6768221238363559 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.6768221238363559.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:16,921]\u001B[0m Trial 1 finished with value: 0.6782579339139816 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 45}. Best is trial 1 with value: 0.6782579339139816.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:22,768]\u001B[0m Trial 2 finished with value: 0.6785140498373584 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 40}. Best is trial 2 with value: 0.6785140498373584.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:32,262]\u001B[0m Trial 3 finished with value: 0.6745882262190501 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.6785140498373584.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:38,491]\u001B[0m Trial 4 finished with value: 0.6857448373414045 and parameters: {'min_samples_split': 9, 'max_features': 75, 'num_arvores': 35}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:46,752]\u001B[0m Trial 5 finished with value: 0.6760933254226948 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 50}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:51,116]\u001B[0m Trial 6 finished with value: 0.6752607423407851 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:48:56,744]\u001B[0m Trial 7 finished with value: 0.6733414420655097 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 30}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:04,813]\u001B[0m Trial 8 finished with value: 0.6689902479761988 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 50}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:12,092]\u001B[0m Trial 9 finished with value: 0.6768102504116073 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 35}. Best is trial 4 with value: 0.6857448373414045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:15,709]\u001B[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_2\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6961805555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:49:21,576]\u001B[0m Trial 0 finished with value: 0.6754237811875198 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 30}. Best is trial 0 with value: 0.6754237811875198.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:30,152]\u001B[0m Trial 1 finished with value: 0.6795839982146817 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.6795839982146817.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:37,531]\u001B[0m Trial 2 finished with value: 0.6827952281904982 and parameters: {'min_samples_split': 11, 'max_features': 75, 'num_arvores': 40}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:43,930]\u001B[0m Trial 3 finished with value: 0.6721260296292328 and parameters: {'min_samples_split': 21, 'max_features': 95, 'num_arvores': 40}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:51,020]\u001B[0m Trial 4 finished with value: 0.6708145698853091 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 40}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:49:56,755]\u001B[0m Trial 5 finished with value: 0.6779110641263016 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 30}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:06,003]\u001B[0m Trial 6 finished with value: 0.67546519246782 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 50}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:17,270]\u001B[0m Trial 7 finished with value: 0.6737391523771347 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 45}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:24,836]\u001B[0m Trial 8 finished with value: 0.6773120425455638 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:30,548]\u001B[0m Trial 9 finished with value: 0.6797448109133972 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 35}. Best is trial 2 with value: 0.6827952281904982.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:34,633]\u001B[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6948567521975594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:50:42,223]\u001B[0m Trial 0 finished with value: 0.6891827382178098 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 40}. Best is trial 0 with value: 0.6891827382178098.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:47,129]\u001B[0m Trial 1 finished with value: 0.6899483745069158 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 30}. Best is trial 1 with value: 0.6899483745069158.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:50:54,464]\u001B[0m Trial 2 finished with value: 0.6954526138271045 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 50}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:01,347]\u001B[0m Trial 3 finished with value: 0.6843630460649158 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 40}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:06,243]\u001B[0m Trial 4 finished with value: 0.6843637271740709 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:13,950]\u001B[0m Trial 5 finished with value: 0.6883374548015059 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 45}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:18,310]\u001B[0m Trial 6 finished with value: 0.6884736876937042 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 30}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:24,167]\u001B[0m Trial 7 finished with value: 0.6888863461276036 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:30,961]\u001B[0m Trial 8 finished with value: 0.6916015189286974 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 40}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:35,270]\u001B[0m Trial 9 finished with value: 0.6857527646376744 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 30}. Best is trial 2 with value: 0.6954526138271045.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:40,146]\u001B[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_4\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671095652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:51:50,345]\u001B[0m Trial 0 finished with value: 0.6802113682238674 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 0 with value: 0.6802113682238674.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:51:57,249]\u001B[0m Trial 1 finished with value: 0.6779445056441373 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 40}. Best is trial 0 with value: 0.6802113682238674.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:06,164]\u001B[0m Trial 2 finished with value: 0.6848910903950939 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 45}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:12,468]\u001B[0m Trial 3 finished with value: 0.679722339348165 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 30}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:18,962]\u001B[0m Trial 4 finished with value: 0.6796824592390974 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:25,882]\u001B[0m Trial 5 finished with value: 0.6796892076932046 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:34,076]\u001B[0m Trial 6 finished with value: 0.6771737614169148 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:40,496]\u001B[0m Trial 7 finished with value: 0.6825953224752149 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:46,096]\u001B[0m Trial 8 finished with value: 0.6823668680629571 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:51,376]\u001B[0m Trial 9 finished with value: 0.6775607487916105 and parameters: {'min_samples_split': 9, 'max_features': 70, 'num_arvores': 30}. Best is trial 2 with value: 0.6848910903950939.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:52:56,649]\u001B[0m A new study created in RDB with name: random_forest_bow_fold_0\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6841568743228451\n",
      "Representação: bow_keywords_exp concluida\n",
      "===== Representação: bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-12 23:53:41,541]\u001B[0m Trial 0 finished with value: 0.7207462651972228 and parameters: {'min_samples_split': 15, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7207462651972228.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:54:44,965]\u001B[0m Trial 1 finished with value: 0.7264816568212208 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:55:31,461]\u001B[0m Trial 2 finished with value: 0.7168345275462102 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 35}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:56:24,726]\u001B[0m Trial 3 finished with value: 0.719769471952398 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 50}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:57:05,065]\u001B[0m Trial 4 finished with value: 0.7156536105736979 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 30}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:57:34,366]\u001B[0m Trial 5 finished with value: 0.7143826619047409 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 35}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:58:15,796]\u001B[0m Trial 6 finished with value: 0.7198956360155719 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 30}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:59:02,516]\u001B[0m Trial 7 finished with value: 0.7197234252288064 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 45}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-12 23:59:54,450]\u001B[0m Trial 8 finished with value: 0.7197234252288064 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 45}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:00:46,452]\u001B[0m Trial 9 finished with value: 0.7181563341596968 and parameters: {'min_samples_split': 17, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.7264816568212208.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:01:29,876]\u001B[0m A new study created in RDB with name: random_forest_bow_fold_1\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743047830923248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-13 00:02:47,635]\u001B[0m Trial 0 finished with value: 0.7189154777060441 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.7189154777060441.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:03:41,330]\u001B[0m Trial 1 finished with value: 0.7111573274405013 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7189154777060441.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:04:21,100]\u001B[0m Trial 2 finished with value: 0.7124018054841041 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.7189154777060441.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:05:24,059]\u001B[0m Trial 3 finished with value: 0.7226816374429902 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 50}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:06:24,780]\u001B[0m Trial 4 finished with value: 0.7139680873582114 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 40}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:07:08,905]\u001B[0m Trial 5 finished with value: 0.7157471539087717 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 30}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:07:55,450]\u001B[0m Trial 6 finished with value: 0.7149490915230129 and parameters: {'min_samples_split': 11, 'max_features': 75, 'num_arvores': 35}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:08:48,535]\u001B[0m Trial 7 finished with value: 0.713248413087627 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:09:29,360]\u001B[0m Trial 8 finished with value: 0.7127041605288511 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 40}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:10:27,097]\u001B[0m Trial 9 finished with value: 0.709969354781366 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.7226816374429902.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:11:16,941]\u001B[0m A new study created in RDB with name: random_forest_bow_fold_2\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7599573257467994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-13 00:12:13,108]\u001B[0m Trial 0 finished with value: 0.7243653630040304 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7243653630040304.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:13:19,144]\u001B[0m Trial 1 finished with value: 0.7221985478260703 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.7243653630040304.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:14:39,238]\u001B[0m Trial 2 finished with value: 0.7175993973142845 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.7243653630040304.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:16:11,908]\u001B[0m Trial 3 finished with value: 0.7210698859091375 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7243653630040304.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:17:58,195]\u001B[0m Trial 4 finished with value: 0.7300766262224437 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:18:41,254]\u001B[0m Trial 5 finished with value: 0.7090410142171345 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:20:07,778]\u001B[0m Trial 6 finished with value: 0.7168981529305739 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 45}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:21:48,438]\u001B[0m Trial 7 finished with value: 0.7227204433813011 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 45}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:23:26,642]\u001B[0m Trial 8 finished with value: 0.7300766262224437 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:24:37,983]\u001B[0m Trial 9 finished with value: 0.7239155413123598 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 45}. Best is trial 4 with value: 0.7300766262224437.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:25:45,212]\u001B[0m A new study created in RDB with name: random_forest_bow_fold_3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7227294997995635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-13 00:27:20,281]\u001B[0m Trial 0 finished with value: 0.7230772336214804 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.7230772336214804.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:28:10,204]\u001B[0m Trial 1 finished with value: 0.7264154559342905 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 35}. Best is trial 1 with value: 0.7264154559342905.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:29:34,819]\u001B[0m Trial 2 finished with value: 0.7199697607659897 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 1 with value: 0.7264154559342905.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:30:29,837]\u001B[0m Trial 3 finished with value: 0.7293329516300396 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 35}. Best is trial 3 with value: 0.7293329516300396.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:31:40,114]\u001B[0m Trial 4 finished with value: 0.7279806120270719 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 40}. Best is trial 3 with value: 0.7293329516300396.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:32:49,474]\u001B[0m Trial 5 finished with value: 0.7182935423071554 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 40}. Best is trial 3 with value: 0.7293329516300396.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:33:44,223]\u001B[0m Trial 6 finished with value: 0.731104385768687 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 35}. Best is trial 6 with value: 0.731104385768687.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:34:40,624]\u001B[0m Trial 7 finished with value: 0.7246345960220067 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 45}. Best is trial 6 with value: 0.731104385768687.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:35:37,589]\u001B[0m Trial 8 finished with value: 0.7197155067320513 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 45}. Best is trial 6 with value: 0.731104385768687.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:36:27,995]\u001B[0m Trial 9 finished with value: 0.7248193238615173 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 30}. Best is trial 6 with value: 0.731104385768687.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:37:16,657]\u001B[0m A new study created in RDB with name: random_forest_bow_fold_4\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296996662958841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-13 00:38:28,236]\u001B[0m Trial 0 finished with value: 0.7144236505967662 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.7144236505967662.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:39:13,009]\u001B[0m Trial 1 finished with value: 0.7113858845912563 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 0 with value: 0.7144236505967662.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:39:46,943]\u001B[0m Trial 2 finished with value: 0.713026227238609 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7144236505967662.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:40:13,371]\u001B[0m Trial 3 finished with value: 0.7090809920672797 and parameters: {'min_samples_split': 21, 'max_features': 85, 'num_arvores': 30}. Best is trial 0 with value: 0.7144236505967662.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:40:52,520]\u001B[0m Trial 4 finished with value: 0.7097634172366312 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.7144236505967662.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:41:59,446]\u001B[0m Trial 5 finished with value: 0.7148175650170376 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 45}. Best is trial 5 with value: 0.7148175650170376.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:42:40,468]\u001B[0m Trial 6 finished with value: 0.7130450471087766 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 50}. Best is trial 5 with value: 0.7148175650170376.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:43:45,990]\u001B[0m Trial 7 finished with value: 0.723079776623837 and parameters: {'min_samples_split': 1, 'max_features': 80, 'num_arvores': 50}. Best is trial 7 with value: 0.723079776623837.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:45:01,598]\u001B[0m Trial 8 finished with value: 0.7231784041313579 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 50}. Best is trial 8 with value: 0.7231784041313579.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:45:58,113]\u001B[0m Trial 9 finished with value: 0.7210625276604518 and parameters: {'min_samples_split': 9, 'max_features': 75, 'num_arvores': 50}. Best is trial 8 with value: 0.7231784041313579.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7331880690598215\n",
      "Representação: bow concluida\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import calcula_experimento_representacao, OtimizacaoObjetivoRandomForest\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\"random_forest\":{\"classe_otimizacao\":OtimizacaoObjetivoRandomForest,\n",
    "                                \"sampler\":optuna.samplers.TPESampler(seed=1, n_startup_trials=10)},\n",
    "              }\n",
    "df_amazon_reviews = pd.read_csv(\"datasets\\\\amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "\n",
    "#executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        col_classe = \"class\"\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 10\n",
    "\n",
    "\n",
    "        nom_experimento = f\"{metodo}_\"+representation.nome\n",
    "        experimento = calcula_experimento_representacao(nom_experimento,representation,df_amazon_reviews,\n",
    "                                            col_classe,num_folds,num_folds_validacao,num_trials,\n",
    "                                            ClasseObjetivoOtimizacao=param_metodo['classe_otimizacao'],\n",
    "                                                sampler=param_metodo['sampler'])\n",
    "        print(f\"Representação: {representation.nome} concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-13 00:49:06,763]\u001B[0m Using an existing study with name 'random_forest_bow_fold_0' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,810]\u001B[0m Using an existing study with name 'random_forest_bow_fold_1' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,841]\u001B[0m Using an existing study with name 'random_forest_bow_fold_2' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,879]\u001B[0m Using an existing study with name 'random_forest_bow_fold_3' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,904]\u001B[0m Using an existing study with name 'random_forest_bow_fold_4' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,951]\u001B[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_0' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:06,982]\u001B[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_1' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,014]\u001B[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_2' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,045]\u001B[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_3' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,079]\u001B[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_4' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,108]\u001B[0m Using an existing study with name 'random_forest_embbeding_fold_0' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,155]\u001B[0m Using an existing study with name 'random_forest_embbeding_fold_1' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,186]\u001B[0m Using an existing study with name 'random_forest_embbeding_fold_2' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,218]\u001B[0m Using an existing study with name 'random_forest_embbeding_fold_3' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,249]\u001B[0m Using an existing study with name 'random_forest_embbeding_fold_4' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,280]\u001B[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_0' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,312]\u001B[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_1' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,343]\u001B[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_2' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,380]\u001B[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_3' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,421]\u001B[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_4' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,468]\u001B[0m Using an existing study with name 'random_forest_emb_nostop_fold_0' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,500]\u001B[0m Using an existing study with name 'random_forest_emb_nostop_fold_1' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,547]\u001B[0m Using an existing study with name 'random_forest_emb_nostop_fold_2' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,594]\u001B[0m Using an existing study with name 'random_forest_emb_nostop_fold_3' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2022-12-13 00:49:07,641]\u001B[0m Using an existing study with name 'random_forest_emb_nostop_fold_4' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom_experimento</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>f1-positive</th>\n",
       "      <th>precision-positive</th>\n",
       "      <th>recall-positive</th>\n",
       "      <th>f1-negative</th>\n",
       "      <th>precision-negative</th>\n",
       "      <th>recall-negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest_bow</td>\n",
       "      <td>0.737724</td>\n",
       "      <td>0.734912</td>\n",
       "      <td>0.739753</td>\n",
       "      <td>0.730643</td>\n",
       "      <td>0.740537</td>\n",
       "      <td>0.736438</td>\n",
       "      <td>0.745155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest_bow_keywords_exp</td>\n",
       "      <td>0.684533</td>\n",
       "      <td>0.675478</td>\n",
       "      <td>0.692037</td>\n",
       "      <td>0.660006</td>\n",
       "      <td>0.693588</td>\n",
       "      <td>0.678520</td>\n",
       "      <td>0.709644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest_embbeding</td>\n",
       "      <td>0.722678</td>\n",
       "      <td>0.718133</td>\n",
       "      <td>0.725953</td>\n",
       "      <td>0.710629</td>\n",
       "      <td>0.727223</td>\n",
       "      <td>0.719860</td>\n",
       "      <td>0.734882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest_emb_keywords_exp</td>\n",
       "      <td>0.686080</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>0.694003</td>\n",
       "      <td>0.662761</td>\n",
       "      <td>0.694799</td>\n",
       "      <td>0.680590</td>\n",
       "      <td>0.710886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest_emb_nostop</td>\n",
       "      <td>0.716860</td>\n",
       "      <td>0.712571</td>\n",
       "      <td>0.719894</td>\n",
       "      <td>0.706562</td>\n",
       "      <td>0.721150</td>\n",
       "      <td>0.715720</td>\n",
       "      <td>0.727707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nom_experimento  macro-f1  f1-positive  precision-positive  \\\n",
       "0               random_forest_bow  0.737724     0.734912            0.739753   \n",
       "1  random_forest_bow_keywords_exp  0.684533     0.675478            0.692037   \n",
       "2         random_forest_embbeding  0.722678     0.718133            0.725953   \n",
       "3  random_forest_emb_keywords_exp  0.686080     0.677361            0.694003   \n",
       "4        random_forest_emb_nostop  0.716860     0.712571            0.719894   \n",
       "\n",
       "   recall-positive  f1-negative  precision-negative  recall-negative  \n",
       "0         0.730643     0.740537            0.736438         0.745155  \n",
       "1         0.660006     0.693588            0.678520         0.709644  \n",
       "2         0.710629     0.727223            0.719860         0.734882  \n",
       "3         0.662761     0.694799            0.680590         0.710886  \n",
       "4         0.706562     0.721150            0.715720         0.727707  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "pd.DataFrame.from_dict(arr_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Qual representação foi melhor?** \n",
    "De acordo com a métrica de avaliação macro f1, sendo melhor o maior valor, a representação bag of words mostrou-se superior as demais.\n",
    "\n",
    "**A restrição de vocabulário ou eliminação de stopwords auxiliou?**\n",
    "Em ambas as representações, bag of words e embbeding, houve uma piora no resultado de macro f1. Apesar disso, uma vez que os resultados foram próximos, pode-se dizer que auxiliou, já que estes métodos adicionam ganhos em tempo de execução do algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}